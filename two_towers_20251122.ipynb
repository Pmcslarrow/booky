{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3c5igSoT9JS"
      },
      "source": [
        "## Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kVvdwqPSMaoM",
        "outputId": "6cb35f12-09ae-4775-a88a-15186337fc78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: kagglehub[pandas-datasets]\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kagglehub[pandas-datasets]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "slTaZzDAMLUM",
        "outputId": "afbda2b2-2ecc-4b8a-bdbc-04d7531c6d0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/0b/t_zq87x958lc5wvb065nb89m0000gn/T/ipykernel_36208/3451704911.py:20: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  books_df = kagglehub.load_dataset(\n",
            "/Users/pmcslarrow/Desktop/booky/.venv/lib/python3.13/site-packages/kagglehub/pandas_datasets.py:91: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  result = read_function(\n",
            "/var/folders/0b/t_zq87x958lc5wvb065nb89m0000gn/T/ipykernel_36208/3451704911.py:26: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  ratings_df = kagglehub.load_dataset(\n",
            "/var/folders/0b/t_zq87x958lc5wvb065nb89m0000gn/T/ipykernel_36208/3451704911.py:32: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  users_df = kagglehub.load_dataset(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from collections import Counter\n",
        "# from google.colab import drive\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "# drive.mount('/content/drive/')\n",
        "\n",
        "# Load the latest version\n",
        "books_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Books.csv\",\n",
        ")\n",
        "\n",
        "ratings_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Ratings.csv\",\n",
        ")\n",
        "\n",
        "users_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Users.csv\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc3cHdnBaBSd"
      },
      "source": [
        "### Only keeping books with more than three ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX05tHiWbIKN",
        "outputId": "c01cace5-e537-4c97-e2b2-ee52262529ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(271360, 8)\n",
            "(3978, 8)\n"
          ]
        }
      ],
      "source": [
        "print(books_df.shape)\n",
        "books_df = books_df[books_df.groupby('Book-Title')['Book-Title'].transform('count') > 5]\n",
        "print(books_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MJPgJ-ea1z5"
      },
      "source": [
        "### Joining Books, Ratings, and Users tables together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "93r6ilG_NQPw",
        "outputId": "95dd915a-1b0a-4b51-c541-01b5beadc396"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    13181.000000\n",
              "mean         7.956301\n",
              "std          1.778792\n",
              "min          1.000000\n",
              "25%          7.000000\n",
              "50%          8.000000\n",
              "75%          9.000000\n",
              "max         10.000000\n",
              "Name: Book-Rating, dtype: float64"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_ratings_books = pd.merge(ratings_df, books_df, on=\"ISBN\", how='inner')\n",
        "df = pd.merge(df_ratings_books, users_df, on='User-ID')\n",
        "df['User-ID'] = df['User-ID'].astype(str)\n",
        "df['Year-Of-Publication'] = pd.to_numeric(df['Year-Of-Publication'], errors='coerce')\n",
        "df = df.dropna(subset=['Year-Of-Publication'])\n",
        "df = df.dropna(subset=['Age'])\n",
        "df = df[df['Book-Rating'] > 0]\n",
        "df = df[df['Age'] <= 100]\n",
        "df = df[df['Year-Of-Publication'] > 0]\n",
        "df['Book-Rating'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7i4lqEGb_pr"
      },
      "source": [
        "### Combining my own data into the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "8tSGkmikGSCI"
      },
      "outputs": [],
      "source": [
        "personal_df = pd.read_csv(\"./fine-tuning-book-set.txt\")\n",
        "end_index = len(df)\n",
        "df = pd.concat([df, personal_df], ignore_index=True, sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "t2gm3-hvb7L_",
        "outputId": "cfa6697b-8a51-41e4-9678-54a351695553"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User-ID</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>Book-Rating</th>\n",
              "      <th>Book-Title</th>\n",
              "      <th>Book-Author</th>\n",
              "      <th>Year-Of-Publication</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1234567890</td>\n",
              "      <td>451521951</td>\n",
              "      <td>5</td>\n",
              "      <td>The Count of Monte Cristo</td>\n",
              "      <td>Alexandre Dumas</td>\n",
              "      <td>1844</td>\n",
              "      <td>Signet Book</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1234567890</td>\n",
              "      <td>684813637</td>\n",
              "      <td>5</td>\n",
              "      <td>1776</td>\n",
              "      <td>David McCullough</td>\n",
              "      <td>2005</td>\n",
              "      <td>Simon &amp; Schuster</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      User-ID       ISBN  Book-Rating                 Book-Title  \\\n",
              "0  1234567890  451521951            5  The Count of Monte Cristo   \n",
              "1  1234567890  684813637            5                       1776   \n",
              "\n",
              "        Book-Author  Year-Of-Publication         Publisher  Age  \n",
              "0   Alexandre Dumas                 1844       Signet Book   23  \n",
              "1  David McCullough                 2005  Simon & Schuster   23  "
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "personal_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXl51Y5OQM0e",
        "outputId": "e537bd70-122f-493a-b1e0-f28183f564fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "hzV4YkTNPSuc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import  StandardScaler\n",
        "\n",
        "# User Tower -- User-ID, Age\n",
        "# Item Tower -- ISBN, Book-Title, Book-Author, Publisher, Year-Of-Publication\n",
        "\n",
        "class BookRecommenderDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class for book recommendation tasks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataframe : pd.DataFrame\n",
        "        The input data containing user, item, and possibly interaction features.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        The processed version of the input dataframe.\n",
        "    encoders : dict\n",
        "        A dictionary mapping column names to fitted label encoders.\n",
        "    reverse_encoders : dict\n",
        "        A dictionary mapping column names to reverse label encoders (index to label).\n",
        "    scalers : dict\n",
        "        A dictionary mapping column names to fitted scalers for numerical features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.encoders = {} # {'Column name': {'value': idx, ...}, ...}\n",
        "        self.reverse_encoders = {} # {'Column name': {idx: 'value', ...}, ...}\n",
        "        self.scalers = {}\n",
        "        self.data = data\n",
        "        # self.data = data.sample(frac=0.20, random_state=42).reset_index(drop=True)\n",
        "        self.preprocess(self.data)\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        self.encode_information()\n",
        "\n",
        "    def encode_information(self):\n",
        "        \"\"\"\n",
        "        Maps {key: index} pairs and StandardScaler for real valued numbers\n",
        "        \"\"\"\n",
        "        label_encoders = ['User-ID', 'ISBN', 'Book-Title', 'Book-Author', 'Publisher']\n",
        "        standard_scalers = ['Age', 'Year-Of-Publication']\n",
        "\n",
        "        for col in label_encoders:\n",
        "            unique_vals = self.data[col].astype(str).unique()\n",
        "            self.encoders[col] = {val: idx + 1 for idx, val in enumerate(unique_vals)}\n",
        "            self.reverse_encoders[col] = {idx + 1: val for idx, val in enumerate(unique_vals)}\n",
        "            self.data[col] = self.data[col].astype(str).map(self.encoders[col]).fillna(0).astype(int)\n",
        "\n",
        "        for col in standard_scalers:\n",
        "            self.scalers[col] = StandardScaler()\n",
        "            self.data[[col]] = self.scalers[col].fit_transform(self.data[[col]])\n",
        "\n",
        "        # Manually adding my own User-ID so I don't need to adjust nn.Embedding later\n",
        "        # max_user_idx = max(self.encoders['User-ID'].values())\n",
        "        # self.encoders['User-ID'][\"1234567890\"] = max_user_idx + 1\n",
        "        # self.reverse_encoders['User-ID'][max_user_idx + 1] = \"1234567890\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        return {\n",
        "            \"User-ID\": torch.tensor(row[\"User-ID\"], dtype=torch.long),\n",
        "            \"User-Age\": torch.tensor(row[\"Age\"], dtype=torch.float32),\n",
        "            \"Book-ISBN\": torch.tensor(row[\"ISBN\"], dtype=torch.long),\n",
        "            \"Book-Title\": torch.tensor(row[\"Book-Title\"], dtype=torch.long),\n",
        "            \"Book-Author\": torch.tensor(row[\"Book-Author\"], dtype=torch.long),\n",
        "            \"Book-Publisher\": torch.tensor(row[\"Publisher\"], dtype=torch.long),\n",
        "            \"Book-Year-Of-Publication\": torch.tensor(row[\"Year-Of-Publication\"], dtype=torch.float32),\n",
        "        }\n",
        "\n",
        "\n",
        "dataset = BookRecommenderDataset(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNCVwwNWwUc7",
        "outputId": "8ab1e53f-7cc6-41bc-e600-9a79d9134290"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'User-ID': tensor(1),\n",
              " 'User-Age': tensor(-0.1115),\n",
              " 'Book-ISBN': tensor(1),\n",
              " 'Book-Title': tensor(1),\n",
              " 'Book-Author': tensor(1),\n",
              " 'Book-Publisher': tensor(1),\n",
              " 'Book-Year-Of-Publication': tensor(0.4470)}"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "5UuengF0Tojk"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.7 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vznGD0IGTxA7",
        "outputId": "2b2d6fdb-f25e-40fc-b444-9d1218f410aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'User-ID': tensor([2266, 1241, 4740, 5530,  549, 1259,   74, 5446, 1971, 5419, 5096,  474,\n",
              "         6252,  519, 6554, 5487, 2162, 2935,  698, 6015, 6232, 5242, 2920, 6167,\n",
              "         3811,  328, 1472, 2422, 2839, 2931, 5406, 6181, 4954, 2728, 1995, 4871,\n",
              "         2867, 3042, 3664, 3086, 4828, 3440, 5737, 2911, 1581, 4488, 1928, 4720,\n",
              "         6159, 4701, 3307, 2129,  929, 5865, 3916, 4920, 4469, 3701, 6069, 1754,\n",
              "         5283, 3599, 3919,  476, 2193, 4896, 6425, 1402, 4689, 2366, 2746, 4090,\n",
              "         2409, 3677,   79,   87,  944, 6537, 5214,  436, 3850, 4401,  838, 2496,\n",
              "         5673, 6240, 2558, 5283, 1140,  810,   60,  749,  505,  738,  758, 2339,\n",
              "         2931, 4715, 1750, 3368,  152, 2575, 6340, 3204, 2912, 4666, 2525, 3284,\n",
              "         2820, 5644, 6258, 1311,  745, 1201, 2886, 1553,  108, 6619, 2554, 6149,\n",
              "         2315, 4987,  197, 2480, 4845, 4221, 3034, 3503]),\n",
              " 'User-Age': tensor([-1.0714, -0.5114, -0.4314, -0.9114, -0.4314,  1.4883, -0.1115,  0.2085,\n",
              "         -0.2715,  0.2085, -0.5114,  0.9284,  0.1285, -0.3515, -0.8314, -0.1915,\n",
              "          0.0485, -1.0714, -1.0714,  0.1285,  0.2085,  0.4485, -0.4314, -0.9914,\n",
              "          0.2085, -0.7514,  1.7283, -1.4713,  0.2885,  1.8883,  1.2484, -0.1115,\n",
              "          0.3685,  0.9284, -0.8314, -0.2715,  3.1681,  0.2085,  0.2085,  0.4485,\n",
              "         -0.1115,  1.8883, -0.1915,  0.1285,  1.7283, -0.3515, -0.5914,  1.4883,\n",
              "         -0.9914,  1.2484, -1.6313, -0.5914,  0.4485,  0.0485, -0.7514,  0.7684,\n",
              "         -0.9114, -0.1915, -0.4314,  1.5683, -0.2715,  1.2484, -0.3515, -0.2715,\n",
              "          0.8484,  1.4883,  2.1283,  1.3284,  1.7283,  0.0485, -0.8314, -1.6313,\n",
              "          1.2484, -0.1115,  1.4083,  0.0485, -0.9914,  0.0485, -1.3913,  0.5284,\n",
              "          2.9282,  0.9284, -0.5114,  0.6884, -0.5114,  0.2885,  0.2885, -0.2715,\n",
              "         -0.1115, -1.4713, -0.9114, -0.9914, -0.1115,  1.0884, -0.2715, -0.1915,\n",
              "          1.8883, -0.1115, -1.3913,  0.8484,  1.0084, -0.4314, -0.2715, -0.2715,\n",
              "          0.6884, -0.4314,  1.8883, -0.5914,  4.9279,  1.2484, -0.5114, -0.1115,\n",
              "         -0.9914, -1.7913,  2.0483, -0.6714, -0.7514,  0.0485, -0.9114, -1.1514,\n",
              "         -1.4713,  0.6084, -0.2715, -0.1115, -0.5114, -0.7514,  0.6084,  0.3685]),\n",
              " 'Book-ISBN': tensor([  67,  885,  271,   26,    6,   37,   98,  349,  242, 1636, 1968,  170,\n",
              "          286,  112,  130, 1453,  285,  200,    6,    5,  136,  453,  526,   27,\n",
              "          319,  356,  933,  474, 1497,   18,  495,  133, 1077,  656,  514,  479,\n",
              "          991,  172,   16,   29, 1700, 1647, 2087,  474,  182,  495,  745, 1920,\n",
              "          318,  685,  691, 1215,  438,  172,   60,  251,   88,  240,   37,   37,\n",
              "          720,  648,  106,   87,   29,  733,  193,  960,  408,  438,   65,  237,\n",
              "         1315,  103,  106,  114,  101,  133,   68,  452, 1758, 1854,   27,  180,\n",
              "           11, 1179,  368,  486,  853,    9,   74,  242,  235,  650,  471,  451,\n",
              "          463, 1918,  609, 1624,  199,   27,   82,   65,  641,  195, 1175, 1605,\n",
              "         1492,  270, 2113,  721,  655,   64, 1015,   65,  135,  126,   87, 1583,\n",
              "          172,  648,  236, 1382,  164,    6,  329,  318]),\n",
              " 'Book-Title': tensor([ 62,  33,  66,  26,   6,  37,  83, 236, 183, 491, 424, 139, 201,  63,\n",
              "         109, 476, 126, 148,   6,   5, 115, 289, 249,  27, 164,  24,  11, 226,\n",
              "         481,  18, 165, 112, 309,  42, 314, 264,  27, 136,  16,  29,   9, 399,\n",
              "          62, 226, 147, 165, 108, 156, 121, 225,  76, 410,  98, 136,  57, 161,\n",
              "          76, 181,  37,  37, 163, 200,  89,  75,  29, 373, 154,  24, 272,  98,\n",
              "           2, 179, 409,  13,  89,  96,  86, 112,  63,  34, 478, 414,  27, 145,\n",
              "          11, 280, 249,  17,  60,   9,  66, 183, 109,  80, 296, 178, 162, 439,\n",
              "          32, 240, 127,  27,  48,   2, 182, 151, 216, 253, 174, 181, 261,  33,\n",
              "         234,   2, 146,   2, 114, 106,  75, 252, 136, 200,  80, 216,  44,   6,\n",
              "         200, 121]),\n",
              " 'Book-Author': tensor([ 58,  31,  60,  26,   6,  34,  72,  45, 145,  47, 416,  16,  43,  28,\n",
              "          91,  69, 104,  96,   6,   5,  95, 223,  28,  24,  31,  16, 405,  31,\n",
              "         613,  18,  31,  91, 249,  39,  92,  95,  16,  29,  16,  28,  59, 189,\n",
              "          58,  31,  45,  31,  90, 125,  21,  31,  61, 388,  83,  29,  53, 131,\n",
              "          53,  78,  34,  34,  31, 162,  16,  67,  28, 332,  79,  16,  95,  83,\n",
              "           2, 144, 535,  13,  16,  81,  75,  91,  28,  31, 304, 167,  24, 118,\n",
              "          11, 214,  28, 239,  56,   9,  60, 145,  29, 311, 232,  31,  31, 395,\n",
              "          31, 190, 127,  24,  31,   2, 160,  56, 181, 146, 612,  78, 206,  31,\n",
              "         312,   2, 119,   2,  94,   3,  67, 201,  29, 162,  31, 181,  41,   6,\n",
              "         162,  21]),\n",
              " 'Book-Publisher': tensor([ 20,  30,  35,  17,   5,  27,  54,  11, 105,  11, 248,  79,  63,  34,\n",
              "          20, 132, 122,  30,   5,   4,   3,  16, 174,  21,   6,  57,  20,  11,\n",
              "          19,   8, 106,  20, 302, 186,  65,   3,  69,  20,  14,  22,  68, 122,\n",
              "          20,  11,  11, 106,  49,  28,  21, 106,  46,  29,  20,  20,   7,  21,\n",
              "           7,  16,  27,  27,   6,  27,  14,  34,  22,   8,  57, 198,  71,  20,\n",
              "          27, 103, 225,  11,  14,  59,  55,  20,   8,  30, 243, 277,  21,  57,\n",
              "           9,  55,  22,  72,  72,   7,   6, 105, 102,  17, 158,  30,   6,  20,\n",
              "          25,  28,  89,  21,  11,  27,  61,  72,  41, 154, 383,   6, 495,   6,\n",
              "         203,  40,  70,  27,  29,   3,  34, 141,  20,  27,  11,  80,   8,   5,\n",
              "         131,  21]),\n",
              " 'Book-Year-Of-Publication': tensor([-0.0708,  0.8612, -1.5208,  0.7577, -0.6922,  0.8612,  0.0327, -0.2780,\n",
              "          0.5505, -0.3815, -0.6922, -0.2780,  0.5505,  0.7577,  0.5505, -1.0029,\n",
              "          0.1363,  0.3434, -0.6922,  0.8612,  0.5505, -1.0029,  0.5505, -0.2780,\n",
              "          0.9648,  0.1363,  0.2398,  0.6541, -0.2780, -0.6922, -0.2780, -0.6922,\n",
              "          0.6541,  0.6541, -0.2780,  0.3434, -0.2780,  0.4470,  0.0327,  0.4470,\n",
              "         -1.6243, -0.1744,  0.4470,  0.6541,  0.6541, -0.2780, -0.6922,  0.7577,\n",
              "          0.1363, -1.3136,  0.6541, -0.5887,  0.7577,  0.4470, -0.8994, -0.2780,\n",
              "         -0.8994, -1.4172,  0.8612,  0.8612,  0.9648,  0.5505,  0.4470, -0.3815,\n",
              "          0.4470,  0.1363,  0.5505,  0.1363,  0.1363,  0.7577,  0.4470,  0.7577,\n",
              "          0.3434, -1.3136,  0.4470,  0.4470, -0.5887, -0.6922,  0.8612,  0.2398,\n",
              "         -1.5208,  0.5505, -0.2780, -0.0708,  0.2398, -1.0029,  0.6541,  0.2398,\n",
              "          0.3434, -0.7958, -0.4851,  0.5505,  0.4470,  0.1363, -0.5887, -0.5887,\n",
              "         -0.0708,  0.4470, -0.0708, -1.9350,  0.3434, -0.2780,  0.6541,  0.4470,\n",
              "          0.6541,  0.5505,  0.0327,  0.5505,  0.1363, -3.4885,  0.3434,  0.2398,\n",
              "         -1.0029,  0.6541,  0.2398,  0.4470, -0.4851,  0.7577, -0.3815,  0.7577,\n",
              "          0.4470,  0.5505,  0.8612,  0.4470,  0.7577, -0.6922,  0.5505,  0.1363])}"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjn33eipT_-e"
      },
      "source": [
        "## Two Tower Model for Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "J4Rh2nDqUCxf"
      },
      "outputs": [],
      "source": [
        "class UserTower(nn.Module):\n",
        "\n",
        "    # User Tower -- User-ID, Age\n",
        "\n",
        "    def __init__(self, num_users, embedding_dim=16):\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.user_mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim + 1, 512), # 1 embedding + 1 numerical\n",
        "            nn.ReLU(),\n",
        "            # nn.LayerNorm(512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, embedding_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, user_id, age):\n",
        "        \"\"\"\n",
        "        user_id: (batch,) int64\n",
        "        review_mean: (batch,) float32\n",
        "        \"\"\"\n",
        "        user_emb = self.user_embedding(user_id)\n",
        "        age = age.unsqueeze(1)\n",
        "        x = torch.cat([user_emb, age], dim=1)\n",
        "        return self.user_mlp(x)\n",
        "\n",
        "    def get_embedding(self, data):\n",
        "        return self.forward(data['User-ID'], data['User-Age'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "hLSZeUSEUNBY"
      },
      "outputs": [],
      "source": [
        "class ItemTower(nn.Module):\n",
        "    def __init__(self, num_isbn, num_titles, num_authors, num_publishers, embedding_dim=16):\n",
        "        super().__init__()\n",
        "\n",
        "        # Item Tower -- ISBN, Book-Title, Book-Author, Publisher, Year-Of-Publication\n",
        "\n",
        "        self.book_isbn_embedding = nn.Embedding(num_isbn, embedding_dim, padding_idx=0)\n",
        "        self.book_title_embedding = nn.Embedding(num_titles, embedding_dim, padding_idx=0)\n",
        "        self.book_author_embedding = nn.Embedding(num_authors, embedding_dim, padding_idx=0)\n",
        "        self.book_publisher_embedding = nn.Embedding(num_publishers, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.item_mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim * 4 + 1, 512),  # 4 embeddings + 1 numerical\n",
        "            nn.ReLU(),\n",
        "            # nn.LayerNorm(512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, embedding_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, isbn, book_title, book_author, book_publisher, book_year_of_publication):\n",
        "        book_isbn_emb = self.book_isbn_embedding(isbn)\n",
        "        book_title_emb = self.book_title_embedding(book_title)\n",
        "        book_author_emb = self.book_author_embedding(book_author)\n",
        "        book_publisher_emb = self.book_publisher_embedding(book_publisher)\n",
        "        book_year = book_year_of_publication.unsqueeze(1)\n",
        "\n",
        "        x = torch.cat([\n",
        "            book_isbn_emb,\n",
        "            book_title_emb,\n",
        "            book_author_emb,\n",
        "            book_publisher_emb,\n",
        "            book_year\n",
        "        ], dim=1)\n",
        "\n",
        "        return self.item_mlp(x)\n",
        "\n",
        "    def get_embedding(self, data):\n",
        "        return self.forward(\n",
        "            data['Book-ISBN'],\n",
        "            data['Book-Title'],\n",
        "            data['Book-Author'],\n",
        "            data['Book-Publisher'],\n",
        "            data['Book-Year-Of-Publication'],\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "l-fw4HLLUOXk"
      },
      "outputs": [],
      "source": [
        "class TwoTowers(nn.Module):\n",
        "    def __init__(self, user_tower: UserTower, item_tower: ItemTower):\n",
        "        super().__init__()\n",
        "        self.user_tower = user_tower\n",
        "        self.item_tower = item_tower\n",
        "\n",
        "    def forward(self, data):\n",
        "        user_vector = self.user_tower.get_embedding(data)\n",
        "        item_vector = self.item_tower.get_embedding(data['pos_item'])\n",
        "        return (user_vector * item_vector).sum(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "jlqHUau8WMjo"
      },
      "outputs": [],
      "source": [
        "# example_data = next(iter(train_loader))\n",
        "\n",
        "NUM_USERS = len(dataset.encoders['User-ID']) + 1\n",
        "NUM_ISBN = len(dataset.encoders['ISBN']) + 1\n",
        "NUM_TITLES = len(dataset.encoders['Book-Title']) + 1\n",
        "NUM_AUTHORS = len(dataset.encoders['Book-Author']) + 1\n",
        "NUM_PUBLISHERS = len(dataset.encoders['Publisher']) + 1\n",
        "\n",
        "EMBEDDING_SIZE = 128\n",
        "\n",
        "user_tower = UserTower(num_users=NUM_USERS,  embedding_dim=EMBEDDING_SIZE)\n",
        "\n",
        "item_tower = ItemTower(\n",
        "    num_isbn=NUM_ISBN,\n",
        "    num_titles=NUM_TITLES,\n",
        "    num_authors=NUM_AUTHORS,\n",
        "    num_publishers=NUM_PUBLISHERS,\n",
        "    embedding_dim=EMBEDDING_SIZE\n",
        ")\n",
        "\n",
        "two_towers = TwoTowers(\n",
        "    user_tower,\n",
        "    item_tower\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_recall_at_k(two_towers, epoch, EPOCHS):\n",
        "    \"\"\"\n",
        "    Gathers the current item embeddings, \n",
        "    calculates similarity between each user and the items. \n",
        "\n",
        "    Calculates and returns recall@k metric of the recommendations \n",
        "    made to the user.\n",
        "    \"\"\"\n",
        "    entire_dataset = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "    all_item_embeddings = []\n",
        "    for batch in entire_dataset:\n",
        "        item_embedding = two_towers.item_tower.get_embedding(batch)\n",
        "        all_item_embeddings.append(item_embedding)\n",
        "    all_item_embeddings = torch.cat(all_item_embeddings, dim=0)\n",
        "\n",
        "    total_recall = 0.0\n",
        "    num_users = 0\n",
        "    k = 20  \n",
        "\n",
        "    for idx, batch in enumerate(test_loader):\n",
        "        user_embedding = two_towers.user_tower.get_embedding(batch) \n",
        "        similarity_scores = user_embedding @ all_item_embeddings.T  # [batch_size, num_items]\n",
        "\n",
        "        top_scores, top_indices = torch.topk(similarity_scores, k=k, dim=1)\n",
        "\n",
        "        for user_id, items, scores in zip(batch['User-ID'], top_indices, top_scores):\n",
        "            user_rows = dataset.data[dataset.data['User-ID'] == user_id.item()]\n",
        "\n",
        "            # Recommended books (Book-Title IDs)\n",
        "            recommended_book_ids_set = set([dataset.data.iloc[idx.item()]['Book-Title'] for idx in items])\n",
        "            actual_book_ids_set = set(user_rows['Book-Title'].tolist())\n",
        "\n",
        "            hits = len(recommended_book_ids_set & actual_book_ids_set)  # intersection\n",
        "            recall_at_k = hits / len(actual_book_ids_set)\n",
        "\n",
        "            total_recall += recall_at_k\n",
        "            num_users += 1\n",
        "\n",
        "    average_recall_at_k = total_recall / num_users\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}, Average Recall@{k}: {average_recall_at_k:.4f}\\n\")\n",
        "    return average_recall_at_k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3QGAbFDa_Lb"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn06WdV-b4Jf"
      },
      "source": [
        "#### Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "bGt_U5rrg7eA"
      },
      "outputs": [],
      "source": [
        "%rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "GlzzNo4_mM0E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 2e-4\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "TEMPERATURE = 0.1\n",
        "MODEL_SAVE_PATH = \"/models\"\n",
        "\n",
        "# loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(two_towers.parameters(), lr=LEARNING_RATE)\n",
        "writer = SummaryWriter('./logs/')\n",
        "\n",
        "best_test_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "global_step = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhQ3lphfa_nU",
        "outputId": "d9ac0b31-4212-41bc-94ca-27f63e855d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Average Training Loss: 0.8589, \n",
            "Epoch 1/100, Average Test Loss: 8.9229, \n",
            "Epoch 1/100, Average Recall@20: 0.2367\n",
            "\n",
            "Epoch 2/100, Average Training Loss: 0.8617, \n",
            "Epoch 2/100, Average Test Loss: 8.7882, \n",
            "Epoch 2/100, Average Recall@20: 0.2401\n",
            "\n",
            "Epoch 3/100, Average Training Loss: 0.8595, \n",
            "Epoch 3/100, Average Test Loss: 8.9298, \n",
            "Epoch 3/100, Average Recall@20: 0.2387\n",
            "\n",
            "Epoch 4/100, Average Training Loss: 0.8648, \n",
            "Epoch 4/100, Average Test Loss: 8.8396, \n",
            "Epoch 4/100, Average Recall@20: 0.2385\n",
            "\n",
            "Epoch 5/100, Average Training Loss: 0.8651, \n",
            "Epoch 5/100, Average Test Loss: 8.8612, \n",
            "Epoch 5/100, Average Recall@20: 0.2402\n",
            "\n",
            "Epoch 6/100, Average Training Loss: 0.8733, \n",
            "Epoch 6/100, Average Test Loss: 8.8952, \n",
            "Epoch 6/100, Average Recall@20: 0.2394\n",
            "\n",
            "Epoch 7/100, Average Training Loss: 0.8480, \n",
            "Epoch 7/100, Average Test Loss: 8.9586, \n",
            "Epoch 7/100, Average Recall@20: 0.2381\n",
            "\n",
            "Epoch 8/100, Average Training Loss: 0.8704, \n",
            "Epoch 8/100, Average Test Loss: 8.9418, \n",
            "Epoch 8/100, Average Recall@20: 0.2375\n",
            "\n",
            "Epoch 9/100, Average Training Loss: 0.8761, \n",
            "Epoch 9/100, Average Test Loss: 8.7964, \n",
            "Epoch 9/100, Average Recall@20: 0.2379\n",
            "\n",
            "Epoch 10/100, Average Training Loss: 0.8591, \n",
            "Epoch 10/100, Average Test Loss: 8.9199, \n",
            "Epoch 10/100, Average Recall@20: 0.2382\n",
            "\n",
            "Epoch 11/100, Average Training Loss: 0.8521, \n",
            "Epoch 11/100, Average Test Loss: 8.9906, \n",
            "Epoch 11/100, Average Recall@20: 0.2369\n",
            "\n",
            "Epoch 12/100, Average Training Loss: 0.8452, \n",
            "Epoch 12/100, Average Test Loss: 8.9407, \n",
            "Epoch 12/100, Average Recall@20: 0.2366\n",
            "\n",
            "Epoch 13/100, Average Training Loss: 0.8596, \n",
            "Epoch 13/100, Average Test Loss: 8.8850, \n",
            "Epoch 13/100, Average Recall@20: 0.2407\n",
            "\n",
            "Epoch 14/100, Average Training Loss: 0.8627, \n",
            "Epoch 14/100, Average Test Loss: 8.9448, \n",
            "Epoch 14/100, Average Recall@20: 0.2395\n",
            "\n",
            "Epoch 15/100, Average Training Loss: 0.8683, \n",
            "Epoch 15/100, Average Test Loss: 8.7937, \n",
            "Epoch 15/100, Average Recall@20: 0.2376\n",
            "\n",
            "Epoch 16/100, Average Training Loss: 0.8478, \n",
            "Epoch 16/100, Average Test Loss: 8.9976, \n",
            "Epoch 16/100, Average Recall@20: 0.2369\n",
            "\n",
            "Epoch 17/100, Average Training Loss: 0.8602, \n",
            "Epoch 17/100, Average Test Loss: 8.8120, \n",
            "Epoch 17/100, Average Recall@20: 0.2373\n",
            "\n",
            "Epoch 18/100, Average Training Loss: 0.8449, \n",
            "Epoch 18/100, Average Test Loss: 8.9840, \n",
            "Epoch 18/100, Average Recall@20: 0.2378\n",
            "\n",
            "Epoch 19/100, Average Training Loss: 0.8630, \n",
            "Epoch 19/100, Average Test Loss: 8.9931, \n",
            "Epoch 19/100, Average Recall@20: 0.2353\n",
            "\n",
            "Epoch 20/100, Average Training Loss: 0.8321, \n",
            "Epoch 20/100, Average Test Loss: 9.0368, \n",
            "Epoch 20/100, Average Recall@20: 0.2367\n",
            "\n",
            "Epoch 21/100, Average Training Loss: 0.8502, \n",
            "Epoch 21/100, Average Test Loss: 9.0876, \n",
            "Epoch 21/100, Average Recall@20: 0.2378\n",
            "\n",
            "Epoch 22/100, Average Training Loss: 0.8498, \n",
            "Epoch 22/100, Average Test Loss: 8.9726, \n",
            "Epoch 22/100, Average Recall@20: 0.2381\n",
            "\n",
            "Epoch 23/100, Average Training Loss: 0.8440, \n",
            "Epoch 23/100, Average Test Loss: 8.9809, \n",
            "Epoch 23/100, Average Recall@20: 0.2372\n",
            "\n",
            "Epoch 24/100, Average Training Loss: 0.8519, \n",
            "Epoch 24/100, Average Test Loss: 8.9733, \n",
            "Epoch 24/100, Average Recall@20: 0.2352\n",
            "\n",
            "Epoch 25/100, Average Training Loss: 0.8407, \n",
            "Epoch 25/100, Average Test Loss: 8.9958, \n",
            "Epoch 25/100, Average Recall@20: 0.2363\n",
            "\n",
            "Epoch 26/100, Average Training Loss: 0.8556, \n",
            "Epoch 26/100, Average Test Loss: 8.9224, \n",
            "Epoch 26/100, Average Recall@20: 0.2356\n",
            "\n",
            "Epoch 27/100, Average Training Loss: 0.8279, \n",
            "Epoch 27/100, Average Test Loss: 8.9422, \n",
            "Epoch 27/100, Average Recall@20: 0.2382\n",
            "\n",
            "Epoch 28/100, Average Training Loss: 0.8479, \n",
            "Epoch 28/100, Average Test Loss: 8.8375, \n",
            "Epoch 28/100, Average Recall@20: 0.2404\n",
            "\n",
            "Epoch 29/100, Average Training Loss: 0.8311, \n",
            "Epoch 29/100, Average Test Loss: 9.0235, \n",
            "Epoch 29/100, Average Recall@20: 0.2376\n",
            "\n",
            "Epoch 30/100, Average Training Loss: 0.8492, \n",
            "Epoch 30/100, Average Test Loss: 8.9508, \n",
            "Epoch 30/100, Average Recall@20: 0.2362\n",
            "\n",
            "Epoch 31/100, Average Training Loss: 0.8558, \n",
            "Epoch 31/100, Average Test Loss: 8.9831, \n",
            "Epoch 31/100, Average Recall@20: 0.2374\n",
            "\n",
            "Epoch 32/100, Average Training Loss: 0.8446, \n",
            "Epoch 32/100, Average Test Loss: 8.9752, \n",
            "Epoch 32/100, Average Recall@20: 0.2362\n",
            "\n",
            "Epoch 33/100, Average Training Loss: 0.8476, \n",
            "Epoch 33/100, Average Test Loss: 8.9099, \n",
            "Epoch 33/100, Average Recall@20: 0.2364\n",
            "\n",
            "Epoch 34/100, Average Training Loss: 0.8492, \n",
            "Epoch 34/100, Average Test Loss: 8.9595, \n",
            "Epoch 34/100, Average Recall@20: 0.2374\n",
            "\n",
            "Epoch 35/100, Average Training Loss: 0.8512, \n",
            "Epoch 35/100, Average Test Loss: 8.9003, \n",
            "Epoch 35/100, Average Recall@20: 0.2370\n",
            "\n",
            "Epoch 36/100, Average Training Loss: 0.8281, \n",
            "Epoch 36/100, Average Test Loss: 8.8626, \n",
            "Epoch 36/100, Average Recall@20: 0.2362\n",
            "\n",
            "Epoch 37/100, Average Training Loss: 0.8366, \n",
            "Epoch 37/100, Average Test Loss: 8.9408, \n",
            "Epoch 37/100, Average Recall@20: 0.2369\n",
            "\n",
            "Epoch 38/100, Average Training Loss: 0.8433, \n",
            "Epoch 38/100, Average Test Loss: 8.8286, \n",
            "Epoch 38/100, Average Recall@20: 0.2366\n",
            "\n",
            "Epoch 39/100, Average Training Loss: 0.8396, \n",
            "Epoch 39/100, Average Test Loss: 8.9244, \n",
            "Epoch 39/100, Average Recall@20: 0.2375\n",
            "\n",
            "Epoch 40/100, Average Training Loss: 0.8440, \n",
            "Epoch 40/100, Average Test Loss: 8.8802, \n",
            "Epoch 40/100, Average Recall@20: 0.2380\n",
            "\n",
            "Epoch 41/100, Average Training Loss: 0.8482, \n",
            "Epoch 41/100, Average Test Loss: 8.8389, \n",
            "Epoch 41/100, Average Recall@20: 0.2378\n",
            "\n",
            "Epoch 42/100, Average Training Loss: 0.8440, \n",
            "Epoch 42/100, Average Test Loss: 8.8617, \n",
            "Epoch 42/100, Average Recall@20: 0.2383\n",
            "\n",
            "Epoch 43/100, Average Training Loss: 0.8514, \n",
            "Epoch 43/100, Average Test Loss: 8.8447, \n",
            "Epoch 43/100, Average Recall@20: 0.2375\n",
            "\n",
            "Epoch 44/100, Average Training Loss: 0.8334, \n",
            "Epoch 44/100, Average Test Loss: 8.7745, \n",
            "Epoch 44/100, Average Recall@20: 0.2390\n",
            "\n",
            "Epoch 45/100, Average Training Loss: 0.8294, \n",
            "Epoch 45/100, Average Test Loss: 8.8257, \n",
            "Epoch 45/100, Average Recall@20: 0.2389\n",
            "\n",
            "Epoch 46/100, Average Training Loss: 0.8013, \n",
            "Epoch 46/100, Average Test Loss: 8.8130, \n",
            "Epoch 46/100, Average Recall@20: 0.2375\n",
            "\n",
            "Epoch 47/100, Average Training Loss: 0.8378, \n",
            "Epoch 47/100, Average Test Loss: 8.9209, \n",
            "Epoch 47/100, Average Recall@20: 0.2391\n",
            "\n",
            "Epoch 48/100, Average Training Loss: 0.8291, \n",
            "Epoch 48/100, Average Test Loss: 8.8765, \n",
            "Epoch 48/100, Average Recall@20: 0.2393\n",
            "\n",
            "Epoch 49/100, Average Training Loss: 0.8219, \n",
            "Epoch 49/100, Average Test Loss: 8.8719, \n",
            "Epoch 49/100, Average Recall@20: 0.2384\n",
            "\n",
            "Epoch 50/100, Average Training Loss: 0.8216, \n",
            "Epoch 50/100, Average Test Loss: 8.8301, \n",
            "Epoch 50/100, Average Recall@20: 0.2407\n",
            "\n",
            "Epoch 51/100, Average Training Loss: 0.8358, \n",
            "Epoch 51/100, Average Test Loss: 8.9349, \n",
            "Epoch 51/100, Average Recall@20: 0.2385\n",
            "\n",
            "Epoch 52/100, Average Training Loss: 0.8376, \n",
            "Epoch 52/100, Average Test Loss: 8.9757, \n",
            "Epoch 52/100, Average Recall@20: 0.2375\n",
            "\n",
            "Epoch 53/100, Average Training Loss: 0.8188, \n",
            "Epoch 53/100, Average Test Loss: 8.8265, \n",
            "Epoch 53/100, Average Recall@20: 0.2361\n",
            "\n",
            "Epoch 54/100, Average Training Loss: 0.8523, \n",
            "Epoch 54/100, Average Test Loss: 8.8704, \n",
            "Epoch 54/100, Average Recall@20: 0.2366\n",
            "\n",
            "Epoch 55/100, Average Training Loss: 0.8151, \n",
            "Epoch 55/100, Average Test Loss: 8.9187, \n",
            "Epoch 55/100, Average Recall@20: 0.2362\n",
            "\n",
            "Epoch 56/100, Average Training Loss: 0.8305, \n",
            "Epoch 56/100, Average Test Loss: 8.8059, \n",
            "Epoch 56/100, Average Recall@20: 0.2391\n",
            "\n",
            "Epoch 57/100, Average Training Loss: 0.8101, \n",
            "Epoch 57/100, Average Test Loss: 9.0029, \n",
            "Epoch 57/100, Average Recall@20: 0.2383\n",
            "\n",
            "Epoch 58/100, Average Training Loss: 0.8305, \n",
            "Epoch 58/100, Average Test Loss: 8.9725, \n",
            "Epoch 58/100, Average Recall@20: 0.2380\n",
            "\n",
            "Epoch 59/100, Average Training Loss: 0.8362, \n",
            "Epoch 59/100, Average Test Loss: 8.8580, \n",
            "Epoch 59/100, Average Recall@20: 0.2397\n",
            "\n",
            "Epoch 60/100, Average Training Loss: 0.8213, \n",
            "Epoch 60/100, Average Test Loss: 9.0214, \n",
            "Epoch 60/100, Average Recall@20: 0.2372\n",
            "\n",
            "Epoch 61/100, Average Training Loss: 0.8225, \n",
            "Epoch 61/100, Average Test Loss: 8.8813, \n",
            "Epoch 61/100, Average Recall@20: 0.2388\n",
            "\n",
            "Epoch 62/100, Average Training Loss: 0.8283, \n",
            "Epoch 62/100, Average Test Loss: 8.9604, \n",
            "Epoch 62/100, Average Recall@20: 0.2370\n",
            "\n",
            "Epoch 63/100, Average Training Loss: 0.8394, \n",
            "Epoch 63/100, Average Test Loss: 8.8739, \n",
            "Epoch 63/100, Average Recall@20: 0.2361\n",
            "\n",
            "Epoch 64/100, Average Training Loss: 0.8371, \n",
            "Epoch 64/100, Average Test Loss: 8.8931, \n",
            "Epoch 64/100, Average Recall@20: 0.2371\n",
            "\n",
            "Epoch 65/100, Average Training Loss: 0.8206, \n",
            "Epoch 65/100, Average Test Loss: 8.8021, \n",
            "Epoch 65/100, Average Recall@20: 0.2363\n",
            "\n",
            "Epoch 66/100, Average Training Loss: 0.8452, \n",
            "Epoch 66/100, Average Test Loss: 8.9166, \n",
            "Epoch 66/100, Average Recall@20: 0.2377\n",
            "\n",
            "Epoch 67/100, Average Training Loss: 0.8134, \n",
            "Epoch 67/100, Average Test Loss: 8.9672, \n",
            "Epoch 67/100, Average Recall@20: 0.2362\n",
            "\n",
            "Epoch 68/100, Average Training Loss: 0.8301, \n",
            "Epoch 68/100, Average Test Loss: 8.9041, \n",
            "Epoch 68/100, Average Recall@20: 0.2379\n",
            "\n",
            "Epoch 69/100, Average Training Loss: 0.8106, \n",
            "Epoch 69/100, Average Test Loss: 8.8822, \n",
            "Epoch 69/100, Average Recall@20: 0.2386\n",
            "\n",
            "Epoch 70/100, Average Training Loss: 0.8173, \n",
            "Epoch 70/100, Average Test Loss: 9.0147, \n",
            "Epoch 70/100, Average Recall@20: 0.2391\n",
            "\n",
            "Epoch 71/100, Average Training Loss: 0.8147, \n",
            "Epoch 71/100, Average Test Loss: 9.0562, \n",
            "Epoch 71/100, Average Recall@20: 0.2379\n",
            "\n",
            "Epoch 72/100, Average Training Loss: 0.8254, \n",
            "Epoch 72/100, Average Test Loss: 9.0769, \n",
            "Epoch 72/100, Average Recall@20: 0.2341\n",
            "\n",
            "Epoch 73/100, Average Training Loss: 0.8127, \n",
            "Epoch 73/100, Average Test Loss: 8.9137, \n",
            "Epoch 73/100, Average Recall@20: 0.2371\n",
            "\n",
            "Epoch 74/100, Average Training Loss: 0.8266, \n",
            "Epoch 74/100, Average Test Loss: 9.0433, \n",
            "Epoch 74/100, Average Recall@20: 0.2366\n",
            "\n",
            "Epoch 75/100, Average Training Loss: 0.8184, \n",
            "Epoch 75/100, Average Test Loss: 9.0031, \n",
            "Epoch 75/100, Average Recall@20: 0.2383\n",
            "\n",
            "Epoch 76/100, Average Training Loss: 0.8194, \n",
            "Epoch 76/100, Average Test Loss: 8.9612, \n",
            "Epoch 76/100, Average Recall@20: 0.2387\n",
            "\n",
            "Epoch 77/100, Average Training Loss: 0.8068, \n",
            "Epoch 77/100, Average Test Loss: 9.0064, \n",
            "Epoch 77/100, Average Recall@20: 0.2380\n",
            "\n",
            "Epoch 78/100, Average Training Loss: 0.8048, \n",
            "Epoch 78/100, Average Test Loss: 9.0298, \n",
            "Epoch 78/100, Average Recall@20: 0.2376\n",
            "\n",
            "Epoch 79/100, Average Training Loss: 0.8161, \n",
            "Epoch 79/100, Average Test Loss: 9.0127, \n",
            "Epoch 79/100, Average Recall@20: 0.2380\n",
            "\n",
            "Epoch 80/100, Average Training Loss: 0.8191, \n",
            "Epoch 80/100, Average Test Loss: 8.9913, \n",
            "Epoch 80/100, Average Recall@20: 0.2373\n",
            "\n",
            "Epoch 81/100, Average Training Loss: 0.8066, \n",
            "Epoch 81/100, Average Test Loss: 9.0092, \n",
            "Epoch 81/100, Average Recall@20: 0.2352\n",
            "\n",
            "Epoch 82/100, Average Training Loss: 0.8173, \n",
            "Epoch 82/100, Average Test Loss: 8.9713, \n",
            "Epoch 82/100, Average Recall@20: 0.2387\n",
            "\n",
            "Epoch 83/100, Average Training Loss: 0.8099, \n",
            "Epoch 83/100, Average Test Loss: 8.9692, \n",
            "Epoch 83/100, Average Recall@20: 0.2373\n",
            "\n",
            "Epoch 84/100, Average Training Loss: 0.8239, \n",
            "Epoch 84/100, Average Test Loss: 9.0581, \n",
            "Epoch 84/100, Average Recall@20: 0.2356\n",
            "\n",
            "Epoch 85/100, Average Training Loss: 0.8180, \n",
            "Epoch 85/100, Average Test Loss: 9.0494, \n",
            "Epoch 85/100, Average Recall@20: 0.2396\n",
            "\n",
            "Epoch 86/100, Average Training Loss: 0.7930, \n",
            "Epoch 86/100, Average Test Loss: 9.0054, \n",
            "Epoch 86/100, Average Recall@20: 0.2393\n",
            "\n",
            "Epoch 87/100, Average Training Loss: 0.8031, \n",
            "Epoch 87/100, Average Test Loss: 8.9325, \n",
            "Epoch 87/100, Average Recall@20: 0.2381\n",
            "\n",
            "Epoch 88/100, Average Training Loss: 0.8038, \n",
            "Epoch 88/100, Average Test Loss: 8.9462, \n",
            "Epoch 88/100, Average Recall@20: 0.2373\n",
            "\n",
            "Epoch 89/100, Average Training Loss: 0.8131, \n",
            "Epoch 89/100, Average Test Loss: 8.9970, \n",
            "Epoch 89/100, Average Recall@20: 0.2364\n",
            "\n",
            "Epoch 90/100, Average Training Loss: 0.7997, \n",
            "Epoch 90/100, Average Test Loss: 9.0293, \n",
            "Epoch 90/100, Average Recall@20: 0.2381\n",
            "\n",
            "Epoch 91/100, Average Training Loss: 0.8177, \n",
            "Epoch 91/100, Average Test Loss: 9.0927, \n",
            "Epoch 91/100, Average Recall@20: 0.2374\n",
            "\n",
            "Epoch 92/100, Average Training Loss: 0.8055, \n",
            "Epoch 92/100, Average Test Loss: 8.9847, \n",
            "Epoch 92/100, Average Recall@20: 0.2380\n",
            "\n",
            "Epoch 93/100, Average Training Loss: 0.8045, \n",
            "Epoch 93/100, Average Test Loss: 8.9991, \n",
            "Epoch 93/100, Average Recall@20: 0.2384\n",
            "\n",
            "Epoch 94/100, Average Training Loss: 0.7963, \n",
            "Epoch 94/100, Average Test Loss: 8.9816, \n",
            "Epoch 94/100, Average Recall@20: 0.2383\n",
            "\n",
            "Epoch 95/100, Average Training Loss: 0.8032, \n",
            "Epoch 95/100, Average Test Loss: 9.0499, \n",
            "Epoch 95/100, Average Recall@20: 0.2377\n",
            "\n",
            "Epoch 96/100, Average Training Loss: 0.8178, \n",
            "Epoch 96/100, Average Test Loss: 8.9301, \n",
            "Epoch 96/100, Average Recall@20: 0.2389\n",
            "\n",
            "Epoch 97/100, Average Training Loss: 0.8002, \n",
            "Epoch 97/100, Average Test Loss: 9.0043, \n",
            "Epoch 97/100, Average Recall@20: 0.2367\n",
            "\n",
            "Epoch 98/100, Average Training Loss: 0.8045, \n",
            "Epoch 98/100, Average Test Loss: 8.9322, \n",
            "Epoch 98/100, Average Recall@20: 0.2366\n",
            "\n",
            "Epoch 99/100, Average Training Loss: 0.8121, \n",
            "Epoch 99/100, Average Test Loss: 9.0807, \n",
            "Epoch 99/100, Average Recall@20: 0.2371\n",
            "\n",
            "Epoch 100/100, Average Training Loss: 0.8177, \n",
            "Epoch 100/100, Average Test Loss: 8.9544, \n",
            "Epoch 100/100, Average Recall@20: 0.2370\n",
            "\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    # -- Main Loop --\n",
        "    running_train_loss = 0.0\n",
        "    two_towers.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        user_embedding = two_towers.user_tower.get_embedding(batch) \n",
        "        item_embedding = two_towers.item_tower.get_embedding(batch)\n",
        "\n",
        "        logits = (user_embedding @ item_embedding.T) / TEMPERATURE\n",
        "        labels = torch.arange(user_embedding.size(0)).to(device) \n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    # -- Test Loop -- \n",
        "    two_towers.eval()\n",
        "    running_test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            user_embedding = two_towers.user_tower.get_embedding(batch) \n",
        "            item_embedding = two_towers.item_tower.get_embedding(batch)\n",
        "\n",
        "            logits = (user_embedding @ item_embedding.T) / TEMPERATURE\n",
        "            labels = torch.arange(user_embedding.size(0)).to(device)\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "            running_test_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_train_loss / len(train_loader)\n",
        "    avg_test_loss = running_test_loss / len(test_loader)\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}, Average Training Loss: {avg_train_loss:.4f}, \")\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}, Average Test Loss: {avg_test_loss:.4f}, \")\n",
        "    calculate_recall_at_k(two_towers, epoch, EPOCHS)\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(two_towers.state_dict(), f\"./{MODEL_SAVE_PATH}/two_towers_epoch{epoch}_test{avg_test_loss:.2}_train{avg_train_loss:.2f}.pt\")\n",
        "\n",
        "\n",
        "writer.close()\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlE4VAFWDDGs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Recall@20 for this batch: 0.2373\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlXm69k4dJpF"
      },
      "source": [
        "### Seeing what the model recommends to me after training\n",
        "\n",
        "- It should have seen me somewhere in the training data and should have learned enough information from the other data to generalize over what I might like.\n",
        "- I will pass my username and age into the User Tower. And then conduct a dot product between my vector and the matrix of learned item embeddings to get relevance scores.\n",
        "- I will then conduct some semi-manual ranking based on removing what I have already read and other info.\n",
        "- Then I will make the final 50 recommendations for me."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'models/two_towers_epoch20_test6.4_train3.18.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Pretrained model:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m two_towers.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/two_towers_epoch20_test6.4_train3.18.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/booky/.venv/lib/python3.13/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/booky/.venv/lib/python3.13/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/booky/.venv/lib/python3.13/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'models/two_towers_epoch20_test6.4_train3.18.pt'"
          ]
        }
      ],
      "source": [
        "\n",
        "# Pretrained model:\n",
        "# two_towers.load_state_dict(torch.load(\"models/two_towers_epoch20_test6.4_train3.18.pt\", map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "KlaJclDBgeeb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/pmcslarrow/Desktop/booky/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Getting all item embeddings\n",
        "entire_dataset = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "all_item_embeddings = []\n",
        "for batch in entire_dataset:\n",
        "    item_embedding = two_towers.item_tower.get_embedding(batch)\n",
        "    all_item_embeddings.append(item_embedding)\n",
        "all_item_embeddings = torch.cat(all_item_embeddings, dim=0)\n",
        "\n",
        "# Getting a single embedding for my learned user\n",
        "paul_user_id = dataset.encoders['User-ID']['1234567890']\n",
        "paul_age = dataset.scalers['Age'].transform([[24]])[0][0]\n",
        "paul_batch = {\n",
        "    'User-ID': torch.tensor([paul_user_id], dtype=torch.long, device=device),\n",
        "    'User-Age': torch.tensor([paul_age], dtype=torch.float32, device=device)\n",
        "}\n",
        "paul_user_embedding = two_towers.user_tower.get_embedding(paul_batch) # [1 batch, 128 dimensions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([13197, 128])\n",
            "torch.Size([1, 128])\n"
          ]
        }
      ],
      "source": [
        "print(all_item_embeddings.shape)\n",
        "print(paul_user_embedding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "similarity_scores = (paul_user_embedding @ all_item_embeddings.T).squeeze()\n",
        "top_k = 1000\n",
        "top_scores, top_indices = torch.topk(similarity_scores, top_k)\n",
        "\n",
        "unique_recommendations = []\n",
        "seen_titles = set()\n",
        "read_isbns = personal_df['ISBN'].astype(str).to_list()\n",
        "\n",
        "for score, idx in zip(top_scores.detach().cpu().numpy(), top_indices.detach().cpu().numpy()):\n",
        "    row = dataset.data.iloc[idx]  # pandas row\n",
        "\n",
        "    title_idx = int(row['Book-Title'])\n",
        "    author_idx = int(row['Book-Author'])\n",
        "    isbn_idx = int(row['ISBN'])\n",
        "\n",
        "    title = dataset.reverse_encoders['Book-Title'][title_idx]\n",
        "    author = dataset.reverse_encoders['Book-Author'][author_idx]\n",
        "    isbn = dataset.reverse_encoders['ISBN'][isbn_idx]\n",
        "\n",
        "    # skip duplicates or already read books\n",
        "    if title in seen_titles or isbn in read_isbns:\n",
        "        continue\n",
        "\n",
        "    seen_titles.add(title)\n",
        "    unique_recommendations.append({\n",
        "        'title': title,\n",
        "        'author': author,\n",
        "        'score': score\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "b6dftVU0rnlP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Howards End, Author: E. M. Forster, Score: 0.5656\n",
            "Title: Selected Poems (Dover Thrift Editions), Author: Walt Whitman, Score: 0.3865\n",
            "Title: The Secret Garden, Author: Frances H. Burnett, Score: 0.3571\n",
            "Title: Spellbound, Author: Helen Glisic, Score: 0.3057\n",
            "Title: Murder on the Orient Express, Author: Agatha Christie, Score: 0.3050\n",
            "Title: Tale of Two Cities, Author: Charles Dickens, Score: 0.2960\n",
            "Title: Collected Stories, Author: Lily Brett, Score: 0.2941\n",
            "Title: Masquerade, Author: Kit Williams, Score: 0.2805\n",
            "Title: Light a Penny Candle, Author: Maeve Binchy, Score: 0.2759\n",
            "Title: David Copperfield, Author: Charles Dickens, Score: 0.2700\n",
            "Title: Lolita, Author: Vladimir Nabokov, Score: 0.2588\n",
            "Title: Promises, Author: Belva Plain, Score: 0.2537\n",
            "Title: And Then There Were None, Author: Agatha Christie, Score: 0.2283\n",
            "Title: Anthem, Author: Ayn Rand, Score: 0.2275\n",
            "Title: Homecoming, Author: Belva Plain, Score: 0.2269\n",
            "Title: Outlander, Author: DIANA GABALDON, Score: 0.2261\n",
            "Title: The Wind in the Willows, Author: Kenneth Grahame, Score: 0.2257\n",
            "Title: Terminal, Author: Robin Cook, Score: 0.2222\n",
            "Title: The Hobbit, Author: J. R. R. Tolkien, Score: 0.2197\n",
            "Title: Unforgettable, Author: Karin Kallmaker, Score: 0.2176\n",
            "Title: Island of the Blue Dolphins, Author: Scott O'Dell, Score: 0.2128\n",
            "Title: Best Friends, Author: Mary Packard, Score: 0.2101\n",
            "Title: Flesh and Blood, Author: Jonathan Kellerman, Score: 0.2051\n",
            "Title: After the Fire, Author: Belva Plain, Score: 0.2051\n",
            "Title: Rebecca, Author: Daphne Dumaurier, Score: 0.2047\n",
            "Title: The Prince, Author: Niccolo Machiavelli, Score: 0.2011\n",
            "Title: Treasure Island, Author: Robert Louis Stevenson, Score: 0.2001\n",
            "Title: Dinosaurs, Author: L. B. Halstead, Score: 0.1847\n",
            "Title: Ethan Frome, Author: Edith Wharton, Score: 0.1813\n",
            "Title: Portrait of a Lady, Author: Henry James, Score: 0.1808\n",
            "Title: Life Before Man, Author: Margaret Atwood, Score: 0.1805\n",
            "Title: The Visitor, Author: Maeve Brennan, Score: 0.1801\n",
            "Title: The Promise, Author: Donna Boyd, Score: 0.1707\n",
            "Title: Housekeeping, Author: Marilynn Robinson, Score: 0.1647\n",
            "Title: Brain, Author: Robin Cook, Score: 0.1601\n",
            "Title: Psychology, Author: David G Myers, Score: 0.1599\n",
            "Title: The Kiss, Author: Danielle Steel, Score: 0.1584\n",
            "Title: Presumed Innocent, Author: Scott Turow, Score: 0.1524\n",
            "Title: A Christmas Carol, Author: Charles Dickens, Score: 0.1512\n",
            "Title: Life and Teaching of the Masters of the Far East (Life &amp; Teaching of the Masters of the Far East), Author: Baird T. Spalding, Score: 0.1462\n",
            "Title: Sons and Lovers, Author: D.H. Lawrence, Score: 0.1431\n",
            "Title: The Edible Woman, Author: MARGARET ATWOOD, Score: 0.1424\n",
            "Title: Ivanhoe, Author: Walter, Sir Scott, Score: 0.1420\n",
            "Title: Scruples, Author: Judith Krantz, Score: 0.1419\n",
            "Title: Nemesis, Author: Agatha Christie, Score: 0.1413\n",
            "Title: Twins, Author: Caroline B. Cooney, Score: 0.1360\n",
            "Title: Once and Future King, Author: T. H. White, Score: 0.1348\n",
            "Title: Fever, Author: Robin Cook, Score: 0.1231\n",
            "Title: The Passion, Author: Jeanette Winterson, Score: 0.1225\n",
            "Title: Ireland, Author: Catharina Day, Score: 0.1219\n",
            "Title: Outsider, Author: Albert Camus, Score: 0.1201\n",
            "Title: Black Water, Author: Joyce Carol Oates, Score: 0.1176\n",
            "Title: Still Waters, Author: Tami Hoag, Score: 0.1176\n",
            "Title: I Know Why the Caged Bird Sings, Author: Maya Angelou, Score: 0.1171\n",
            "Title: Remember When, Author: Nora Roberts, Score: 0.1156\n",
            "Title: Fear, Author: Francine Pascal, Score: 0.1125\n",
            "Title: Red Dragon, Author: Thomas Harris, Score: 0.1115\n",
            "Title: Archangel, Author: Sharon Shinn, Score: 0.1110\n",
            "Title: I Never Promised You a Rose Garden, Author: Hannah Green, Score: 0.1096\n",
            "Title: Selected Poems, Author: Galway Kinnell, Score: 0.1085\n",
            "Title: Hawaii, Author: James A. Michener, Score: 0.1078\n",
            "Title: Second Chances, Author: Alice Adams, Score: 0.1067\n",
            "Title: Adventures of Huckleberry Finn, Author: Mark Twain, Score: 0.1066\n",
            "Title: The Screwtape Letters, Author: C. S. Lewis, Score: 0.1030\n",
            "Title: Blind Spot, Author: Judy Mercer, Score: 0.1028\n",
            "Title: Jade, Author: Betty Brooks, Score: 0.1009\n",
            "Title: The Adventures of Tom Sawyer, Author: Mark Twain, Score: 0.0978\n",
            "Title: Birds of Prey, Author: Wilbur Smith, Score: 0.0970\n",
            "Title: God Emperor of Dune (Dune Chronicles, Book 4), Author: Frank Herbert, Score: 0.0955\n",
            "Title: Restoree, Author: Anne McCaffrey, Score: 0.0927\n",
            "Title: All Creatures Great and Small, Author: James Herriot, Score: 0.0917\n",
            "Title: Jurassic Park, Author: Walter Simonson, Score: 0.0905\n",
            "Title: Payback, Author: Donald E. Westlake, Score: 0.0877\n",
            "Title: Dracula, Author: Bram Stoker, Score: 0.0874\n",
            "Title: Lip Service, Author: Russell Lucas, Score: 0.0860\n",
            "Title: Swiss Family Robinson, Author: Johann Wyss, Score: 0.0858\n",
            "Title: The Velveteen Rabbit, Author: Margaret Williams, Score: 0.0849\n",
            "Title: The Scarlet Letter, Author: Nathaniel Hawthorne, Score: 0.0843\n",
            "Title: A Little Princess, Author: Frances Hodgson Burnett, Score: 0.0842\n",
            "Title: Little Women, Author: Louisa May Alcott, Score: 0.0839\n",
            "Title: Women in Love, Author: D.H. Lawrence, Score: 0.0830\n",
            "Title: Dune, Author: Frank Herbert, Score: 0.0811\n",
            "Title: The Other Woman, Author: Jill McGown, Score: 0.0807\n",
            "Title: The Firm, Author: John Grisham, Score: 0.0792\n",
            "Title: To the Lighthouse, Author: Virginia Woolf, Score: 0.0776\n",
            "Title: Back Roads, Author: Tawni O'Dell, Score: 0.0774\n",
            "Title: The President's Daughter, Author: Jack Higgins, Score: 0.0774\n",
            "Title: Savannah, Author: Eugenia Price, Score: 0.0769\n",
            "Title: Vanished, Author: Mary McGarry Morris, Score: 0.0757\n",
            "Title: Beloved, Author: Stella Cameron, Score: 0.0742\n",
            "Title: Mythology, Author: Edith Hamilton, Score: 0.0741\n",
            "Title: Rainbow Six, Author: Tom Clancy, Score: 0.0729\n",
            "Title: Siddhartha, Author: Hermann Hesse, Score: 0.0728\n",
            "Title: Wild Horses, Author: Dick Francis, Score: 0.0723\n",
            "Title: Madame Bovary, Author: Gustave Flaubert, Score: 0.0717\n",
            "Title: Misery, Author: Stephen King, Score: 0.0694\n",
            "Title: The Hitchhiker's Guide to the Galaxy, Author: Douglas Adams, Score: 0.0658\n",
            "Title: Kim, Author: Rudyard Kipling, Score: 0.0657\n",
            "Title: All Things Bright and Beautiful, Author: James Herriot, Score: 0.0654\n",
            "Title: The Fellowship of the Ring (The Lord of the Rings, Part 1), Author: J.R.R. Tolkien, Score: 0.0635\n"
          ]
        }
      ],
      "source": [
        "for rec in unique_recommendations[:100]:\n",
        "    print(f\"Title: {rec['title']}, Author: {rec['author']}, Score: {rec['score']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMmJhTtO2FCD1vDMILVH8qi",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
