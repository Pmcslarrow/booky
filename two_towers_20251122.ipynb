{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3c5igSoT9JS"
      },
      "source": [
        "## Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 458,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kVvdwqPSMaoM",
        "outputId": "6cb35f12-09ae-4775-a88a-15186337fc78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: no matches found: kagglehub[pandas-datasets]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kagglehub[pandas-datasets]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 459,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "slTaZzDAMLUM",
        "outputId": "afbda2b2-2ecc-4b8a-bdbc-04d7531c6d0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/0b/t_zq87x958lc5wvb065nb89m0000gn/T/ipykernel_53967/628222823.py:17: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  books_df = kagglehub.load_dataset(\n",
            "/Users/pmcslarrow/Desktop/booky/.venv/lib/python3.13/site-packages/kagglehub/pandas_datasets.py:91: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  result = read_function(\n",
            "/var/folders/0b/t_zq87x958lc5wvb065nb89m0000gn/T/ipykernel_53967/628222823.py:23: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  ratings_df = kagglehub.load_dataset(\n",
            "/var/folders/0b/t_zq87x958lc5wvb065nb89m0000gn/T/ipykernel_53967/628222823.py:29: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  users_df = kagglehub.load_dataset(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from collections import Counter\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "books_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Books.csv\",\n",
        ")\n",
        "\n",
        "ratings_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Ratings.csv\",\n",
        ")\n",
        "\n",
        "users_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Users.csv\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc3cHdnBaBSd"
      },
      "source": [
        "### Only keeping books with more than three ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 460,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX05tHiWbIKN",
        "outputId": "c01cace5-e537-4c97-e2b2-ee52262529ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(271360, 8)\n"
          ]
        }
      ],
      "source": [
        "# print(books_df.shape)\n",
        "# books_df = books_df[books_df.groupby('Book-Title')['Book-Title'].transform('count') > 5]\n",
        "print(books_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MJPgJ-ea1z5"
      },
      "source": [
        "### Joining Books, Ratings, and Users tables together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 461,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "93r6ilG_NQPw",
        "outputId": "95dd915a-1b0a-4b51-c541-01b5beadc396"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    264742.000000\n",
              "mean          7.738848\n",
              "std           1.813809\n",
              "min           1.000000\n",
              "25%           7.000000\n",
              "50%           8.000000\n",
              "75%           9.000000\n",
              "max          10.000000\n",
              "Name: Book-Rating, dtype: float64"
            ]
          },
          "execution_count": 461,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_ratings_books = pd.merge(ratings_df, books_df, on=\"ISBN\", how='inner')\n",
        "df = pd.merge(df_ratings_books, users_df, on='User-ID')\n",
        "df['User-ID'] = df['User-ID'].astype(str)\n",
        "df['Year-Of-Publication'] = pd.to_numeric(df['Year-Of-Publication'], errors='coerce')\n",
        "df = df.dropna(subset=['Year-Of-Publication'])\n",
        "df = df.dropna(subset=['Age'])\n",
        "df = df[df['Book-Rating'] > 0]\n",
        "df = df[df['Age'] <= 100]\n",
        "df = df[df['Year-Of-Publication'] > 0]\n",
        "df['User-Age'] = pd.cut(\n",
        "    df['Age'],\n",
        "    bins=[0, 18, 25, 35, 50, 100],\n",
        "    labels=['<18', '18-25', '26-35', '36-50', '50+'],\n",
        "    right=False\n",
        ")\n",
        "df['Book-Year-Of-Publication'] = pd.cut(\n",
        "    df['Year-Of-Publication'],\n",
        "    bins=[0, 1950, 1980, 2000, 2010, 2020, 2050],\n",
        "    labels=['pre-1950', '1950-1979', '1980-1999', '2000-2009', '2010-2019', '2020+'],\n",
        "    right=False\n",
        ")\n",
        "df['Book-Rating'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7i4lqEGb_pr"
      },
      "source": [
        "### Combining my own data into the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 462,
      "metadata": {
        "id": "8tSGkmikGSCI"
      },
      "outputs": [],
      "source": [
        "personal_df = pd.read_csv(\"./fine-tuning-book-set.txt\")\n",
        "end_index = len(df)\n",
        "df = pd.concat([df, personal_df], ignore_index=True, sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 463,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "t2gm3-hvb7L_",
        "outputId": "cfa6697b-8a51-41e4-9678-54a351695553"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User-ID</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>Book-Rating</th>\n",
              "      <th>Book-Title</th>\n",
              "      <th>Book-Author</th>\n",
              "      <th>Year-Of-Publication</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>Age</th>\n",
              "      <th>Book-Year-Of-Publication</th>\n",
              "      <th>User-Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1234567890</td>\n",
              "      <td>451521951</td>\n",
              "      <td>5</td>\n",
              "      <td>The Count of Monte Cristo</td>\n",
              "      <td>Alexandre Dumas</td>\n",
              "      <td>1844</td>\n",
              "      <td>Signet Book</td>\n",
              "      <td>23</td>\n",
              "      <td>pre-1950</td>\n",
              "      <td>18-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1234567890</td>\n",
              "      <td>684813637</td>\n",
              "      <td>5</td>\n",
              "      <td>1776</td>\n",
              "      <td>David McCullough</td>\n",
              "      <td>2005</td>\n",
              "      <td>Simon &amp; Schuster</td>\n",
              "      <td>23</td>\n",
              "      <td>2000-2009</td>\n",
              "      <td>18-25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      User-ID       ISBN  Book-Rating                 Book-Title  \\\n",
              "0  1234567890  451521951            5  The Count of Monte Cristo   \n",
              "1  1234567890  684813637            5                       1776   \n",
              "\n",
              "        Book-Author  Year-Of-Publication         Publisher  Age  \\\n",
              "0   Alexandre Dumas                 1844       Signet Book   23   \n",
              "1  David McCullough                 2005  Simon & Schuster   23   \n",
              "\n",
              "  Book-Year-Of-Publication User-Age  \n",
              "0                 pre-1950    18-25  \n",
              "1                2000-2009    18-25  "
            ]
          },
          "execution_count": 463,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "personal_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 464,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User-ID</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>Book-Rating</th>\n",
              "      <th>Book-Title</th>\n",
              "      <th>Book-Author</th>\n",
              "      <th>Year-Of-Publication</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>Image-URL-S</th>\n",
              "      <th>Image-URL-M</th>\n",
              "      <th>Image-URL-L</th>\n",
              "      <th>Location</th>\n",
              "      <th>Age</th>\n",
              "      <th>User-Age</th>\n",
              "      <th>Book-Year-Of-Publication</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>276729</td>\n",
              "      <td>052165615X</td>\n",
              "      <td>3</td>\n",
              "      <td>Help!: Level 1</td>\n",
              "      <td>Philip Prowse</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>Cambridge University Press</td>\n",
              "      <td>http://images.amazon.com/images/P/052165615X.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/052165615X.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/052165615X.0...</td>\n",
              "      <td>rijeka, n/a, croatia</td>\n",
              "      <td>16.0</td>\n",
              "      <td>&lt;18</td>\n",
              "      <td>1980-1999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>276729</td>\n",
              "      <td>0521795028</td>\n",
              "      <td>6</td>\n",
              "      <td>The Amsterdam Connection : Level 4 (Cambridge ...</td>\n",
              "      <td>Sue Leather</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>Cambridge University Press</td>\n",
              "      <td>http://images.amazon.com/images/P/0521795028.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/0521795028.0...</td>\n",
              "      <td>http://images.amazon.com/images/P/0521795028.0...</td>\n",
              "      <td>rijeka, n/a, croatia</td>\n",
              "      <td>16.0</td>\n",
              "      <td>&lt;18</td>\n",
              "      <td>2000-2009</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  User-ID        ISBN  Book-Rating  \\\n",
              "0  276729  052165615X            3   \n",
              "1  276729  0521795028            6   \n",
              "\n",
              "                                          Book-Title    Book-Author  \\\n",
              "0                                     Help!: Level 1  Philip Prowse   \n",
              "1  The Amsterdam Connection : Level 4 (Cambridge ...    Sue Leather   \n",
              "\n",
              "   Year-Of-Publication                   Publisher  \\\n",
              "0               1999.0  Cambridge University Press   \n",
              "1               2001.0  Cambridge University Press   \n",
              "\n",
              "                                         Image-URL-S  \\\n",
              "0  http://images.amazon.com/images/P/052165615X.0...   \n",
              "1  http://images.amazon.com/images/P/0521795028.0...   \n",
              "\n",
              "                                         Image-URL-M  \\\n",
              "0  http://images.amazon.com/images/P/052165615X.0...   \n",
              "1  http://images.amazon.com/images/P/0521795028.0...   \n",
              "\n",
              "                                         Image-URL-L              Location  \\\n",
              "0  http://images.amazon.com/images/P/052165615X.0...  rijeka, n/a, croatia   \n",
              "1  http://images.amazon.com/images/P/0521795028.0...  rijeka, n/a, croatia   \n",
              "\n",
              "    Age User-Age Book-Year-Of-Publication  \n",
              "0  16.0      <18                1980-1999  \n",
              "1  16.0      <18                2000-2009  "
            ]
          },
          "execution_count": 464,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 465,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXl51Y5OQM0e",
        "outputId": "e537bd70-122f-493a-b1e0-f28183f564fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 466,
      "metadata": {
        "id": "hzV4YkTNPSuc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import  StandardScaler\n",
        "\n",
        "# User Tower -- User-ID, Age\n",
        "# Item Tower -- ISBN, Book-Title, Book-Author, Publisher, Year-Of-Publication\n",
        "\n",
        "class BookRecommenderDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class for book recommendation tasks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataframe : pd.DataFrame\n",
        "        The input data containing user, item, and possibly interaction features.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        The processed version of the input dataframe.\n",
        "    encoders : dict\n",
        "        A dictionary mapping column names to fitted label encoders.\n",
        "    reverse_encoders : dict\n",
        "        A dictionary mapping column names to reverse label encoders (index to label).\n",
        "    scalers : dict\n",
        "        A dictionary mapping column names to fitted scalers for numerical features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.encoders = {} # {'Column name': {'value': idx, ...}, ...}\n",
        "        self.reverse_encoders = {} # {'Column name': {idx: 'value', ...}, ...}\n",
        "        self.data = data\n",
        "        self.preprocess(self.data)\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        self.encode_information()\n",
        "\n",
        "    def encode_information(self):\n",
        "        \"\"\"\n",
        "        Maps {key: index} pairs and StandardScaler for real valued numbers\n",
        "        \"\"\"\n",
        "        label_encoders = ['User-ID', 'ISBN', 'Book-Author', 'Book-Title', 'Publisher', \"User-Age\", \"Book-Year-Of-Publication\"]\n",
        "\n",
        "        for col in label_encoders:\n",
        "            unique_vals = self.data[col].astype(str).unique()\n",
        "            self.encoders[col] = {val: idx + 1 for idx, val in enumerate(unique_vals)}\n",
        "            self.reverse_encoders[col] = {idx + 1: val for idx, val in enumerate(unique_vals)}\n",
        "            self.data[col] = self.data[col].astype(str).map(self.encoders[col]).fillna(0).astype(int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        return {\n",
        "            \"User-ID\": torch.tensor(row[\"User-ID\"], dtype=torch.long),\n",
        "            \"User-Age\": torch.tensor(row[\"User-Age\"], dtype=torch.long),\n",
        "            \"Book-ISBN\": torch.tensor(row[\"ISBN\"], dtype=torch.long),\n",
        "            \"Book-Title\": torch.tensor(row[\"Book-Title\"], dtype=torch.long),\n",
        "            \"Book-Author\": torch.tensor(row[\"Book-Author\"], dtype=torch.long),\n",
        "            \"Book-Publisher\": torch.tensor(row[\"Publisher\"], dtype=torch.long),\n",
        "            \"Book-Year-Of-Publication\": torch.tensor(row[\"Book-Year-Of-Publication\"], dtype=torch.long),\n",
        "            \"Book-Title-Text\": row['Book-Title']\n",
        "        }\n",
        "\n",
        "\n",
        "dataset = BookRecommenderDataset(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 467,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNCVwwNWwUc7",
        "outputId": "8ab1e53f-7cc6-41bc-e600-9a79d9134290"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'User-ID': tensor(1),\n",
              " 'User-Age': tensor(1),\n",
              " 'Book-ISBN': tensor(1),\n",
              " 'Book-Title': tensor(1),\n",
              " 'Book-Author': tensor(1),\n",
              " 'Book-Publisher': tensor(1),\n",
              " 'Book-Year-Of-Publication': tensor(1),\n",
              " 'Book-Title-Text': np.int64(1)}"
            ]
          },
          "execution_count": 467,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 468,
      "metadata": {
        "id": "5UuengF0Tojk"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.7 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_data, batch_size=512, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=512, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 469,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vznGD0IGTxA7",
        "outputId": "2b2d6fdb-f25e-40fc-b444-9d1218f410aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'User-ID': tensor([ 1949, 18398,  1872, 25388, 34721, 22661, 20926, 30338, 30104, 25629,\n",
              "         33762, 19403,  8470, 34671, 16164, 11578, 34617, 19246, 35279, 26644,\n",
              "         15631, 10588,  9043, 20884,  3655, 27047,  2143, 21033, 14361,  4529,\n",
              "          5757,  8275, 22010, 18864, 32320,  1561, 29155, 32726,  4893, 16416,\n",
              "          8119,  2823, 14361, 18178,    33,  2708, 14361, 21048, 16066, 10214,\n",
              "         37103, 15224, 13774, 38990, 25491, 11346, 14713, 22156, 38666, 18048,\n",
              "         37607, 24252, 13351, 34428, 10396,  3865, 25277, 13337,  7317,  1744,\n",
              "         15269,  5508, 32324, 39240, 22326,  8752,  8112,  9663, 20973,  7637,\n",
              "         16505, 29769, 20847, 20340, 16769, 28434, 29338, 12931, 21283,  7151,\n",
              "         29420,  6931, 12662, 26493, 17431,   462, 25629, 38800, 20935, 24267,\n",
              "          6945,  8948, 39336, 38052, 10210, 34315, 13555, 16658,  7821,  9603,\n",
              "         39024, 38369, 26878, 10920, 32594, 20105,  8965, 35494,  9477, 31386,\n",
              "         35568, 30200,  6456,   319, 30621, 12261,  2579,  4512, 22010,  6012,\n",
              "         31053,  8336, 14361, 10289, 39758, 22050, 29088, 38702, 18771, 17275,\n",
              "         33762, 12494, 13880, 24802,  5038, 19667, 20469, 22302, 24195, 18034,\n",
              "           516, 20957, 18402,  8018, 31611, 37893, 19149, 17608, 11183, 22452,\n",
              "         17812, 18533,  3625, 30340, 14361, 17974, 22480, 15757, 38262, 26476,\n",
              "         18681,  4309, 39070, 39493, 37074, 22302,  4495, 31068, 31256, 28911,\n",
              "         23966, 18163,  7702, 22417, 28154,  9609, 18502, 14361, 35744, 35802,\n",
              "         16072, 39306, 35634,  8863,  3852, 33977, 26947, 32632, 14020, 35704,\n",
              "         24691, 36741,   888,  9117, 38055, 38051, 33796,   231, 12820, 18305,\n",
              "         26947, 20117, 19057, 19174, 22533, 13220, 19187, 20893, 17614, 13647,\n",
              "         25157, 25406, 39535,  1184,  7975, 39062, 18035, 10017, 25837,  8380,\n",
              "         15760,  6303,  3865,  8327, 16025, 35241,  2207, 16311, 29414, 14361,\n",
              "          2136, 30978,  8593, 14420, 21305, 32058, 22560, 18771, 27627, 13908,\n",
              "          2852, 19384,  3978, 37221, 17216,   905, 36690, 17567, 10330, 34237,\n",
              "         22087,  4961, 29816,  9061, 35557,  5757, 14693, 22302,  6575, 29481,\n",
              "          2451, 28280, 21911, 20279, 13495, 16158,   202,  2175, 27008, 38448,\n",
              "         24043, 22942, 22302, 21272, 14969, 24027, 23139, 33865, 33460, 33118,\n",
              "         29973, 14547,  1472, 27489, 22302, 31015, 24466, 16331, 32002, 22672,\n",
              "         32343, 30375,   356, 38262, 14361,  7178,  3448, 28434,  8900, 36752,\n",
              "         14361,  9673, 26092,  8613,  5038, 11283, 18305, 38970, 16164,  2957,\n",
              "         35245, 38508, 10381, 11407, 10175, 34322, 26953, 15389, 31053,  4064,\n",
              "         30150, 28412, 17223, 12654, 34197, 15719, 23987, 12509, 10882, 26572,\n",
              "         33834, 29116, 33344, 16697,  6214, 24582,  1272, 30892,  7144, 10359,\n",
              "         27855, 10882, 35746, 23835, 39591, 24766, 13971, 15726, 17528, 34553,\n",
              "         35917, 19556, 31053, 11927, 34828, 32699, 38804,  3507,  8701, 21897,\n",
              "         11735, 35063, 18773, 16812, 18435, 31258, 21924, 22504, 29243, 25253,\n",
              "         25002, 24196, 29508, 36249, 18660, 25457, 22847,  5472, 30715,  6139,\n",
              "         15449, 28097,  9626, 27855, 26407,  1136, 39426, 35935, 34546,   391,\n",
              "         30638, 38262,  8168, 28987, 20013, 29262, 27576, 15959,  7637,  4556,\n",
              "         20850,  5235,  9718,  3067, 33373, 10186, 16052, 13647, 20065, 11506,\n",
              "         29874, 38172, 36556, 26164, 35100, 14212, 16842,  8330,  9160, 11573,\n",
              "         39092,  2204, 29990, 31504, 12852, 15180, 38765, 10175, 35381, 37265,\n",
              "         21983, 17911, 24271, 27804, 33116, 35566, 15223, 22695, 31342, 25206,\n",
              "         31490, 18937, 36887, 26738, 39092,  5125,  7745,  8953,  9959, 11631,\n",
              "         19829, 14361,  9631,  4343, 20057, 30509, 14966, 34183,  5955,  6496,\n",
              "          2972, 23590, 16910, 12174,  2402, 12306,  8900, 11840, 14584,  7734,\n",
              "          9128, 33122, 19309, 38528, 34837, 13343, 23577,  2531,  3804, 14361,\n",
              "         31053, 22661,  1322, 35408,  8912, 37431, 34444, 11736,   374,  5269,\n",
              "         12317,  1839, 10346,  7637, 19052, 24828, 33899, 24130, 14104, 35990,\n",
              "         32102, 11462]),\n",
              " 'User-Age': tensor([2, 2, 5, 2, 3, 3, 2, 4, 3, 2, 3, 2, 3, 4, 3, 2, 2, 5, 2, 2, 3, 2, 4, 5,\n",
              "         4, 3, 5, 3, 5, 2, 2, 2, 5, 2, 3, 4, 4, 2, 1, 3, 3, 4, 5, 3, 4, 3, 5, 2,\n",
              "         4, 3, 5, 5, 3, 5, 4, 2, 2, 4, 4, 2, 5, 4, 3, 2, 4, 2, 4, 2, 2, 2, 2, 3,\n",
              "         5, 4, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 3, 3, 5, 3, 3, 3, 4,\n",
              "         2, 3, 2, 3, 1, 5, 2, 2, 3, 3, 5, 2, 2, 2, 4, 3, 4, 4, 3, 2, 3, 4, 4, 3,\n",
              "         4, 3, 3, 2, 5, 5, 2, 4, 5, 2, 2, 3, 5, 3, 3, 3, 2, 3, 5, 3, 3, 5, 3, 3,\n",
              "         4, 2, 3, 3, 3, 3, 3, 2, 4, 3, 4, 4, 2, 3, 5, 3, 2, 2, 3, 3, 5, 3, 3, 1,\n",
              "         2, 4, 3, 4, 3, 3, 2, 3, 5, 4, 4, 3, 2, 2, 3, 5, 2, 3, 3, 5, 1, 3, 2, 4,\n",
              "         3, 3, 3, 3, 5, 2, 5, 5, 3, 3, 5, 2, 4, 2, 2, 2, 2, 3, 5, 3, 2, 3, 2, 2,\n",
              "         3, 2, 3, 5, 2, 4, 1, 2, 2, 3, 2, 5, 2, 4, 2, 4, 2, 5, 5, 2, 2, 3, 4, 5,\n",
              "         2, 4, 1, 2, 2, 5, 4, 5, 2, 2, 4, 4, 2, 3, 4, 3, 4, 4, 1, 3, 2, 5, 4, 2,\n",
              "         5, 2, 2, 3, 5, 2, 1, 2, 2, 3, 2, 3, 2, 4, 2, 2, 2, 5, 3, 2, 2, 5, 3, 3,\n",
              "         2, 4, 2, 4, 3, 3, 3, 2, 5, 4, 2, 3, 1, 3, 4, 2, 5, 4, 2, 3, 3, 5, 5, 4,\n",
              "         2, 5, 4, 2, 3, 2, 3, 3, 3, 5, 5, 2, 3, 2, 5, 2, 2, 3, 4, 2, 2, 2, 2, 3,\n",
              "         3, 1, 4, 3, 3, 2, 4, 2, 5, 4, 5, 5, 3, 2, 3, 4, 1, 2, 2, 3, 3, 2, 3, 5,\n",
              "         5, 3, 2, 2, 4, 3, 1, 2, 3, 2, 2, 3, 3, 5, 3, 1, 3, 3, 3, 4, 2, 2, 2, 3,\n",
              "         2, 3, 3, 5, 2, 5, 2, 2, 3, 3, 2, 3, 4, 3, 2, 5, 3, 2, 2, 4, 2, 5, 2, 5,\n",
              "         2, 3, 2, 3, 2, 1, 5, 2, 2, 5, 4, 3, 5, 3, 2, 3, 3, 2, 2, 4, 5, 3, 2, 2,\n",
              "         3, 2, 4, 5, 5, 3, 3, 2, 1, 5, 4, 4, 5, 3, 2, 3, 4, 2, 5, 4, 4, 2, 2, 4,\n",
              "         2, 2, 4, 3, 3, 5, 3, 5, 3, 5, 1, 3, 4, 3, 5, 4, 2, 3, 4, 5, 3, 2, 3, 4,\n",
              "         5, 2, 3, 3, 3, 1, 2, 2, 5, 5, 2, 3, 2, 3, 5, 2, 2, 3, 3, 2, 3, 4, 5, 2,\n",
              "         2, 3, 5, 4, 2, 1, 2, 1]),\n",
              " 'Book-ISBN': tensor([  2916,  21224,   1622,   8199,   5163,  26219,  37651,  52709,   6559,\n",
              "          88185,  11515,  72314,   1190, 108035,   1999,   3753, 107989,  57022,\n",
              "           5041,  90628,  21162,   1705,  10223,  27027,  16490,  91862,  10091,\n",
              "          12397,  55919,  20232,  25590,  34297,  78225,  71191,   1303,    558,\n",
              "          96534,   2399,   2395,    424,  33697,   3776,  56753,  19060,    154,\n",
              "           1801,  56775,  76309,   5954,  41407,   5160,   1608,   2570,  69741,\n",
              "          87790,  45032,   6531,  21108,  11141,  69038, 113835,   1662,  51079,\n",
              "          22933,   2206,  17580,  67921,  51052,  25049,   8391,    701,   8181,\n",
              "           2239,    772,    553,   4076,   1516,   7521,  69556,  31824,  65059,\n",
              "           1994,  28088,  74491,  30436,  84668,  10836,  47778,   6798,   4005,\n",
              "          97030,  29322,  19636,   9230,   3058,   1962,  19417, 115832,  48456,\n",
              "          84834,  29613,   2303, 116771,  17565,  19006,  15038,  51578,  41513,\n",
              "           7904,   7354, 116324,  65591,   4045,  43669,  37009,  42048,   5725,\n",
              "          42255,  27458,   6577,   1562,  84272,  27981,   1299,  95663,   8107,\n",
              "           5757,    865,  37368,   8175,  57160,   6541,  58610,  41785,  39575,\n",
              "          78287,   6942,  18376,  70772,  20565,  43748,  48377,  25751,  64103,\n",
              "           1906,  72823,   8144,   5379,  15634,  68972,   2134,  31121,  69769,\n",
              "           4023,   1597,  37497,  11845,   2017,  30273,  79986,   7185,  10299,\n",
              "           3671,  37713,  58539,  68622,  80017,   2759,   6972,   7938,   3254,\n",
              "          19226,   1160,   1751,  46921,  78938,   6900,  98833, 100810,  96039,\n",
              "          84056,  69259,  32235,  79911,  40542,  34818,  70055,  58076,    394,\n",
              "         110763,   2708,    280,  44928,   8254,  17433, 106558,  80172,  72856,\n",
              "          12381, 110077,  85896, 106532,   4186,    873,  60918,  11151,  65589,\n",
              "           1079,  33875,  69611,  64104,  73886,   1042,  71838,   6246,   4972,\n",
              "           7345,   1754,  32891,   3681,  12860,   2831,  23951,   5495,   6201,\n",
              "             97,   5042,   4137,  36536,  35468,  23163,    985,    440,  35040,\n",
              "          63828,   3348,   7997,  64685,  59559,  56032,   1588, 100077,   1161,\n",
              "           2248,  41596, 102365,  80225,  17712,  39839,  53199,     64,   8577,\n",
              "          18010,  10369,  56298,   4253, 112318,   2769,  41901,   2101,   4884,\n",
              "          19177,  97961,   2994,  37542,  25560,  28923,  79656,  28394,   1785,\n",
              "          11390,  11994,  37612,  74370,  44859,    674,    902,   6662,  91629,\n",
              "         115274,  84231,  81395,  15414,  55563,  61251,  84203,  66018, 106247,\n",
              "            876, 104782,  37162,  59393,   7102,  51266,  79455, 100111,  85007,\n",
              "          64822,  69270,  80655,  83136,  16256,   1425, 115052,  57924,  30235,\n",
              "          15741,  94403,  15395,   8925,  58075,  39701,   7806,  36252,  22248,\n",
              "          16430,   9062,  12410,  64319,  10907, 102911,   2890,  42168,   1772,\n",
              "          29893,   2629,  91542,  15112, 100233,  18478,  45516,  11999,  65595,\n",
              "          48943,   4130,  63062,  84111,  48431,   4091,  90375,  25261,   8776,\n",
              "           2759,  35346,  14291,  85266,   5684,  99986,  30086,  32917,  93738,\n",
              "           1785,  25215,  10499,  44366,  86195,  51598,  15500,  67670, 107617,\n",
              "         103505,  27013,  77384,  16250,  32340,  10330,  31875,  15949,  36548,\n",
              "          21383,    667,   6034,  70927,  66123,   2003,  27900,   4869,  80099,\n",
              "          69315,  68423,  17986,   1418,  23290,  72303,   3748,  70453,  81152,\n",
              "           4619,  64046,   1442,  35404,  14955,   3704,  93748,   3463,    173,\n",
              "              4,      3, 107582,   1556,  99546,  41560,   7963,   3459,  73583,\n",
              "           4581,   3259,  63666,  31758,  20330,  33304,  23293,   5093,  14578,\n",
              "           5632,  41301,   1292,  29474,  21603,   1429,  25483,    609, 112053,\n",
              "           8982, 109126,  54276,  17291,   2033,  36549,  45716,  21727,   3165,\n",
              "          98251,   2559,   8207,  18115, 115771,  15612, 109571,   7170,  78169,\n",
              "          18869,  48310,  93485,  29393, 109869,  61768,    816, 101053,   3700,\n",
              "           2202,  71347,   5365,  90766, 101140,  23049,  20401,   1282,  14252,\n",
              "          46059,  73241,  43472,  39581,   6502,  65931,  99251,   6076, 106891,\n",
              "          26323,  28171,  14288,   9314,  46862,  47586,  11148,   4059,  37063,\n",
              "          46864,  32396,  32343,  37948,      9,  15542,   3819, 108666,  51074,\n",
              "          10328,  11758,  17357,  54967,   5858,  80456,   2141,  12981,   6544,\n",
              "           2738,  19183,   8835,   1494,  10097,  48078,   1471,   4897,  31667,\n",
              "          11831,  39284, 106341,  63193,  54000,  78279, 102431,  45291]),\n",
              " 'Book-Title': tensor([  2849,  20013,   1587,   7886,   5018,  24613,  34764,  48842,   6338,\n",
              "          80851,  11015,  66581,   1183,  98695,   1956,   3653,  98654,  35649,\n",
              "           4904,  83010,  19968,    365,   9790,   1338,  15628,  84145,   9666,\n",
              "          11833,  51810,  19124,   3371,  32044,  71863,  65584,   1289,    287,\n",
              "          88353,   2344,   2340,    424,  31509,   3676,  52562,  18033,    154,\n",
              "           1763,  52582,  70137,   5756,  38558,   5015,   1573,   2512,  64289,\n",
              "          80506,  27582,   6312,   1355,  10654,  63658, 103921,   1627,  47369,\n",
              "          21589,   2158,  16650,  62657,  47342,  23542,   8069,    698,   7869,\n",
              "           2188,    769,    552,   3963,   1490,   7237,  64126,  18799,  60074,\n",
              "           1951,    773,  68519,  28513,  77673,  10363,  44367,   6565,   3896,\n",
              "          88796,  27508,  18584,   8852,   2987,   1921,  18376, 105695,  44986,\n",
              "          77823,  27776,   2250, 106544,  11835,  17985,  14295,  47811,  38653,\n",
              "           7604,   7086, 106129,  12733,   3934,  40605,  34560,  39140,   5539,\n",
              "           9735,  25772,   6354,   1534,  16396,  26263,    827,  87576,   7797,\n",
              "           5571,    862,  14677,   7863,  52922,   6321,  54275,  38899,  36913,\n",
              "          71919,   6700,  17411,  26639,  17420,  40682,  44913,  15219,  59244,\n",
              "           1867,  67035,   7834,   5217,   6923,  63596,   2089,  29146,  64311,\n",
              "           3914,   1567,  35007,  11321,   1974,  28365,  73467,   6930,   9863,\n",
              "           3575,  35204,  54208,  63291,  73495,   2697,   6730,   7634,   3171,\n",
              "          18187,   1153,   1714,  43590,  72510,   6662,  90424,  92221,  87909,\n",
              "          77136,  31910,  30178,  73400,  37785,  32536,  64566,  53782,    394,\n",
              "          51130,   2646,    280,  41770,   7939,  16511,  97382,  73629,  67068,\n",
              "          11817, 100550,  78776,  97360,   4068,    870,  56371,   6900,  16393,\n",
              "           1074,   7446,  64179,  59245,  67983,   1037,  66162,   3080,   4836,\n",
              "           7078,   1717,  10097,   3585,  12257,   2768,  22536,   5324,   3478,\n",
              "             97,   4905,   4020,  34142,  33159,  14202,    980,    440,  32749,\n",
              "          58998,   3264,   7688,  59740,  55134,  51919,   1559,  28130,   1154,\n",
              "           2197,  38726,  93622,  73676,  16781,  37141,  49293,     64,   8243,\n",
              "           5191,   9925,  52144,   4132, 102551,   2707,  39006,   2058,   4749,\n",
              "          18142,  89639,   2743,  35048,  14471,  27143,  73162,  26649,   1747,\n",
              "          10894,  11457,  35111,  68410,  41708,    671,    899,   6433,  83927,\n",
              "         105212,  77287,  74713,  13754,  51461,  56679,  77260,  60955,  97105,\n",
              "            873,  95791,  34697,  54976,   6854,  19887,  72976,  91576,  77977,\n",
              "          59857,  63870,  11846,  76270,  12863,   1406, 105010,  53641,  28331,\n",
              "          14937,  86437,  14617,   8562,  53781,  37021,   7510,  33884,  20963,\n",
              "          15569,   8692,  11844,   3077,  10428,  94112,    609,  39248,   1735,\n",
              "           7230,   2570,  83847,   2931,  91687,  17506,  42296,  11462,  24290,\n",
              "          45418,   4014,  58299,  77180,  44962,   3978,  82801,  23736,   8426,\n",
              "           2697,  33045,   9662,  78211,   5501,  91460,  28200,  30804,  85835,\n",
              "           1747,   9836,  10043,  29994,  13671,  47828,  14716,  62429,  98311,\n",
              "          46046,   4895,  71099,  15400,  30282,   9891,  17925,  15124,  34154,\n",
              "          20155,    664,   5832,  65342,  61045,   1960,  26187,   1281,  73569,\n",
              "          63907,   8408,  17041,   1400,  21920,  66572,   3648,  31524,  74497,\n",
              "           4492,  46626,   1417,  33099,  14216,   3607,  85845,   3374,    173,\n",
              "              4,      3,  98282,   1528,  91066,  38698,   7656,   3370,  67704,\n",
              "           4455,   3176,  58852,  29733,  19219,  31146,  21922,   4951,  13863,\n",
              "           5455,  38465,   1282,  27644,   1563,   1154,  23940,    607, 102327,\n",
              "           8616,  99692,  50265,  16393,   1990,  34155,  42484,  20483,   3090,\n",
              "          89897,   2501,   7894,   1336, 105642,  14816, 100108,   6917,  71816,\n",
              "          16368,   5193,  85606,  27575,  35043,  57141,    813,  92451,   3603,\n",
              "           2154,   4024,   5203,  83133,  74690,  21697,  17895,   1273,  13554,\n",
              "          42798,   6897,  40422,  36918,   6283,  11943,  90797,   5874,  42208,\n",
              "          24713,  26439,  13588,   8926,  43540,  44196,  10661,   3947,  34610,\n",
              "          25246,  30332,  30285,  35418,      9,  14753,   1813,  99271,  47364,\n",
              "           9889,  11243,  16444,  50902,   5667,  73882,   2096,  12369,   6323,\n",
              "           2676,  18148,   8481,   1468,   1199,  44648,   1446,   4762,  29644,\n",
              "          11307,  36653,  97186,  58421,  50019,  71912,   8552,  42095]),\n",
              " 'Book-Author': tensor([ 2000,   332,   903,   184,   348, 13403,   854,  1448,  3691,  4474,\n",
              "          4070, 32573,   237, 46398,  1429,  2493, 26729, 26244,   323,  1385,\n",
              "         11110,   288,  4447,   449,  8877, 40141,  5755,   346, 26138, 10750,\n",
              "          5651,  1451, 34845, 32109,   761,    91, 41989,   547,   130,   338,\n",
              "         16740,   680, 26320,   502,   131,  1229, 26326, 34074,  3733,   796,\n",
              "           323,   313,  1803,  8395,  4459,   231,   754, 11086,  2655, 31335,\n",
              "          2664,    75, 24276, 11888,    96,   806, 30863, 24258,   467,  1119,\n",
              "           556,  1268,   332,   606,   447,  2669,  1108,  4553, 13017, 15895,\n",
              "          1324,  1425,   607, 14059, 15324,  8056,  6073,  2805,   224,   755,\n",
              "          9643, 14813,  2484,   787,  2083,  1400,  1670, 19635,    89, 37415,\n",
              "         14952,   692,  2321,  4851,  8577,  1799, 24492,  7301,   498,  4472,\n",
              "          2122,   348,  2655,  2384,  1031, 20624,  3627,   197,  2307,  2268,\n",
              "          1139,  1956, 13232,   649,  7989,  1511,  1368,   678,  2003,   627,\n",
              "           704,  4078, 26907,   869,  6592, 34872,  4252,  9894, 32004,  1670,\n",
              "          2096, 23120,    43, 29324,  1323,  1632,   401,  3436,  3909, 31305,\n",
              "          1409, 15613, 31617,  1397,  1162, 18616,  6590,   908,   375, 19812,\n",
              "          4373,  1439,   808,   348,  7408, 31162,   857,  1900,  4165,  4768,\n",
              "          2196,  9138,    55,  1263,  1913,   188,  4234,  8751, 43645, 40551,\n",
              "          1317,  1431, 16092, 35498,   148,  1670, 31742,  7751,    16,  5445,\n",
              "           179,   224,   142,  4919,  5335, 45799,  3674, 32742,   179, 34368,\n",
              "         37789,  9894,  2737,   683, 28026,  2805,   348,   817,     9, 11590,\n",
              "         29325, 33135,   801,  1943,  1433,  1277,  1602,  1264,   142,   449,\n",
              "           627,   754, 12330,  3495,  2389,    82,  3258,  2668,  5767, 17680,\n",
              "          3656,   761,   350, 17429, 29217,  2264,   348, 29509, 27376,  3756,\n",
              "          1158,  5069,    55,   895,   815, 44222, 35618,  7331,  1137,     9,\n",
              "            54,  1511,  4536,   224,  4265,  2770, 32741,   163, 13840,  1493,\n",
              "          3169,   224, 42531,    79,  9462,  1264, 14622, 35365, 14382,   249,\n",
              "          6363,   702,  4109, 33316, 12663,   539,   704,  2718,  2887,  1538,\n",
              "         37168, 36054,  2165, 10773, 28150,  5701, 30034, 45694,   225, 18221,\n",
              "         18462, 27305,  4335,  2706, 35277, 20483, 37483, 29556,  3942, 35761,\n",
              "           123,   655,   295,  2631, 26642,  5459,  5217, 41204,  8106,   184,\n",
              "          7751, 19684,  2746,  4076,  1014,  8835,   655,  6888, 16777,  6110,\n",
              "          1538,   498, 20668,     3,   967,   543,  1006,    78, 43388,   761,\n",
              "          6723,   311,   642,   868,  2699, 28886, 37125,    43,  2679, 39575,\n",
              "           213,    79,  1900,  2213,  1417, 37595,  1370, 11564, 15172,   733,\n",
              "          4000,   249,   351,  1851,  4565, 16838,  4131,  8381, 30756, 46206,\n",
              "         22391,  3252, 34521,  3821,  1069,    79,  8288,  8597, 18219, 11201,\n",
              "           535,  3770,  3251, 30079,  1430,  4212,   808,   287, 31440,  5166,\n",
              "          9724,  1024, 12054, 32569,   288,  1799, 35948,   120,  1883,  1049,\n",
              "         14374,  7713,    91, 40941,  1125,   149,     4,     3,   603,  1135,\n",
              "         29258, 20460,    79,  2337, 33006,   761,  2200, 29153, 15871,  5446,\n",
              "          2016,   184,   761,  7933,   556,  3600,   484,  4246,    43,    55,\n",
              "          4089,   323,  3741,  5267, 46796,  1521,   499,  1448,  8733, 21016,\n",
              "          5163,   857, 37903,   205,   610,  6919, 49333,  8441, 46966,   498,\n",
              "         18006,   733,   544,  3478,   312, 47064,  1750,   642, 43727,   681,\n",
              "           144,  2707,   288, 37152, 36044, 11940,  7300,   967,   627,  8301,\n",
              "          2807,  5727,  2953,  1423,  1031,   361,  1900,   808, 13462, 14277,\n",
              "          7798,     4,   778, 22789,    46,    11,   508,  1400,   919,  1070,\n",
              "         18810,     9,  2655,  1327, 46617, 24274,    79,     9,  9373,  9984,\n",
              "           367,  3114,   224,  7154,  4080,  1884,   224,    10,  1088,   908,\n",
              "            67,  1069,  3180,  2853,  6585,  5717, 45724,  5460,  6107, 24564,\n",
              "           188, 21898]),\n",
              " 'Book-Publisher': tensor([  71,   71,  142,    2,  107,    3,   61,   16,   10, 1364,  317, 3231,\n",
              "           10,  131,  332,  472,  107,   71,  151, 3626,  127,  104,    8, 2068,\n",
              "          215,  319,    3,   61,  380,  328,  133,  994, 1697,  524,  335,  169,\n",
              "            4,  133,  151,  132,  133,   63,  287,  106,   97,   16,  287,  138,\n",
              "            9,  282,  102,  152,  160,  159,  357,  103,  103,  518,  192, 1682,\n",
              "          404,   87, 2335,   87,  104,  169, 6422,  382,   51,  356,   61,  562,\n",
              "           71,  149,  282,   68,  202,  261,  388,  334,   82,  359,  350,    8,\n",
              "          268,  241,    4,  901,  108,  156, 8282, 3619,  112,  398,  816,  236,\n",
              "          169, 2251,   36, 4673, 3644,    2,   86,  168,  154,  152,  112,  373,\n",
              "          152,   98,   53,  307,  192,  170,  142,  398,  104,  146,   92,  822,\n",
              "           27,   10,  275,   63,  104,  227,   63,  112,   71,  169,  434,   10,\n",
              "         3793,   71,  472,  524,  344,  152,  152,  160,  367,   12, 3265, 2361,\n",
              "          521,  280, 1386,    3,  146,  233,  112,   19, 1079,  323,  224,  198,\n",
              "            3,  237,   10, 1869,   71,   27,    3,   63, 4363,    3, 2515,  227,\n",
              "           10, 1500,  837,  421,   32,   32,  100,  282,  104, 1985,  310,  292,\n",
              "          453,  154, 1350, 6797,  110,  236,  409,  334,  172,  233,  344,   61,\n",
              "           61,  562, 1009, 7742,  506,  373,  139, 9050,  800,  220, 1015,  210,\n",
              "          773,   29,  307,  346,  172,  223,  243, 6800,  373,  233,  513,  133,\n",
              "            9,   61,   61,    2,    3,  103,  465,   32,   27,   63,  104,  940,\n",
              "           26, 4174,  172,  104,   63, 4083,  671,    4,   63,  398,  101,  382,\n",
              "          282,  831,  111,    3,    3,  782,  389,  281,    3,   11,   46,  108,\n",
              "         2364,  108,   61,  143,  373,  170, 1376,  620,   63,  373, 1033,   61,\n",
              "           61,    2, 3523,  414,  159,  237,  468,  442,  347,  427,  152,  132,\n",
              "          346,  210,  373,   26,  369,    3,   29,  104,  342,  231,  133,  472,\n",
              "            3,  198,  383,  332,  927,   71,  404, 3753, 4164, 1764,  160,  958,\n",
              "          146,  154,  367,  280, 1371, 3683,  280,  362, 2334,   31,  334,   71,\n",
              "           26,  168,  133,  379,  154,  125,  481,  158,  281,  228,  152,  112,\n",
              "           70,    3,  159,   61,  113,  104,  348,  146,  390,   44,  104,   69,\n",
              "          840,  103,   99, 7867,  102,  104,  227, 1532,  106, 1940,   64,  915,\n",
              "          104,  383,  314,  237,   19,  100,  160,  293,   68,  125, 1662,  354,\n",
              "           61,   27,  104,  176,  146,   61,  104,  213,  528,   26,  122, 1257,\n",
              "          164,   63,  104, 1353,    3, 7218,  335,  710,  125,  112,  753, 5696,\n",
              "          104,   11, 1644,  142,  163,  482, 4169,   43,    3,  879,    8,  100,\n",
              "            3,    2,  146,   18,  334,   10,   61,  161, 6767,  108,  462,  347,\n",
              "            9,  373,    2,    2,  335,   30,  327,   26,  297,    3,  122,   32,\n",
              "           71,  102,  823,  132, 5155, 1248,   63,  112, 2366,   56,  327,   32,\n",
              "          159,   63,  390,    2,  557,  326, 4355,  152,   84,  323,  104, 1559,\n",
              "          102, 7568,  178,  107,   71,  940,  107,   68,    3,  199,  154,  221,\n",
              "          373,  132,    3,    8,   16, 3798,  334,   68,  526,   39,  227,    3,\n",
              "         1070, 3514,   31,    3, 1636,   62,   12,  169,  153,   75,  152,   69,\n",
              "         2336,    8,  192,  359, 4860,  991,   61,    9,  172,  317,  156,   26,\n",
              "          108,  383,   10,  236,  108,   39,  502,  237,  341,  348,  107,  280,\n",
              "          630,  377, 5008,  198,  159,  351,    9, 4847]),\n",
              " 'Book-Year-Of-Publication': tensor([1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2,\n",
              "         2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 3, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1,\n",
              "         1, 1, 2, 2, 2, 1, 1, 1, 1, 3, 2, 3, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1,\n",
              "         1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 3, 2, 1, 2, 1,\n",
              "         2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2,\n",
              "         2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 2, 3, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1,\n",
              "         1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1,\n",
              "         2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2,\n",
              "         2, 1, 2, 3, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2,\n",
              "         1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2,\n",
              "         1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2,\n",
              "         1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1,\n",
              "         2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2,\n",
              "         2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 3,\n",
              "         2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 2, 1, 2, 3, 1, 1, 2,\n",
              "         2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1,\n",
              "         1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1,\n",
              "         1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1,\n",
              "         3, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1,\n",
              "         2, 1, 1, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1,\n",
              "         1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1,\n",
              "         2, 2, 1, 1, 2, 1, 1, 1]),\n",
              " 'Book-Title-Text': tensor([  2849,  20013,   1587,   7886,   5018,  24613,  34764,  48842,   6338,\n",
              "          80851,  11015,  66581,   1183,  98695,   1956,   3653,  98654,  35649,\n",
              "           4904,  83010,  19968,    365,   9790,   1338,  15628,  84145,   9666,\n",
              "          11833,  51810,  19124,   3371,  32044,  71863,  65584,   1289,    287,\n",
              "          88353,   2344,   2340,    424,  31509,   3676,  52562,  18033,    154,\n",
              "           1763,  52582,  70137,   5756,  38558,   5015,   1573,   2512,  64289,\n",
              "          80506,  27582,   6312,   1355,  10654,  63658, 103921,   1627,  47369,\n",
              "          21589,   2158,  16650,  62657,  47342,  23542,   8069,    698,   7869,\n",
              "           2188,    769,    552,   3963,   1490,   7237,  64126,  18799,  60074,\n",
              "           1951,    773,  68519,  28513,  77673,  10363,  44367,   6565,   3896,\n",
              "          88796,  27508,  18584,   8852,   2987,   1921,  18376, 105695,  44986,\n",
              "          77823,  27776,   2250, 106544,  11835,  17985,  14295,  47811,  38653,\n",
              "           7604,   7086, 106129,  12733,   3934,  40605,  34560,  39140,   5539,\n",
              "           9735,  25772,   6354,   1534,  16396,  26263,    827,  87576,   7797,\n",
              "           5571,    862,  14677,   7863,  52922,   6321,  54275,  38899,  36913,\n",
              "          71919,   6700,  17411,  26639,  17420,  40682,  44913,  15219,  59244,\n",
              "           1867,  67035,   7834,   5217,   6923,  63596,   2089,  29146,  64311,\n",
              "           3914,   1567,  35007,  11321,   1974,  28365,  73467,   6930,   9863,\n",
              "           3575,  35204,  54208,  63291,  73495,   2697,   6730,   7634,   3171,\n",
              "          18187,   1153,   1714,  43590,  72510,   6662,  90424,  92221,  87909,\n",
              "          77136,  31910,  30178,  73400,  37785,  32536,  64566,  53782,    394,\n",
              "          51130,   2646,    280,  41770,   7939,  16511,  97382,  73629,  67068,\n",
              "          11817, 100550,  78776,  97360,   4068,    870,  56371,   6900,  16393,\n",
              "           1074,   7446,  64179,  59245,  67983,   1037,  66162,   3080,   4836,\n",
              "           7078,   1717,  10097,   3585,  12257,   2768,  22536,   5324,   3478,\n",
              "             97,   4905,   4020,  34142,  33159,  14202,    980,    440,  32749,\n",
              "          58998,   3264,   7688,  59740,  55134,  51919,   1559,  28130,   1154,\n",
              "           2197,  38726,  93622,  73676,  16781,  37141,  49293,     64,   8243,\n",
              "           5191,   9925,  52144,   4132, 102551,   2707,  39006,   2058,   4749,\n",
              "          18142,  89639,   2743,  35048,  14471,  27143,  73162,  26649,   1747,\n",
              "          10894,  11457,  35111,  68410,  41708,    671,    899,   6433,  83927,\n",
              "         105212,  77287,  74713,  13754,  51461,  56679,  77260,  60955,  97105,\n",
              "            873,  95791,  34697,  54976,   6854,  19887,  72976,  91576,  77977,\n",
              "          59857,  63870,  11846,  76270,  12863,   1406, 105010,  53641,  28331,\n",
              "          14937,  86437,  14617,   8562,  53781,  37021,   7510,  33884,  20963,\n",
              "          15569,   8692,  11844,   3077,  10428,  94112,    609,  39248,   1735,\n",
              "           7230,   2570,  83847,   2931,  91687,  17506,  42296,  11462,  24290,\n",
              "          45418,   4014,  58299,  77180,  44962,   3978,  82801,  23736,   8426,\n",
              "           2697,  33045,   9662,  78211,   5501,  91460,  28200,  30804,  85835,\n",
              "           1747,   9836,  10043,  29994,  13671,  47828,  14716,  62429,  98311,\n",
              "          46046,   4895,  71099,  15400,  30282,   9891,  17925,  15124,  34154,\n",
              "          20155,    664,   5832,  65342,  61045,   1960,  26187,   1281,  73569,\n",
              "          63907,   8408,  17041,   1400,  21920,  66572,   3648,  31524,  74497,\n",
              "           4492,  46626,   1417,  33099,  14216,   3607,  85845,   3374,    173,\n",
              "              4,      3,  98282,   1528,  91066,  38698,   7656,   3370,  67704,\n",
              "           4455,   3176,  58852,  29733,  19219,  31146,  21922,   4951,  13863,\n",
              "           5455,  38465,   1282,  27644,   1563,   1154,  23940,    607, 102327,\n",
              "           8616,  99692,  50265,  16393,   1990,  34155,  42484,  20483,   3090,\n",
              "          89897,   2501,   7894,   1336, 105642,  14816, 100108,   6917,  71816,\n",
              "          16368,   5193,  85606,  27575,  35043,  57141,    813,  92451,   3603,\n",
              "           2154,   4024,   5203,  83133,  74690,  21697,  17895,   1273,  13554,\n",
              "          42798,   6897,  40422,  36918,   6283,  11943,  90797,   5874,  42208,\n",
              "          24713,  26439,  13588,   8926,  43540,  44196,  10661,   3947,  34610,\n",
              "          25246,  30332,  30285,  35418,      9,  14753,   1813,  99271,  47364,\n",
              "           9889,  11243,  16444,  50902,   5667,  73882,   2096,  12369,   6323,\n",
              "           2676,  18148,   8481,   1468,   1199,  44648,   1446,   4762,  29644,\n",
              "          11307,  36653,  97186,  58421,  50019,  71912,   8552,  42095])}"
            ]
          },
          "execution_count": 469,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjn33eipT_-e"
      },
      "source": [
        "## Two Tower Model for Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 470,
      "metadata": {
        "id": "J4Rh2nDqUCxf"
      },
      "outputs": [],
      "source": [
        "class UserTower(nn.Module):\n",
        "\n",
        "    # User Tower -- User-ID, Age\n",
        "\n",
        "    def __init__(self, num_users, num_ages, embedding_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        user_embedding_dim = 128    \n",
        "        age_embedding_dim = 16   \n",
        "        linear_in = user_embedding_dim + age_embedding_dim\n",
        "\n",
        "        self.user_embedding = nn.Embedding(num_users, user_embedding_dim, padding_idx=0)\n",
        "        self.user_age_embedding = nn.Embedding(num_ages, age_embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.user_mlp = nn.Sequential(\n",
        "            nn.Linear(linear_in, 512), # 2 embeddings (user-id, user-age)\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, embedding_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, user_id, age):\n",
        "        user_emb = self.user_embedding(user_id)\n",
        "        age_emb = self.user_age_embedding(age)\n",
        "        x = torch.cat([user_emb, age_emb], dim=1)\n",
        "        return self.user_mlp(x)\n",
        "\n",
        "    def get_embedding(self, data):\n",
        "        return self.forward(data['User-ID'], data['User-Age'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLSZeUSEUNBY"
      },
      "outputs": [],
      "source": [
        "class ItemTower(nn.Module):\n",
        "    def __init__(self, num_isbn, num_titles, num_authors, num_publishers, num_year_of_publications, embedding_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Item Tower -- ISBN, Book-Title, Book-Author, Publisher, Year-Of-Publication\n",
        "        # book_isbn_embedding_size = 64\n",
        "        book_title_embedding_size = 32\n",
        "        book_author_embedding_size = 32\n",
        "        book_publisher_embedding_size = 16\n",
        "        book_year_of_publication_embedding_size = 8\n",
        "\n",
        "        # Categorical embeddings\n",
        "        # self.book_isbn_embedding = nn.Embedding(num_isbn, book_isbn_embedding_size, padding_idx=0)\n",
        "        self.book_title_embedding = nn.Embedding(num_titles, book_title_embedding_size, padding_idx=0)\n",
        "        self.book_author_embedding = nn.Embedding(num_authors, book_author_embedding_size, padding_idx=0)\n",
        "        self.book_publisher_embedding = nn.Embedding(num_publishers, book_publisher_embedding_size, padding_idx=0)\n",
        "        self.book_year_of_publication_embedding = nn.Embedding(num_year_of_publications, book_year_of_publication_embedding_size, padding_idx=0)\n",
        "\n",
        "        # book_isbn_embedding_size + \n",
        "        linear_in = book_title_embedding_size + book_author_embedding_size + book_publisher_embedding_size + book_year_of_publication_embedding_size\n",
        "\n",
        "        self.item_mlp = nn.Sequential(\n",
        "            nn.Linear(linear_in, 512), \n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, embedding_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, isbn, book_title, book_author, book_publisher, book_year_of_publication):\n",
        "        # book_isbn_emb = self.book_isbn_embedding(isbn)\n",
        "        book_title_emb = self.book_title_embedding(book_title)\n",
        "        book_author_emb = self.book_author_embedding(book_author)\n",
        "        book_publisher_emb = self.book_publisher_embedding(book_publisher)\n",
        "        book_year = self.book_year_of_publication_embedding(book_year_of_publication)\n",
        "\n",
        "        x = torch.cat([\n",
        "            # book_isbn_emb,\n",
        "            book_title_emb,\n",
        "            book_author_emb,\n",
        "            book_publisher_emb,\n",
        "            book_year\n",
        "        ], dim=1)\n",
        "\n",
        "        return self.item_mlp(x)\n",
        "\n",
        "    def get_embedding(self, data):\n",
        "        return self.forward(\n",
        "            data['Book-ISBN'],\n",
        "            data['Book-Title'],\n",
        "            data['Book-Author'],\n",
        "            data['Book-Publisher'],\n",
        "            data['Book-Year-Of-Publication'],\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 472,
      "metadata": {
        "id": "l-fw4HLLUOXk"
      },
      "outputs": [],
      "source": [
        "class TwoTowers(nn.Module):\n",
        "    def __init__(self, user_tower: UserTower, item_tower: ItemTower):\n",
        "        super().__init__()\n",
        "        self.user_tower = user_tower\n",
        "        self.item_tower = item_tower\n",
        "\n",
        "    def forward(self, data):\n",
        "        user_emb = self.user_tower.get_embedding(data)\n",
        "        item_emb = self.item_tower.get_embedding(data)\n",
        "\n",
        "        user_emb = F.normalize(user_emb, p=2, dim=1)\n",
        "        item_emb = F.normalize(item_emb, p=2, dim=1)\n",
        "        \n",
        "        return user_emb, item_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 473,
      "metadata": {
        "id": "jlqHUau8WMjo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NUM_USERS: 39813\n",
            "NUM_AGES: 7\n",
            "NUM_ISBN: 117562\n",
            "NUM_TITLES: 107251\n",
            "NUM_AUTHORS: 50048\n",
            "NUM_PUBLISHERS: 9474\n",
            "NUM_YEAR_OF_PUBLICATIONS: 8\n"
          ]
        }
      ],
      "source": [
        "NUM_USERS = len(dataset.encoders['User-ID']) + 1\n",
        "NUM_AGES = len(dataset.encoders['User-Age']) + 1\n",
        "NUM_ISBN = len(dataset.encoders['ISBN']) + 1\n",
        "NUM_TITLES = len(dataset.encoders['Book-Title']) + 1\n",
        "NUM_AUTHORS = len(dataset.encoders['Book-Author']) + 1\n",
        "NUM_PUBLISHERS = len(dataset.encoders['Publisher']) + 1\n",
        "NUM_YEAR_OF_PUBLICATIONS = len(dataset.encoders['Book-Year-Of-Publication']) + 1\n",
        "\n",
        "EMBEDDING_SIZE = 128\n",
        "\n",
        "user_tower = UserTower(\n",
        "    num_users=NUM_USERS,  \n",
        "    num_ages=NUM_AGES,\n",
        "    embedding_dim=EMBEDDING_SIZE\n",
        ")\n",
        "\n",
        "item_tower = ItemTower(\n",
        "    num_isbn=NUM_ISBN,\n",
        "    num_titles=NUM_TITLES,\n",
        "    num_authors=NUM_AUTHORS,\n",
        "    num_publishers=NUM_PUBLISHERS,\n",
        "    num_year_of_publications=NUM_YEAR_OF_PUBLICATIONS,\n",
        "    embedding_dim=EMBEDDING_SIZE\n",
        ")\n",
        "\n",
        "two_towers = TwoTowers(\n",
        "    user_tower,\n",
        "    item_tower\n",
        ").to(device)\n",
        "\n",
        "print(\"NUM_USERS:\", NUM_USERS)\n",
        "print(\"NUM_AGES:\", NUM_AGES)\n",
        "print(\"NUM_ISBN:\", NUM_ISBN)\n",
        "print(\"NUM_TITLES:\", NUM_TITLES)\n",
        "print(\"NUM_AUTHORS:\", NUM_AUTHORS)\n",
        "print(\"NUM_PUBLISHERS:\", NUM_PUBLISHERS)\n",
        "print(\"NUM_YEAR_OF_PUBLICATIONS:\", NUM_YEAR_OF_PUBLICATIONS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 474,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_recall_at_k(two_towers, epoch, EPOCHS, k):\n",
        "    \"\"\"\n",
        "    Gathers the current item embeddings, \n",
        "    calculates similarity between each user and the items. \n",
        "\n",
        "    Calculates and returns recall@k metric of the recommendations \n",
        "    made to the user.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Getting all item embeddings --- \n",
        "    entire_dataset = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "    all_item_embeddings = []\n",
        "    for batch in entire_dataset:\n",
        "        _, item_embedding = two_towers(batch)\n",
        "        all_item_embeddings.append(item_embedding)\n",
        "    all_item_embeddings = torch.cat(all_item_embeddings, dim=0)\n",
        "\n",
        "    total_recall = 0.0\n",
        "    num_users = 0\n",
        "\n",
        "    # --- Calculating recall@k --- \n",
        "    for idx, batch in enumerate(test_loader):\n",
        "        user_embedding, _ = two_towers(batch)\n",
        "        similarity_scores = user_embedding @ all_item_embeddings.T  # [batch_size, num_items]\n",
        "\n",
        "        top_scores, top_indices = torch.topk(similarity_scores, k=k, dim=1)\n",
        "\n",
        "        for user_id, items, scores in zip(batch['User-ID'], top_indices, top_scores):\n",
        "            user_rows = dataset.data[dataset.data['User-ID'] == user_id.item()]\n",
        "\n",
        "            # Recommended books (Book-Title IDs)\n",
        "            recommended_book_ids_set = set([dataset.data.iloc[idx.item()]['Book-Title'] for idx in items])\n",
        "            actual_book_ids_set = set(user_rows['Book-Title'].tolist())\n",
        "\n",
        "            hits = len(recommended_book_ids_set & actual_book_ids_set)  # intersection\n",
        "            recall_at_k = hits / len(actual_book_ids_set)\n",
        "\n",
        "            total_recall += recall_at_k\n",
        "            num_users += 1\n",
        "\n",
        "    average_recall_at_k = total_recall / num_users\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}, Average Recall@{k}: {average_recall_at_k:.4f}\\n\")\n",
        "    return average_recall_at_k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3QGAbFDa_Lb"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn06WdV-b4Jf"
      },
      "source": [
        "#### Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 475,
      "metadata": {
        "id": "bGt_U5rrg7eA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "%rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 476,
      "metadata": {
        "id": "GlzzNo4_mM0E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 5e-4\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "TEMPERATURE = 0.1\n",
        "WEIGHT_DECAY = 1e-5\n",
        "MODEL_SAVE_PATH = \"/models\"\n",
        "\n",
        "optimizer = torch.optim.Adam(two_towers.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "writer = SummaryWriter('./logs/')\n",
        "\n",
        "best_test_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "global_step = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 478,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhQ3lphfa_nU",
        "outputId": "d9ac0b31-4212-41bc-94ca-27f63e855d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Average Training Loss: 6.1493, \n",
            "Epoch 1/50, Average Test Loss: 6.1049, \n",
            "Epoch 2/50, Average Training Loss: 6.0447, \n",
            "Epoch 2/50, Average Test Loss: 6.0206, \n",
            "Epoch 3/50, Average Training Loss: 5.9251, \n",
            "Epoch 3/50, Average Test Loss: 5.9530, \n",
            "Epoch 4/50, Average Training Loss: 5.7980, \n",
            "Epoch 4/50, Average Test Loss: 5.9025, \n",
            "Epoch 5/50, Average Training Loss: 5.6671, \n",
            "Epoch 5/50, Average Test Loss: 5.8649, \n",
            "Epoch 6/50, Average Training Loss: 5.5294, \n",
            "Epoch 6/50, Average Test Loss: 5.8438, \n",
            "Epoch 7/50, Average Training Loss: 5.3924, \n",
            "Epoch 7/50, Average Test Loss: 5.8371, \n",
            "Epoch 8/50, Average Training Loss: 5.2515, \n",
            "Epoch 8/50, Average Test Loss: 5.8359, \n",
            "Epoch 9/50, Average Training Loss: 5.1093, \n",
            "Epoch 9/50, Average Test Loss: 5.8371, \n",
            "Epoch 10/50, Average Training Loss: 4.9729, \n",
            "Epoch 10/50, Average Test Loss: 5.8390, \n",
            "Epoch 11/50, Average Training Loss: 4.8360, \n",
            "Epoch 11/50, Average Test Loss: 5.8532, \n",
            "Epoch 12/50, Average Training Loss: 4.7042, \n",
            "Epoch 12/50, Average Test Loss: 5.8519, \n",
            "Epoch 13/50, Average Training Loss: 4.5767, \n",
            "Epoch 13/50, Average Test Loss: 5.8722, \n",
            "Epoch 14/50, Average Training Loss: 4.4530, \n",
            "Epoch 14/50, Average Test Loss: 5.8823, \n",
            "Epoch 15/50, Average Training Loss: 4.3318, \n",
            "Epoch 15/50, Average Test Loss: 5.8968, \n",
            "Epoch 16/50, Average Training Loss: 4.2114, \n",
            "Epoch 16/50, Average Test Loss: 5.9048, \n",
            "Epoch 17/50, Average Training Loss: 4.1006, \n",
            "Epoch 17/50, Average Test Loss: 5.9161, \n",
            "Epoch 18/50, Average Training Loss: 3.9943, \n",
            "Epoch 18/50, Average Test Loss: 5.9330, \n",
            "Epoch 19/50, Average Training Loss: 3.8876, \n",
            "Epoch 19/50, Average Test Loss: 5.9464, \n",
            "Epoch 20/50, Average Training Loss: 3.7829, \n",
            "Epoch 20/50, Average Test Loss: 5.9608, \n",
            "Epoch 21/50, Average Training Loss: 3.6874, \n",
            "Epoch 21/50, Average Test Loss: 5.9855, \n",
            "Epoch 22/50, Average Training Loss: 3.5953, \n",
            "Epoch 22/50, Average Test Loss: 5.9892, \n",
            "Epoch 23/50, Average Training Loss: 3.5078, \n",
            "Epoch 23/50, Average Test Loss: 6.0075, \n",
            "Epoch 24/50, Average Training Loss: 3.4272, \n",
            "Epoch 24/50, Average Test Loss: 6.0245, \n",
            "Epoch 25/50, Average Training Loss: 3.3490, \n",
            "Epoch 25/50, Average Test Loss: 6.0492, \n",
            "Epoch 26/50, Average Training Loss: 3.2780, \n",
            "Epoch 26/50, Average Test Loss: 6.0546, \n",
            "Epoch 27/50, Average Training Loss: 3.2071, \n",
            "Epoch 27/50, Average Test Loss: 6.0671, \n",
            "Epoch 28/50, Average Training Loss: 3.1441, \n",
            "Epoch 28/50, Average Test Loss: 6.0975, \n",
            "Epoch 29/50, Average Training Loss: 3.0861, \n",
            "Epoch 29/50, Average Test Loss: 6.0961, \n",
            "Epoch 30/50, Average Training Loss: 3.0287, \n",
            "Epoch 30/50, Average Test Loss: 6.1105, \n",
            "Epoch 31/50, Average Training Loss: 2.9755, \n",
            "Epoch 31/50, Average Test Loss: 6.1327, \n",
            "Epoch 32/50, Average Training Loss: 2.9281, \n",
            "Epoch 32/50, Average Test Loss: 6.1396, \n",
            "Epoch 33/50, Average Training Loss: 2.8797, \n",
            "Epoch 33/50, Average Test Loss: 6.1685, \n",
            "Epoch 34/50, Average Training Loss: 2.8358, \n",
            "Epoch 34/50, Average Test Loss: 6.1740, \n",
            "Epoch 35/50, Average Training Loss: 2.7963, \n",
            "Epoch 35/50, Average Test Loss: 6.1858, \n",
            "Epoch 36/50, Average Training Loss: 2.7576, \n",
            "Epoch 36/50, Average Test Loss: 6.2011, \n",
            "Epoch 37/50, Average Training Loss: 2.7218, \n",
            "Epoch 37/50, Average Test Loss: 6.2099, \n",
            "Epoch 38/50, Average Training Loss: 2.6820, \n",
            "Epoch 38/50, Average Test Loss: 6.2357, \n",
            "Epoch 39/50, Average Training Loss: 2.6550, \n",
            "Epoch 39/50, Average Test Loss: 6.2335, \n",
            "Epoch 40/50, Average Training Loss: 2.6225, \n",
            "Epoch 40/50, Average Test Loss: 6.3026, \n",
            "Epoch 41/50, Average Training Loss: 2.5914, \n",
            "Epoch 41/50, Average Test Loss: 6.2461, \n",
            "Epoch 42/50, Average Training Loss: 2.5646, \n",
            "Epoch 42/50, Average Test Loss: 6.2714, \n",
            "Epoch 43/50, Average Training Loss: 2.5349, \n",
            "Epoch 43/50, Average Test Loss: 6.2775, \n",
            "Epoch 44/50, Average Training Loss: 2.5094, \n",
            "Epoch 44/50, Average Test Loss: 6.2804, \n",
            "Epoch 45/50, Average Training Loss: 2.4857, \n",
            "Epoch 45/50, Average Test Loss: 6.2993, \n",
            "Epoch 46/50, Average Training Loss: 2.4626, \n",
            "Epoch 46/50, Average Test Loss: 6.2808, \n",
            "Epoch 47/50, Average Training Loss: 2.4428, \n",
            "Epoch 47/50, Average Test Loss: 6.3295, \n",
            "Epoch 48/50, Average Training Loss: 2.4198, \n",
            "Epoch 48/50, Average Test Loss: 6.3071, \n",
            "Epoch 49/50, Average Training Loss: 2.4008, \n",
            "Epoch 49/50, Average Test Loss: 6.3111, \n",
            "Epoch 50/50, Average Training Loss: 2.3760, \n",
            "Epoch 50/50, Average Test Loss: 6.3380, \n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    # -- Main Loop --\n",
        "    running_train_loss = 0.0\n",
        "    two_towers.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        user_embedding, item_embedding = two_towers(batch)\n",
        "\n",
        "        logits = (user_embedding @ item_embedding.T) / TEMPERATURE\n",
        "        labels = torch.arange(user_embedding.size(0)).to(device) \n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    # -- Test Loop -- \n",
        "    two_towers.eval()\n",
        "    running_test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            user_embedding, item_embedding = two_towers(batch)\n",
        "\n",
        "            logits = (user_embedding @ item_embedding.T) / TEMPERATURE\n",
        "            labels = torch.arange(user_embedding.size(0)).to(device)\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "            running_test_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_train_loss / len(train_loader)\n",
        "    avg_test_loss = running_test_loss / len(test_loader)\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}, Average Training Loss: {avg_train_loss:.4f}, \")\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}, Average Test Loss: {avg_test_loss:.4f}, \")\n",
        "    # calculate_recall_at_k(two_towers, epoch, EPOCHS, 25)\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(two_towers.state_dict(), f\"./{MODEL_SAVE_PATH}/two_towers_epoch{epoch}_test{avg_test_loss:.2}_train{avg_train_loss:.2f}.pt\")\n",
        "\n",
        "writer.close()\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlXm69k4dJpF"
      },
      "source": [
        "### Seeing what the model recommends to me after training\n",
        "\n",
        "- It should have seen me somewhere in the training data and should have learned enough information from the other data to generalize over what I might like.\n",
        "- I will pass my username and age into the User Tower. And then conduct a dot product between my vector and the matrix of learned item embeddings to get relevance scores.\n",
        "- I will then conduct some semi-manual ranking based on removing what I have already read and other info.\n",
        "- Then I will make the final 50 recommendations for me."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for TwoTowers:\n\tUnexpected key(s) in state_dict: \"item_tower.book_isbn_embedding.weight\". \n\tsize mismatch for user_tower.user_embedding.weight: copying a param with shape torch.Size([11996, 128]) from checkpoint, the shape in current model is torch.Size([39813, 128]).\n\tsize mismatch for item_tower.book_title_embedding.weight: copying a param with shape torch.Size([1958, 32]) from checkpoint, the shape in current model is torch.Size([107251, 32]).\n\tsize mismatch for item_tower.book_author_embedding.weight: copying a param with shape torch.Size([2112, 32]) from checkpoint, the shape in current model is torch.Size([50048, 32]).\n\tsize mismatch for item_tower.book_publisher_embedding.weight: copying a param with shape torch.Size([863, 16]) from checkpoint, the shape in current model is torch.Size([9474, 16]).\n\tsize mismatch for item_tower.book_year_of_publication_embedding.weight: copying a param with shape torch.Size([7, 8]) from checkpoint, the shape in current model is torch.Size([8, 8]).\n\tsize mismatch for item_tower.item_mlp.0.weight: copying a param with shape torch.Size([512, 152]) from checkpoint, the shape in current model is torch.Size([512, 88]).",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[484]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Pretrained model:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtwo_towers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/two_towers_epoch50_test6.8_train3.13_favorite.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/booky/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for TwoTowers:\n\tUnexpected key(s) in state_dict: \"item_tower.book_isbn_embedding.weight\". \n\tsize mismatch for user_tower.user_embedding.weight: copying a param with shape torch.Size([11996, 128]) from checkpoint, the shape in current model is torch.Size([39813, 128]).\n\tsize mismatch for item_tower.book_title_embedding.weight: copying a param with shape torch.Size([1958, 32]) from checkpoint, the shape in current model is torch.Size([107251, 32]).\n\tsize mismatch for item_tower.book_author_embedding.weight: copying a param with shape torch.Size([2112, 32]) from checkpoint, the shape in current model is torch.Size([50048, 32]).\n\tsize mismatch for item_tower.book_publisher_embedding.weight: copying a param with shape torch.Size([863, 16]) from checkpoint, the shape in current model is torch.Size([9474, 16]).\n\tsize mismatch for item_tower.book_year_of_publication_embedding.weight: copying a param with shape torch.Size([7, 8]) from checkpoint, the shape in current model is torch.Size([8, 8]).\n\tsize mismatch for item_tower.item_mlp.0.weight: copying a param with shape torch.Size([512, 152]) from checkpoint, the shape in current model is torch.Size([512, 88])."
          ]
        }
      ],
      "source": [
        "\n",
        "# Pretrained model:\n",
        "two_towers.load_state_dict(torch.load(\"models/two_towers_epoch50_test6.3_train2.38.pt\", map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlaJclDBgeeb"
      },
      "outputs": [],
      "source": [
        "# Getting all item embeddings\n",
        "entire_dataset = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "all_item_embeddings = []\n",
        "for batch in entire_dataset:\n",
        "    _, item_embedding = two_towers(batch)\n",
        "    all_item_embeddings.append(item_embedding)\n",
        "all_item_embeddings = torch.cat(all_item_embeddings, dim=0)\n",
        "\n",
        "# Getting a single embedding for my learned user\n",
        "paul_user_id = dataset.encoders['User-ID']['1234567890']\n",
        "paul_age = dataset.encoders['User-Age']['18-25']\n",
        "paul_batch = {\n",
        "    'User-ID': torch.tensor([paul_user_id], dtype=torch.long, device=device),\n",
        "    'User-Age': torch.tensor([paul_age], dtype=torch.long, device=device)\n",
        "}\n",
        "paul_user_embedding = two_towers.user_tower.get_embedding(paul_batch) # [1 batch, 128 dimensions]\n",
        "paul_user_embedding = F.normalize(paul_user_embedding, p=2, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([264758, 128])\n",
            "torch.Size([1, 128])\n"
          ]
        }
      ],
      "source": [
        "print(all_item_embeddings.shape)\n",
        "print(paul_user_embedding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "similarity_scores = (paul_user_embedding @ all_item_embeddings.T).squeeze()\n",
        "top_k = 1000\n",
        "top_scores, top_indices = torch.topk(similarity_scores, top_k)\n",
        "\n",
        "unique_recommendations = []\n",
        "seen_titles = set()\n",
        "read_isbns = personal_df['ISBN'].astype(str).to_list()\n",
        "\n",
        "for score, idx in zip(top_scores.detach().cpu().numpy(), top_indices.detach().cpu().numpy()):\n",
        "    row = dataset.data.iloc[idx]  # pandas row\n",
        "\n",
        "    title_idx = int(row['Book-Title'])\n",
        "    author_idx = int(row['Book-Author'])\n",
        "    isbn_idx = int(row['ISBN'])\n",
        "\n",
        "    title = dataset.reverse_encoders['Book-Title'][title_idx]\n",
        "    author = dataset.reverse_encoders['Book-Author'][author_idx]\n",
        "    isbn = dataset.reverse_encoders['ISBN'][isbn_idx]\n",
        "\n",
        "    # skip duplicates or already read books\n",
        "    if title in seen_titles or isbn in read_isbns:\n",
        "        continue\n",
        "\n",
        "    seen_titles.add(title)\n",
        "    unique_recommendations.append({\n",
        "        'title': title,\n",
        "        'author': author,\n",
        "        'score': score\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6dftVU0rnlP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Journey to the End of the Night, Author: Louis-Ferdinand Celine, Score: 0.7245\n",
            "Title: Notes from the Underground (Dover Thrift Editions), Author: Fyodor Dostoyevsky, Score: 0.6672\n",
            "Title: Like a Hole in the Head, Author: Jen Banbury, Score: 0.6457\n",
            "Title: And Then There Were None, Author: Agatha Christie, Score: 0.6418\n",
            "Title: Dune, Author: Frank Herbert, Score: 0.6367\n",
            "Title: The Boggart, Author: Susan Cooper, Score: 0.6345\n",
            "Title: Masterpieces of murder, Author: Agatha Christie, Score: 0.6320\n",
            "Title: O medo (Documenta potica), Author: Al Berto, Score: 0.6319\n",
            "Title: When Bunny Grows Up, Author: Patsy Scarry, Score: 0.6307\n",
            "Title: Dracula: A Symphony in Moonlight &amp; Nightmares, Author: Jon J. Muth, Score: 0.6276\n",
            "Title: Time Travel: Fact, Fiction, &amp; Possibility, Author: Jenny Randles, Score: 0.6242\n",
            "Title: Gods Themselves, Author: Isaac Asimov, Score: 0.6218\n",
            "Title: Tao Teh King: Nature and Intelligence, Author: Lao Tzu, Score: 0.6205\n",
            "Title: Here Comes the Cat!, Author: Frank Asch, Score: 0.6183\n",
            "Title: The Guinness Book of Film, Author: Tessa Clayton, Score: 0.6146\n",
            "Title: The New Hugo Winners, Vol. 2, Author: Isaac Asimov, Score: 0.6136\n",
            "Title: Honest to God, Author: John Arthur Thomas Robinson, Score: 0.6121\n",
            "Title: LIFE UNIVERS EVRTH (Hitchhiker's Trilogy (Paperback)), Author: Douglas Adams, Score: 0.6107\n",
            "Title: The Selected Letters of Lewis Carroll, Author: Lewis Carroll, Score: 0.6073\n",
            "Title: Los Lagartos Terribles, Author: Isaac Asimov, Score: 0.6062\n",
            "Title: Inherit the Wind, Author: JEROME LAWRENCE, Score: 0.6013\n",
            "Title: Collected Poems 1947-1980, Author: Allen Ginsberg, Score: 0.6004\n",
            "Title: L'??le du jour d'avant, Author: Umberto Eco, Score: 0.5994\n",
            "Title: The Death of Ivan Ilyich, Author: LEO TOLSTOY, Score: 0.5989\n",
            "Title: Our Times, Author: Robert Atwan, Score: 0.5978\n",
            "Title: Twilight Los Angeles, 1992: On the Road : A Search for American Character, Author: Anna Deavere Smith, Score: 0.5972\n",
            "Title: The Tragedy of King Lear (The Folger Library general reader's Shakespeare), Author: William Shakespeare, Score: 0.5946\n",
            "Title: Adventures of Huckleberry Finn (Dover Thrift Editions), Author: Mark Twain, Score: 0.5946\n",
            "Title: Knock `Em Dead 1997: The Ultimate Job Seekers Handbook (10th ed), Author: Martin John Yate, Score: 0.5890\n",
            "Title: By Any Means Necessary, Author: Malcolm X, Score: 0.5855\n",
            "Title: Things Fall Apart, Author: Chinua Achebe, Score: 0.5834\n",
            "Title: The Air-Conditioned Nightmare, Author: Henry Miller, Score: 0.5829\n",
            "Title: The White Mercedes, Author: PHILIP PULLMAN, Score: 0.5824\n",
            "Title: Vorkurs der Ingenieurmathematik., Author: J?rgen Wendeler, Score: 0.5818\n",
            "Title: Barney &amp; Baby Bop Follow That Cat! (Barney Discovery), Author: Stephen White, Score: 0.5817\n",
            "Title: SKINNY LEGS AND ALL, Author: TOM ROBBINS, Score: 0.5780\n",
            "Title: MURDER ORIENT EXPS, Author: Agatha Christie, Score: 0.5763\n",
            "Title: SO LONG THANK FISH (Hitchhiker's Trilogy (Paperback)), Author: Douglas Adams, Score: 0.5759\n",
            "Title: Waldo and Magic, Inc., Author: Robert A. Heinlein, Score: 0.5757\n",
            "Title: Nightmare Passage  (Deathlands 40) (Deathlands Series, No 40), Author: James Axler, Score: 0.5750\n",
            "Title: LIFE, THE UNIVERSE AND EVERYTHING (Hitchhiker's Trilogy (Paperback)), Author: Douglas Adams, Score: 0.5743\n",
            "Title: Madame Bovary (Penguin Popular Classics), Author: Gustave Flaubert, Score: 0.5734\n",
            "Title: Genji Monogatari (Tut Books. L), Author: Shikibu Murasaki, Score: 0.5730\n",
            "Title: How to Stop Worrying and Start Living, Author: Dale Carnegie, Score: 0.5730\n",
            "Title: Infinite Self: 33 Steps to Reclaiming Your Inner Power, Author: Stuart Wilde, Score: 0.5718\n",
            "Title: The Moving Finger (Miss Marple Mysteries (Paperback)), Author: Agatha Christie, Score: 0.5710\n",
            "Title: Walden and Civil Disobedience (Penguin American Library), Author: Henry David Thoreau, Score: 0.5681\n",
            "Title: Murder On the Links, Author: Agatha Christie, Score: 0.5680\n",
            "Title: Sexy, Author: Candy Barr, Score: 0.5678\n",
            "Title: Kidnapped (Penguin Classics), Author: Robert Louis Stevenson, Score: 0.5668\n",
            "Title: Treasure Island, Author: Robert Louis Stevenson, Score: 0.5647\n",
            "Title: Dubliners, Author: James Joyce, Score: 0.5646\n",
            "Title: Black Swan, White Raven, Author: Ellen Datlow, Score: 0.5645\n",
            "Title: Macbeth (Oxford School Shakespeare Series), Author: Roma Gill, Score: 0.5639\n",
            "Title: Divine Comedy Purgatorio, Author: Dante Alighieri, Score: 0.5634\n",
            "Title: Battle Cry of Freedom: The Civil War Era (Oxford History of the United States), Author: James M. McPherson, Score: 0.5631\n",
            "Title: The TEMPEST, Author: William Shakespeare, Score: 0.5618\n",
            "Title: No One Here Gets Out Alive, Author: Jerry Dan Hopkins, Score: 0.5615\n",
            "Title: De Profundis (Dover Thrift Editions), Author: Oscar Wilde, Score: 0.5611\n",
            "Title: Oscar Wilde's Wit and Wisdom: A Book of Quotations (Dover Thrift Editions), Author: Oscar Wilde, Score: 0.5611\n",
            "Title: Guerra Del Tiempo, Author: Alejo Carpentier, Score: 0.5609\n",
            "Title: No-Sew Applique: Holiday Magic, Author: Jean Wells, Score: 0.5606\n",
            "Title: The Illustrated Hitchhiker's Guide to the Galaxy, Author: Douglas Adams, Score: 0.5599\n",
            "Title: Off Limits: Tales of Alien Sex, Author: Ellen Datlow, Score: 0.5571\n",
            "Title: Whit, Author: Iain Banks, Score: 0.5571\n",
            "Title: Lizard Music, Author: DANIEL MANUS PINKWATER, Score: 0.5567\n",
            "Title: Some Prefer Nettles: A Novel (First Vintage International), Author: Junichiro Tanizaki, Score: 0.5561\n",
            "Title: Measure for Measure: With New Dramatic Criticism and an Updated Bibliography (Shakespeare, William, Works.), Author: William Shakespeare, Score: 0.5546\n",
            "Title: Life on the Mississippi (Penguin Classics), Author: Mark Twain, Score: 0.5540\n",
            "Title: Walden and Civil Disobedience, Author: Henry David Thoreau, Score: 0.5527\n",
            "Title: Frankenstein, Author: Mary Wollstonecraft Shelley, Score: 0.5524\n",
            "Title: Witch Alone: Thirteen Moons to Master Natural Magic, Author: Marian Green, Score: 0.5522\n",
            "Title: The Finishing School, Author: GAIL GODWIN, Score: 0.5519\n",
            "Title: Stranger (Everyman's Library Series), Author: ALBERT CAMUS, Score: 0.5501\n",
            "Title: Romeo und Julia. Zweisprachige Ausgabe. Englisch / Deutsch., Author: William Shakespeare, Score: 0.5496\n",
            "Title: A Confissao De Lucio, Author: Sa-Carneiro, Score: 0.5496\n",
            "Title: Confessions of a Mask (New Directions Paperbook), Author: Yukio Mishima, Score: 0.5493\n",
            "Title: Starship Titanic, Author: Douglas Adams, Score: 0.5486\n",
            "Title: As valkrias, Author: Paulo Coelho, Score: 0.5483\n",
            "Title: Selected Poems, 1908-1959, Author: Ezra Pound, Score: 0.5479\n",
            "Title: The Idiot, Author: Fyodor Dostoyevsky, Score: 0.5476\n",
            "Title: Museum of Science Book of Answers &amp; Questions, Author: Ann Rae Jonas, Score: 0.5455\n",
            "Title: Murder at the Vicarage (Agatha Christie Mysteries Collection (Library)), Author: Agatha Christie, Score: 0.5452\n",
            "Title: The Art of War, Author: Niccol Machiavelli, Score: 0.5449\n",
            "Title: Gulliver's Travels and Other Writings (Bantam Classics), Author: Jonathan Swift, Score: 0.5448\n",
            "Title: Adventures of Huckleberry Finn, Author: Mark Twain, Score: 0.5446\n",
            "Title: Still Life with Woodpecker, Author: TOM ROBBINS, Score: 0.5432\n",
            "Title: Signs of Life in the U.S.A.: Readings on Popular Culture for Writers, Author: Sonia Maasik, Score: 0.5429\n",
            "Title: Deutschland Deine Zukunft, Author: Franz Josef Strauss, Score: 0.5428\n",
            "Title: The Stuarts: A study in English kingship (British monarchy series), Author: J. P Kenyon, Score: 0.5423\n",
            "Title: The Immortal Dragon, Author: Michael Peterson, Score: 0.5413\n",
            "Title: BY PRICKING THUMB, Author: Agatha Christie, Score: 0.5410\n",
            "Title: Adventures of Tom Sawyer (Children's Classics), Author: MARK TWAIN, Score: 0.5408\n",
            "Title: Colony, Author: Ben Bova, Score: 0.5398\n",
            "Title: Cassells French &amp; English Dictionary, Author: J. H.  Douglas, Score: 0.5396\n",
            "Title: The Stranger, Author: Albert Camus, Score: 0.5391\n",
            "Title: Nabokov's Quartet, Author: Vladimir Vladimirovich Nabokov, Score: 0.5390\n",
            "Title: The marijuana papers; (Panther modern society), Author: David Solomon, Score: 0.5390\n",
            "Title: The Black Panthers Speak, Author: Philip S. Foner, Score: 0.5387\n",
            "Title: The Deeper Meaning of Liff: A Dictionary of Things That There Aren't Any Words for Yet, Author: Douglas Adams, Score: 0.5385\n"
          ]
        }
      ],
      "source": [
        "for rec in unique_recommendations[:100]:\n",
        "    print(f\"Title: {rec['title']}, Author: {rec['author']}, Score: {rec['score']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/0, Average Recall@25: 0.2348\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.23479582032851187"
            ]
          },
          "execution_count": 483,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_recall_at_k(two_towers, 0, 0, 25)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMmJhTtO2FCD1vDMILVH8qi",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
