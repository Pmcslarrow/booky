{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdUdtMTDALXY1xCLiEOBUt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dataset"
      ],
      "metadata": {
        "id": "L3c5igSoT9JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install kagglehub[pandas-datasets]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kVvdwqPSMaoM",
        "outputId": "7f15747f-8408-47ca-e2d3-201b77524f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "slTaZzDAMLUM",
        "outputId": "33e8f3ec-64b4-4328-83b8-13b283e16e1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-591258671.py:19: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  books_df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'book-recommendation-dataset' dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/kagglehub/pandas_datasets.py:91: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  result = read_function(\n",
            "/tmp/ipython-input-591258671.py:25: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  ratings_df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'book-recommendation-dataset' dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-591258671.py:31: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  users_df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'book-recommendation-dataset' dataset.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Load the latest version\n",
        "books_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Books.csv\",\n",
        ")\n",
        "\n",
        "ratings_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Ratings.csv\",\n",
        ")\n",
        "\n",
        "users_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Users.csv\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ratings_books = pd.merge(ratings_df, books_df, on=\"ISBN\", how='inner')\n",
        "df = pd.merge(df_ratings_books, users_df, on='User-ID')\n",
        "df['User-ID'] = df['User-ID'].astype(str)\n",
        "df['Year-Of-Publication'] = pd.to_numeric(df['Year-Of-Publication'], errors='coerce')\n",
        "df = df.dropna(subset=['Year-Of-Publication'])\n",
        "df = df.dropna(subset=['Age'])\n",
        "df = df[df['Book-Rating'] > 0]\n",
        "df = df[df['Age'] <= 100]\n",
        "df = df[df['Year-Of-Publication'] > 0]\n",
        "df['Book-Rating'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "93r6ilG_NQPw",
        "outputId": "ebbe720e-0709-4272-c6fe-327a2a7472ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    264742.000000\n",
              "mean          7.738848\n",
              "std           1.813809\n",
              "min           1.000000\n",
              "25%           7.000000\n",
              "50%           8.000000\n",
              "75%           9.000000\n",
              "max          10.000000\n",
              "Name: Book-Rating, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Book-Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>264742.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>7.738848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.813809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Book-Rating'].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kxkPFigfCob",
        "outputId": "20b8172a-b142-464f-ab68-f73261179686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Book-Rating\n",
            "1       862\n",
            "2      1525\n",
            "3      3260\n",
            "4      4991\n",
            "5     27203\n",
            "6     21047\n",
            "7     44663\n",
            "8     63605\n",
            "9     44496\n",
            "10    53090\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Age'].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5BbXb9fi-iW",
        "outputId": "35fb206c-4507-4b59-85c0-aa9ef73d99db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age\n",
            "0.0      203\n",
            "1.0      198\n",
            "2.0      191\n",
            "3.0       82\n",
            "4.0      143\n",
            "        ... \n",
            "96.0       2\n",
            "97.0      46\n",
            "98.0       1\n",
            "99.0       5\n",
            "100.0     51\n",
            "Name: count, Length: 96, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer\n",
        "import ast\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# User Tower -- User-ID, Age\n",
        "# Item Tower -- ISBN, Book-Title, Book-Author, Publisher, Year-Of-Publication\n",
        "\n",
        "class BookRecommenderDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data.sample(frac=0.05, random_state=42)\n",
        "        self.data = self.preprocess(self.data)\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        self.encoders = {} # {'Column name': {'value': idx, ...}, ...}\n",
        "        self.reverse_encoders = {} # {'Column name': {idx: 'value', ...}, ...}\n",
        "        self.scalers = {}\n",
        "\n",
        "        label_encoders = ['User-ID', 'ISBN', 'Book-Title', 'Book-Author', 'Publisher']\n",
        "        standard_scalers = ['Age', 'Year-Of-Publication']\n",
        "\n",
        "        for col in label_encoders:\n",
        "            unique_vals = data[col].astype(str).unique()\n",
        "            self.encoders[col] = {val: idx + 1 for idx, val in enumerate(unique_vals)}\n",
        "            self.reverse_encoders[col] = {idx + 1: val for idx, val in enumerate(unique_vals)}\n",
        "            data[col] = data[col].astype(str).map(self.encoders[col]).fillna(0).astype(int)\n",
        "\n",
        "        for col in standard_scalers:\n",
        "            self.scalers[col] = StandardScaler()\n",
        "            data[[col]] = self.scalers[col].fit_transform(data[[col]])\n",
        "\n",
        "        # Manually adding my own User-ID so I don't need to adjust nn.Embedding later\n",
        "        max_user_idx = max(self.encoders['User-ID'].values())\n",
        "        self.encoders['User-ID'][\"1234567890\"] = max_user_idx + 1\n",
        "        self.reverse_encoders['User-ID'][max_user_idx + 1] = \"1234567890\"\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        return {\n",
        "            'User-ID': torch.tensor(item['User-ID'], dtype=torch.long),\n",
        "            'User-Age': torch.tensor(item['Age'], dtype=torch.float32),\n",
        "            'Book-ISBN': torch.tensor(item['ISBN'], dtype=torch.long),\n",
        "            'Book-Title': torch.tensor(item['Book-Title'], dtype=torch.long),\n",
        "            'Book-Author': torch.tensor(item['Book-Author'], dtype=torch.long),\n",
        "            'Book-Publisher': torch.tensor(item['Publisher'], dtype=torch.long),\n",
        "            'Book-Year-Of-Publication': torch.tensor(item['Year-Of-Publication'], dtype=torch.float32),\n",
        "            'Rating': torch.tensor(item['Book-Rating'], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "dataset = BookRecommenderDataset(df)\n"
      ],
      "metadata": {
        "id": "hzV4YkTNPSuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.5 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "5UuengF0Tojk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vznGD0IGTxA7",
        "outputId": "b04fac10-b82a-48ed-f469-742a1cb3a823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'User-ID': tensor([5850, 1709,  321,   14, 6571, 4281,  700, 1000, 5001, 1509, 1226, 1081,\n",
              "         5866,  795, 1707, 1433,  846, 2575, 3539, 1913, 1160, 2416, 5471, 1678,\n",
              "         1284, 2506,   38,  798,  442, 1520, 2290, 1754, 1434,   38,  177, 5631,\n",
              "          504, 3926,    2,   42,  598, 4829, 1229, 5179, 2263,   97, 2571, 5156,\n",
              "          738, 3586,  752,  625, 4284,   14, 1289, 2505, 1635, 1678,  895, 4039,\n",
              "         4492, 2173, 2566, 5424]),\n",
              " 'User-Age': tensor([-1.5119, -1.1064,  0.8398, -0.2955,  0.3533, -0.1333,  0.3533,  1.1642,\n",
              "         -0.1333, -0.5388,  0.5965,  0.7587, -0.6199, -0.2955,  2.1373,  1.0020,\n",
              "          0.3533,  1.3264, -0.7010, -0.7821,  0.3533,  2.9482, -0.6199,  0.4343,\n",
              "          0.0289,  1.3264,  1.2453, -0.9442, -2.0796, -2.2417,  0.1100, -1.1064,\n",
              "         -1.5930,  1.2453, -0.3766, -1.0253, -1.5119, -1.9985,  0.8398, -0.4577,\n",
              "          0.8398,  1.7318,  0.0289,  0.0289, -0.7821, -0.0522, -0.3766, -0.2955,\n",
              "         -0.1333,  0.4343, -0.6199,  0.1911,  2.3806, -0.2955, -0.7010,  0.7587,\n",
              "          1.9751,  0.4343, -0.6199,  1.7318,  0.4343, -0.6199, -1.2686, -0.4577]),\n",
              " 'Book-ISBN': tensor([ 9253,  2058, 10524,  1885, 10776,  6153,  6667,  7342,  7562,  1791,\n",
              "          1427,  1788,  9284,   894,   335,  1688,   953,  3324, 10584,  2340,\n",
              "          9095,  5668,  9767,  3494,  1768,  3202,  1007,   897,   483,  7017,\n",
              "         10885,  2122,   908,  2166,   186,  8810,  2064,  5511,   443,  7182,\n",
              "          7035,  7226,  6821,  7909,  2832,  5842,  3320,  7852,   827,  4940,\n",
              "           845,   692,  6161,  6203,  7052,  4313,  1968,  8905, 10196,  5722,\n",
              "          2377,  5288,  3304,  7926]),\n",
              " 'Book-Title': tensor([ 8880,  2028, 10081,  1859,  8093,  5939,  6430,  7077,  7289,  1768,\n",
              "          1411,  1765,  8910,   887,   333,  1667,   945,  3253, 10137,  2305,\n",
              "          8728,  5481,  9370,  3418,  1745,  3137,   998,   890,   480,  6769,\n",
              "          9446,  2089,   901,  2132,   184,  8458,  2033,  5339,   440,  6927,\n",
              "          6786,  6967,  6581,  7615,  2780,  2775,   674,  7562,   821,  4792,\n",
              "           839,   686,  5947,  5987,  6801,  4191,  1940,  8550,  9782,  5532,\n",
              "          2342,  5123,  3234,  7632]),\n",
              " 'Book-Author': tensor([4074, 1580, 1459,  228,  567,  124,  720, 4657, 4763, 1406,  680,  612,\n",
              "         5702,  776,  307, 1340,  817, 2374, 6380, 1773, 5603,  551, 5951,  103,\n",
              "          392, 2308,  857,  636,  434, 3260,  589, 1023,  586, 1654,  171,  615,\n",
              "           22, 3644,  267,  418, 4490, 4592, 1163, 4956,  228,  683,  124, 2457,\n",
              "          721, 3329,  737,  613, 4011,   16, 4500, 1527,   84, 4242,  294, 1561,\n",
              "         1062,  543,  625, 1747]),\n",
              " 'Book-Publisher': tensor([ 340,   93,   43,    1,   53,   12,   43,  170,   45,  610,  260,  103,\n",
              "          581,  168,    5,  151,  406,  908,   27,  526,   21,   12,  815,  149,\n",
              "           17,   45,  421,   21,  179,  306,  120,  510,  394,  149,  133,  329,\n",
              "          265, 1252,    1,  114,    1,  352,   33, 1298,  173,   43,  173,  928,\n",
              "           23, 1163,  369,   58,   25,   99, 1461,  170,  102,  429,  422,   93,\n",
              "           58,   99,   71,  301]),\n",
              " 'Book-Year-Of-Publication': tensor([ 0.4313,  0.0187, -0.8064, -0.3938,  0.4313,  0.9814, -0.2563,  0.5688,\n",
              "          1.1189,  0.8439,  0.7063,  0.4313,  0.7063, -1.4940,  0.1562, -0.3938,\n",
              "          0.9814,  0.5688, -0.9439, -0.5314, -0.5314, -0.3938,  0.4313, -1.9066,\n",
              "         -0.2563,  0.7063,  1.1189, -0.8064,  0.2938, -0.6689,  0.0187, -0.6689,\n",
              "          0.4313,  0.9814,  0.4313,  0.7063,  0.7063,  1.1189,  0.5688, -0.1188,\n",
              "         -1.6315,  0.5688, -1.4940,  0.7063, -1.0815,  0.5688,  0.0187,  0.8439,\n",
              "         -0.9439, -2.4567,  0.1562,  0.9814, -0.3938,  0.4313,  0.9814,  0.8439,\n",
              "         -1.4940, -0.8064, -1.0815,  0.9814,  0.0187,  0.1562, -1.9066,  0.1562]),\n",
              " 'Rating': tensor([ 3.,  7., 10.,  5.,  8.,  5.,  7.,  6.,  6.,  8., 10.,  9.,  8., 10.,\n",
              "          9.,  8.,  6.,  9.,  8.,  6., 10.,  8.,  5.,  9.,  6.,  4.,  9.,  8.,\n",
              "          6.,  8.,  7.,  7., 10., 10.,  7.,  6.,  5., 10.,  5.,  5.,  8.,  8.,\n",
              "         10.,  8., 10.,  9.,  7.,  3.,  3.,  6.,  8., 10., 10.,  5., 10.,  5.,\n",
              "          8., 10., 10.,  8.,  8.,  8.,  8.,  6.])}"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two Tower Model for Recommendations"
      ],
      "metadata": {
        "id": "qjn33eipT_-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserTower(nn.Module):\n",
        "\n",
        "    # User Tower -- User-ID, Age\n",
        "\n",
        "    def __init__(self, num_users, embedding_dim=32):\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.user_mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim + 1, 128), # 1 embedding + 1 numerical\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, user_id, age):\n",
        "        \"\"\"\n",
        "        user_id: (batch,) int64\n",
        "        review_mean: (batch,) float32\n",
        "        \"\"\"\n",
        "        user_emb = self.user_embedding(user_id)\n",
        "        age = age.unsqueeze(1)\n",
        "        x = torch.cat([user_emb, age], dim=1)\n",
        "        return self.user_mlp(x)\n",
        "\n",
        "    def get_embedding(self, data):\n",
        "        return self.forward(data['User-ID'], data['User-Age'])\n"
      ],
      "metadata": {
        "id": "J4Rh2nDqUCxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ItemTower(nn.Module):\n",
        "    def __init__(self, num_isbn, num_titles, num_authors, num_publishers, embedding_dim=32):\n",
        "        super().__init__()\n",
        "\n",
        "        # Item Tower -- ISBN, Book-Title, Book-Author, Publisher, Year-Of-Publication\n",
        "\n",
        "        self.book_isbn_embedding = nn.Embedding(num_isbn, embedding_dim, padding_idx=0)\n",
        "        self.book_title_embedding = nn.Embedding(num_titles, embedding_dim, padding_idx=0)\n",
        "        self.book_author_embedding = nn.Embedding(num_authors, embedding_dim, padding_idx=0)\n",
        "        self.book_publisher_embedding = nn.Embedding(num_publishers, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.item_mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim * 4 + 1, 128),  # 4 embeddings + 1 numerical\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, isbn, book_title, book_author, book_publisher, book_year_of_publication):\n",
        "        book_isbn_emb = self.book_isbn_embedding(isbn)\n",
        "        book_title_emb = self.book_title_embedding(book_title)\n",
        "        book_author_emb = self.book_author_embedding(book_author)\n",
        "        book_publisher_emb = self.book_publisher_embedding(book_publisher)\n",
        "        book_year = book_year_of_publication.unsqueeze(1)\n",
        "\n",
        "        x = torch.cat([\n",
        "            book_isbn_emb,\n",
        "            book_title_emb,\n",
        "            book_author_emb,\n",
        "            book_publisher_emb,\n",
        "            book_year\n",
        "        ], dim=1)\n",
        "\n",
        "        return self.item_mlp(x)\n",
        "\n",
        "    def get_embedding(self, data):\n",
        "        return self.forward(\n",
        "            data['Book-ISBN'],\n",
        "            data['Book-Title'],\n",
        "            data['Book-Author'],\n",
        "            data['Book-Publisher'],\n",
        "            data['Book-Year-Of-Publication'],\n",
        "        )\n"
      ],
      "metadata": {
        "id": "hLSZeUSEUNBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoTowers(nn.Module):\n",
        "    def __init__(self, user_tower: UserTower, item_tower: ItemTower):\n",
        "        super().__init__()\n",
        "        self.user_tower = user_tower\n",
        "        self.item_tower = item_tower\n",
        "\n",
        "    def forward(self, data):\n",
        "        user_vector = self.user_tower.get_embedding(data)\n",
        "        item_vector = self.item_tower.get_embedding(data)\n",
        "        return (user_vector * item_vector).sum(dim=1)"
      ],
      "metadata": {
        "id": "l-fw4HLLUOXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_data = next(iter(train_loader))\n",
        "\n",
        "NUM_USERS = len(dataset.encoders['User-ID']) + 1\n",
        "NUM_ISBN = len(dataset.encoders['ISBN']) + 1\n",
        "NUM_TITLES = len(dataset.encoders['Book-Title']) + 1\n",
        "NUM_AUTHORS = len(dataset.encoders['Book-Author']) + 1\n",
        "NUM_PUBLISHERS = len(dataset.encoders['Publisher']) + 1\n",
        "\n",
        "user_tower = UserTower(num_users=NUM_USERS)\n",
        "\n",
        "item_tower = ItemTower(\n",
        "    num_isbn=NUM_ISBN,\n",
        "    num_titles=NUM_TITLES,\n",
        "    num_authors=NUM_AUTHORS,\n",
        "    num_publishers=NUM_PUBLISHERS,\n",
        ")\n",
        "\n",
        "two_towers = TwoTowers(\n",
        "    user_tower,\n",
        "    item_tower\n",
        ")"
      ],
      "metadata": {
        "id": "jlqHUau8WMjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "A3QGAbFDa_Lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Functions"
      ],
      "metadata": {
        "id": "bcsBmPJIb196"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Helpers ---\n",
        "def train_one_epoch(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, data in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(data)\n",
        "        targets = data['Rating']\n",
        "        loss = loss_fn(preds, targets)\n",
        "\n",
        "        # Sanity checks\n",
        "        assert not torch.isnan(preds).any(), \"NaN in predictions\"\n",
        "        assert not torch.isnan(targets).any(), \"NaN in targets\"\n",
        "        assert not torch.isinf(preds).any(), \"Inf in predictions\"\n",
        "        assert not torch.isinf(targets).any(), \"Inf in targets\"\n",
        "\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CLIP_GRAD_NORM)\n",
        "        optimizer.step()\n",
        "\n",
        "        yield batch_idx, loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_test_loss(model, test_loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for data in test_loader:\n",
        "        preds = model(data)\n",
        "        targets = data['Rating']\n",
        "        loss = loss_fn(preds, targets)\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "\n",
        "def evaluate_and_checkpoint(model, epoch, global_step, best_loss, counter, test_loader, loss_fn):\n",
        "    test_loss = calculate_test_loss(model, test_loader, loss_fn)\n",
        "\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        counter = 0\n",
        "        timestamp = datetime.datetime.now().strftime('%Y%m%d')\n",
        "        save_path = f\"{MODEL_SAVE_PATH}/two_towers_best_model_{timestamp}.pt\"\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"[Epoch {epoch}] âœ… Improved! Test Loss: {test_loss:.4f}. Model saved.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"[Epoch {epoch}] No improvement. Test Loss: {test_loss:.4f} ({counter}/{EARLY_STOPPING_PATIENCE})\")\n",
        "\n",
        "    return best_loss, counter"
      ],
      "metadata": {
        "id": "w5-fijrXb3Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Training Loop"
      ],
      "metadata": {
        "id": "Hn06WdV-b4Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf ./logs/"
      ],
      "metadata": {
        "id": "bGt_U5rrg7eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "\n",
        "EPOCHS = 200\n",
        "LOG_INTERVAL = 100\n",
        "CLIP_GRAD_NORM = 1.0\n",
        "LEARNING_RATE = 0.001\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models\"\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(two_towers.parameters(), lr=LEARNING_RATE)\n",
        "writer = SummaryWriter('./logs/')\n",
        "\n",
        "best_test_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "global_step = 0\n",
        "\n",
        "all_items_loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
        "items = next(iter(all_items_loader))\n",
        "item_vector = two_towers.item_tower.get_embedding(items)"
      ],
      "metadata": {
        "id": "GlzzNo4_mM0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    # -- Main Loop --\n",
        "    running_loss = 0.0\n",
        "    two_towers.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = two_towers(batch)\n",
        "        targets = batch['Rating']\n",
        "        loss = loss_fn(preds, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch}, Average Training Loss (based on rating): {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # -----------------------\n",
        "    # -- Recall@K Metric --\n",
        "    #\n",
        "    # Work in progress... Implement the recall@k metric to see out of the user's total ratings, how many of these appeared in the top-k.\n",
        "    #\n",
        "\n",
        "    # user_row = next(iter(train_loader))\n",
        "    # user_vector = two_towers.user_tower.get_embedding(user_row)\n",
        "\n",
        "    # scores = torch.matmul(item_vector, user_vector.T)\n",
        "    # scores = scores.T\n",
        "    #\n",
        "    # top_k = 20\n",
        "    # for i, user_id_encoded in enumerate(user_row['User-ID']):\n",
        "    #     top_scores, top_indices = torch.topk(scores[i], top_k)\n",
        "    #     user_id_decoded = dataset.reverse_encoders['User-ID'][user_id_encoded.item()]\n",
        "    #     ground_truth_books = df[df['User-ID'] == user_id_decoded]['ISBN']\n",
        "\n",
        "    #     print(f\"Top k books predicted for user {user_id_decoded}\")\n",
        "    #     print(top_indices)\n",
        "    #     print(f\"Book ratings for user {user_id_decoded}\")\n",
        "    #     print(ground_truth_books)\n",
        "    #\n",
        "    # -----------------------\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} completed.\")\n",
        "\n",
        "writer.close()\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhQ3lphfa_nU",
        "outputId": "5977ef6d-4e08-4670-eb2f-884d9324029d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Training Loss (based on rating): 11.6375\n",
            "Epoch 1/200 completed.\n",
            "Epoch 2, Average Training Loss (based on rating): 2.9920\n",
            "Epoch 2/200 completed.\n",
            "Epoch 3, Average Training Loss (based on rating): 2.0109\n",
            "Epoch 3/200 completed.\n",
            "Epoch 4, Average Training Loss (based on rating): 1.4614\n",
            "Epoch 4/200 completed.\n",
            "Epoch 5, Average Training Loss (based on rating): 1.0928\n",
            "Epoch 5/200 completed.\n",
            "Epoch 6, Average Training Loss (based on rating): 0.8664\n",
            "Epoch 6/200 completed.\n",
            "Epoch 7, Average Training Loss (based on rating): 0.6933\n",
            "Epoch 7/200 completed.\n",
            "Epoch 8, Average Training Loss (based on rating): 0.5886\n",
            "Epoch 8/200 completed.\n",
            "Epoch 9, Average Training Loss (based on rating): 0.5255\n",
            "Epoch 9/200 completed.\n",
            "Epoch 10, Average Training Loss (based on rating): 0.4998\n",
            "Epoch 10/200 completed.\n",
            "Epoch 11, Average Training Loss (based on rating): 0.4643\n",
            "Epoch 11/200 completed.\n",
            "Epoch 12, Average Training Loss (based on rating): 0.4495\n",
            "Epoch 12/200 completed.\n",
            "Epoch 13, Average Training Loss (based on rating): 0.4572\n",
            "Epoch 13/200 completed.\n",
            "Epoch 14, Average Training Loss (based on rating): 0.4381\n",
            "Epoch 14/200 completed.\n",
            "Epoch 15, Average Training Loss (based on rating): 0.4155\n",
            "Epoch 15/200 completed.\n",
            "Epoch 16, Average Training Loss (based on rating): 0.3899\n",
            "Epoch 16/200 completed.\n",
            "Epoch 17, Average Training Loss (based on rating): 0.3864\n",
            "Epoch 17/200 completed.\n",
            "Epoch 18, Average Training Loss (based on rating): 0.3716\n",
            "Epoch 18/200 completed.\n",
            "Epoch 19, Average Training Loss (based on rating): 0.3796\n",
            "Epoch 19/200 completed.\n",
            "Epoch 20, Average Training Loss (based on rating): 0.3599\n",
            "Epoch 20/200 completed.\n",
            "Epoch 21, Average Training Loss (based on rating): 0.3575\n",
            "Epoch 21/200 completed.\n",
            "Epoch 22, Average Training Loss (based on rating): 0.3430\n",
            "Epoch 22/200 completed.\n",
            "Epoch 23, Average Training Loss (based on rating): 0.3519\n",
            "Epoch 23/200 completed.\n",
            "Epoch 24, Average Training Loss (based on rating): 0.3338\n",
            "Epoch 24/200 completed.\n",
            "Epoch 25, Average Training Loss (based on rating): 0.3540\n",
            "Epoch 25/200 completed.\n",
            "Epoch 26, Average Training Loss (based on rating): 0.3309\n",
            "Epoch 26/200 completed.\n",
            "Epoch 27, Average Training Loss (based on rating): 0.3149\n",
            "Epoch 27/200 completed.\n",
            "Epoch 28, Average Training Loss (based on rating): 0.3036\n",
            "Epoch 28/200 completed.\n",
            "Epoch 29, Average Training Loss (based on rating): 0.2948\n",
            "Epoch 29/200 completed.\n",
            "Epoch 30, Average Training Loss (based on rating): 0.2819\n",
            "Epoch 30/200 completed.\n",
            "Epoch 31, Average Training Loss (based on rating): 0.2923\n",
            "Epoch 31/200 completed.\n",
            "Epoch 32, Average Training Loss (based on rating): 0.2857\n",
            "Epoch 32/200 completed.\n",
            "Epoch 33, Average Training Loss (based on rating): 0.2854\n",
            "Epoch 33/200 completed.\n",
            "Epoch 34, Average Training Loss (based on rating): 0.2686\n",
            "Epoch 34/200 completed.\n",
            "Epoch 35, Average Training Loss (based on rating): 0.2852\n",
            "Epoch 35/200 completed.\n",
            "Epoch 36, Average Training Loss (based on rating): 0.2578\n",
            "Epoch 36/200 completed.\n",
            "Epoch 37, Average Training Loss (based on rating): 0.2544\n",
            "Epoch 37/200 completed.\n",
            "Epoch 38, Average Training Loss (based on rating): 0.2502\n",
            "Epoch 38/200 completed.\n",
            "Epoch 39, Average Training Loss (based on rating): 0.2348\n",
            "Epoch 39/200 completed.\n",
            "Epoch 40, Average Training Loss (based on rating): 0.2341\n",
            "Epoch 40/200 completed.\n",
            "Epoch 41, Average Training Loss (based on rating): 0.2469\n",
            "Epoch 41/200 completed.\n",
            "Epoch 42, Average Training Loss (based on rating): 0.2484\n",
            "Epoch 42/200 completed.\n",
            "Epoch 43, Average Training Loss (based on rating): 0.2276\n",
            "Epoch 43/200 completed.\n",
            "Epoch 44, Average Training Loss (based on rating): 0.2306\n",
            "Epoch 44/200 completed.\n",
            "Epoch 45, Average Training Loss (based on rating): 0.2387\n",
            "Epoch 45/200 completed.\n",
            "Epoch 46, Average Training Loss (based on rating): 0.2241\n",
            "Epoch 46/200 completed.\n",
            "Epoch 47, Average Training Loss (based on rating): 0.2260\n",
            "Epoch 47/200 completed.\n",
            "Epoch 48, Average Training Loss (based on rating): 0.2094\n",
            "Epoch 48/200 completed.\n",
            "Epoch 49, Average Training Loss (based on rating): 0.2150\n",
            "Epoch 49/200 completed.\n",
            "Epoch 50, Average Training Loss (based on rating): 0.1946\n",
            "Epoch 50/200 completed.\n",
            "Epoch 51, Average Training Loss (based on rating): 0.1916\n",
            "Epoch 51/200 completed.\n",
            "Epoch 52, Average Training Loss (based on rating): 0.2049\n",
            "Epoch 52/200 completed.\n",
            "Epoch 53, Average Training Loss (based on rating): 0.2037\n",
            "Epoch 53/200 completed.\n",
            "Epoch 54, Average Training Loss (based on rating): 0.2055\n",
            "Epoch 54/200 completed.\n",
            "Epoch 55, Average Training Loss (based on rating): 0.1916\n",
            "Epoch 55/200 completed.\n",
            "Epoch 56, Average Training Loss (based on rating): 0.2101\n",
            "Epoch 56/200 completed.\n",
            "Epoch 57, Average Training Loss (based on rating): 0.2232\n",
            "Epoch 57/200 completed.\n",
            "Epoch 58, Average Training Loss (based on rating): 0.1963\n",
            "Epoch 58/200 completed.\n",
            "Epoch 59, Average Training Loss (based on rating): 0.1934\n",
            "Epoch 59/200 completed.\n",
            "Epoch 60, Average Training Loss (based on rating): 0.1783\n",
            "Epoch 60/200 completed.\n",
            "Epoch 61, Average Training Loss (based on rating): 0.1831\n",
            "Epoch 61/200 completed.\n",
            "Epoch 62, Average Training Loss (based on rating): 0.1907\n",
            "Epoch 62/200 completed.\n",
            "Epoch 63, Average Training Loss (based on rating): 0.1812\n",
            "Epoch 63/200 completed.\n",
            "Epoch 64, Average Training Loss (based on rating): 0.2044\n",
            "Epoch 64/200 completed.\n",
            "Epoch 65, Average Training Loss (based on rating): 0.1811\n",
            "Epoch 65/200 completed.\n",
            "Epoch 66, Average Training Loss (based on rating): 0.1665\n",
            "Epoch 66/200 completed.\n",
            "Epoch 67, Average Training Loss (based on rating): 0.1480\n",
            "Epoch 67/200 completed.\n",
            "Epoch 68, Average Training Loss (based on rating): 0.1492\n",
            "Epoch 68/200 completed.\n",
            "Epoch 69, Average Training Loss (based on rating): 0.1505\n",
            "Epoch 69/200 completed.\n",
            "Epoch 70, Average Training Loss (based on rating): 0.1533\n",
            "Epoch 70/200 completed.\n",
            "Epoch 71, Average Training Loss (based on rating): 0.1509\n",
            "Epoch 71/200 completed.\n",
            "Epoch 72, Average Training Loss (based on rating): 0.1603\n",
            "Epoch 72/200 completed.\n",
            "Epoch 73, Average Training Loss (based on rating): 0.1577\n",
            "Epoch 73/200 completed.\n",
            "Epoch 74, Average Training Loss (based on rating): 0.1407\n",
            "Epoch 74/200 completed.\n",
            "Epoch 75, Average Training Loss (based on rating): 0.1360\n",
            "Epoch 75/200 completed.\n",
            "Epoch 76, Average Training Loss (based on rating): 0.1385\n",
            "Epoch 76/200 completed.\n",
            "Epoch 77, Average Training Loss (based on rating): 0.1477\n",
            "Epoch 77/200 completed.\n",
            "Epoch 78, Average Training Loss (based on rating): 0.1582\n",
            "Epoch 78/200 completed.\n",
            "Epoch 79, Average Training Loss (based on rating): 0.1516\n",
            "Epoch 79/200 completed.\n",
            "Epoch 80, Average Training Loss (based on rating): 0.1535\n",
            "Epoch 80/200 completed.\n",
            "Epoch 81, Average Training Loss (based on rating): 0.1664\n",
            "Epoch 81/200 completed.\n",
            "Epoch 82, Average Training Loss (based on rating): 0.1550\n",
            "Epoch 82/200 completed.\n",
            "Epoch 83, Average Training Loss (based on rating): 0.1417\n",
            "Epoch 83/200 completed.\n",
            "Epoch 84, Average Training Loss (based on rating): 0.1311\n",
            "Epoch 84/200 completed.\n",
            "Epoch 85, Average Training Loss (based on rating): 0.1405\n",
            "Epoch 85/200 completed.\n",
            "Epoch 86, Average Training Loss (based on rating): 0.1242\n",
            "Epoch 86/200 completed.\n",
            "Epoch 87, Average Training Loss (based on rating): 0.1220\n",
            "Epoch 87/200 completed.\n",
            "Epoch 88, Average Training Loss (based on rating): 0.1262\n",
            "Epoch 88/200 completed.\n",
            "Epoch 89, Average Training Loss (based on rating): 0.1302\n",
            "Epoch 89/200 completed.\n",
            "Epoch 90, Average Training Loss (based on rating): 0.1271\n",
            "Epoch 90/200 completed.\n",
            "Epoch 91, Average Training Loss (based on rating): 0.1379\n",
            "Epoch 91/200 completed.\n",
            "Epoch 92, Average Training Loss (based on rating): 0.1280\n",
            "Epoch 92/200 completed.\n",
            "Epoch 93, Average Training Loss (based on rating): 0.1425\n",
            "Epoch 93/200 completed.\n",
            "Epoch 94, Average Training Loss (based on rating): 0.1332\n",
            "Epoch 94/200 completed.\n",
            "Epoch 95, Average Training Loss (based on rating): 0.1180\n",
            "Epoch 95/200 completed.\n",
            "Epoch 96, Average Training Loss (based on rating): 0.1278\n",
            "Epoch 96/200 completed.\n",
            "Epoch 97, Average Training Loss (based on rating): 0.1325\n",
            "Epoch 97/200 completed.\n",
            "Epoch 98, Average Training Loss (based on rating): 0.1209\n",
            "Epoch 98/200 completed.\n",
            "Epoch 99, Average Training Loss (based on rating): 0.1149\n",
            "Epoch 99/200 completed.\n",
            "Epoch 100, Average Training Loss (based on rating): 0.1120\n",
            "Epoch 100/200 completed.\n",
            "Epoch 101, Average Training Loss (based on rating): 0.1214\n",
            "Epoch 101/200 completed.\n",
            "Epoch 102, Average Training Loss (based on rating): 0.1123\n",
            "Epoch 102/200 completed.\n",
            "Epoch 103, Average Training Loss (based on rating): 0.1104\n",
            "Epoch 103/200 completed.\n",
            "Epoch 104, Average Training Loss (based on rating): 0.1177\n",
            "Epoch 104/200 completed.\n",
            "Epoch 105, Average Training Loss (based on rating): 0.1126\n",
            "Epoch 105/200 completed.\n",
            "Epoch 106, Average Training Loss (based on rating): 0.1196\n",
            "Epoch 106/200 completed.\n",
            "Epoch 107, Average Training Loss (based on rating): 0.1107\n",
            "Epoch 107/200 completed.\n",
            "Epoch 108, Average Training Loss (based on rating): 0.1058\n",
            "Epoch 108/200 completed.\n",
            "Epoch 109, Average Training Loss (based on rating): 0.1004\n",
            "Epoch 109/200 completed.\n",
            "Epoch 110, Average Training Loss (based on rating): 0.1046\n",
            "Epoch 110/200 completed.\n",
            "Epoch 111, Average Training Loss (based on rating): 0.1043\n",
            "Epoch 111/200 completed.\n",
            "Epoch 112, Average Training Loss (based on rating): 0.0932\n",
            "Epoch 112/200 completed.\n",
            "Epoch 113, Average Training Loss (based on rating): 0.1013\n",
            "Epoch 113/200 completed.\n",
            "Epoch 114, Average Training Loss (based on rating): 0.1095\n",
            "Epoch 114/200 completed.\n",
            "Epoch 115, Average Training Loss (based on rating): 0.1118\n",
            "Epoch 115/200 completed.\n",
            "Epoch 116, Average Training Loss (based on rating): 0.0960\n",
            "Epoch 116/200 completed.\n",
            "Epoch 117, Average Training Loss (based on rating): 0.1090\n",
            "Epoch 117/200 completed.\n",
            "Epoch 118, Average Training Loss (based on rating): 0.0957\n",
            "Epoch 118/200 completed.\n",
            "Epoch 119, Average Training Loss (based on rating): 0.0962\n",
            "Epoch 119/200 completed.\n",
            "Epoch 120, Average Training Loss (based on rating): 0.0952\n",
            "Epoch 120/200 completed.\n",
            "Epoch 121, Average Training Loss (based on rating): 0.0946\n",
            "Epoch 121/200 completed.\n",
            "Epoch 122, Average Training Loss (based on rating): 0.0958\n",
            "Epoch 122/200 completed.\n",
            "Epoch 123, Average Training Loss (based on rating): 0.1027\n",
            "Epoch 123/200 completed.\n",
            "Epoch 124, Average Training Loss (based on rating): 0.0973\n",
            "Epoch 124/200 completed.\n",
            "Epoch 125, Average Training Loss (based on rating): 0.0951\n",
            "Epoch 125/200 completed.\n",
            "Epoch 126, Average Training Loss (based on rating): 0.0889\n",
            "Epoch 126/200 completed.\n",
            "Epoch 127, Average Training Loss (based on rating): 0.0878\n",
            "Epoch 127/200 completed.\n",
            "Epoch 128, Average Training Loss (based on rating): 0.0874\n",
            "Epoch 128/200 completed.\n",
            "Epoch 129, Average Training Loss (based on rating): 0.0896\n",
            "Epoch 129/200 completed.\n",
            "Epoch 130, Average Training Loss (based on rating): 0.0908\n",
            "Epoch 130/200 completed.\n",
            "Epoch 131, Average Training Loss (based on rating): 0.1017\n",
            "Epoch 131/200 completed.\n",
            "Epoch 132, Average Training Loss (based on rating): 0.0986\n",
            "Epoch 132/200 completed.\n",
            "Epoch 133, Average Training Loss (based on rating): 0.0929\n",
            "Epoch 133/200 completed.\n",
            "Epoch 134, Average Training Loss (based on rating): 0.0986\n",
            "Epoch 134/200 completed.\n",
            "Epoch 135, Average Training Loss (based on rating): 0.0919\n",
            "Epoch 135/200 completed.\n",
            "Epoch 136, Average Training Loss (based on rating): 0.0924\n",
            "Epoch 136/200 completed.\n",
            "Epoch 137, Average Training Loss (based on rating): 0.0775\n",
            "Epoch 137/200 completed.\n",
            "Epoch 138, Average Training Loss (based on rating): 0.0836\n",
            "Epoch 138/200 completed.\n",
            "Epoch 139, Average Training Loss (based on rating): 0.0838\n",
            "Epoch 139/200 completed.\n",
            "Epoch 140, Average Training Loss (based on rating): 0.0802\n",
            "Epoch 140/200 completed.\n",
            "Epoch 141, Average Training Loss (based on rating): 0.0811\n",
            "Epoch 141/200 completed.\n",
            "Epoch 142, Average Training Loss (based on rating): 0.0851\n",
            "Epoch 142/200 completed.\n",
            "Epoch 143, Average Training Loss (based on rating): 0.0996\n",
            "Epoch 143/200 completed.\n",
            "Epoch 144, Average Training Loss (based on rating): 0.0960\n",
            "Epoch 144/200 completed.\n",
            "Epoch 145, Average Training Loss (based on rating): 0.0862\n",
            "Epoch 145/200 completed.\n",
            "Epoch 146, Average Training Loss (based on rating): 0.0765\n",
            "Epoch 146/200 completed.\n",
            "Epoch 147, Average Training Loss (based on rating): 0.0782\n",
            "Epoch 147/200 completed.\n",
            "Epoch 148, Average Training Loss (based on rating): 0.0784\n",
            "Epoch 148/200 completed.\n",
            "Epoch 149, Average Training Loss (based on rating): 0.0717\n",
            "Epoch 149/200 completed.\n",
            "Epoch 150, Average Training Loss (based on rating): 0.0799\n",
            "Epoch 150/200 completed.\n",
            "Epoch 151, Average Training Loss (based on rating): 0.0756\n",
            "Epoch 151/200 completed.\n",
            "Epoch 152, Average Training Loss (based on rating): 0.0732\n",
            "Epoch 152/200 completed.\n",
            "Epoch 153, Average Training Loss (based on rating): 0.0782\n",
            "Epoch 153/200 completed.\n",
            "Epoch 154, Average Training Loss (based on rating): 0.0840\n",
            "Epoch 154/200 completed.\n",
            "Epoch 155, Average Training Loss (based on rating): 0.0812\n",
            "Epoch 155/200 completed.\n",
            "Epoch 156, Average Training Loss (based on rating): 0.0816\n",
            "Epoch 156/200 completed.\n",
            "Epoch 157, Average Training Loss (based on rating): 0.0815\n",
            "Epoch 157/200 completed.\n",
            "Epoch 158, Average Training Loss (based on rating): 0.0755\n",
            "Epoch 158/200 completed.\n",
            "Epoch 159, Average Training Loss (based on rating): 0.0690\n",
            "Epoch 159/200 completed.\n",
            "Epoch 160, Average Training Loss (based on rating): 0.0651\n",
            "Epoch 160/200 completed.\n",
            "Epoch 161, Average Training Loss (based on rating): 0.0707\n",
            "Epoch 161/200 completed.\n",
            "Epoch 162, Average Training Loss (based on rating): 0.0711\n",
            "Epoch 162/200 completed.\n",
            "Epoch 163, Average Training Loss (based on rating): 0.0678\n",
            "Epoch 163/200 completed.\n",
            "Epoch 164, Average Training Loss (based on rating): 0.0657\n",
            "Epoch 164/200 completed.\n",
            "Epoch 165, Average Training Loss (based on rating): 0.0672\n",
            "Epoch 165/200 completed.\n",
            "Epoch 166, Average Training Loss (based on rating): 0.0660\n",
            "Epoch 166/200 completed.\n",
            "Epoch 167, Average Training Loss (based on rating): 0.0700\n",
            "Epoch 167/200 completed.\n",
            "Epoch 168, Average Training Loss (based on rating): 0.0675\n",
            "Epoch 168/200 completed.\n",
            "Epoch 169, Average Training Loss (based on rating): 0.0758\n",
            "Epoch 169/200 completed.\n",
            "Epoch 170, Average Training Loss (based on rating): 0.0757\n",
            "Epoch 170/200 completed.\n",
            "Epoch 171, Average Training Loss (based on rating): 0.0766\n",
            "Epoch 171/200 completed.\n",
            "Epoch 172, Average Training Loss (based on rating): 0.0747\n",
            "Epoch 172/200 completed.\n",
            "Epoch 173, Average Training Loss (based on rating): 0.0743\n",
            "Epoch 173/200 completed.\n",
            "Epoch 174, Average Training Loss (based on rating): 0.0685\n",
            "Epoch 174/200 completed.\n",
            "Epoch 175, Average Training Loss (based on rating): 0.0671\n",
            "Epoch 175/200 completed.\n",
            "Epoch 176, Average Training Loss (based on rating): 0.0677\n",
            "Epoch 176/200 completed.\n",
            "Epoch 177, Average Training Loss (based on rating): 0.0640\n",
            "Epoch 177/200 completed.\n",
            "Epoch 178, Average Training Loss (based on rating): 0.0655\n",
            "Epoch 178/200 completed.\n",
            "Epoch 179, Average Training Loss (based on rating): 0.0685\n",
            "Epoch 179/200 completed.\n",
            "Epoch 180, Average Training Loss (based on rating): 0.0763\n",
            "Epoch 180/200 completed.\n",
            "Epoch 181, Average Training Loss (based on rating): 0.0695\n",
            "Epoch 181/200 completed.\n",
            "Epoch 182, Average Training Loss (based on rating): 0.0654\n",
            "Epoch 182/200 completed.\n",
            "Epoch 183, Average Training Loss (based on rating): 0.0651\n",
            "Epoch 183/200 completed.\n",
            "Epoch 184, Average Training Loss (based on rating): 0.0637\n",
            "Epoch 184/200 completed.\n",
            "Epoch 185, Average Training Loss (based on rating): 0.0668\n",
            "Epoch 185/200 completed.\n",
            "Epoch 186, Average Training Loss (based on rating): 0.0702\n",
            "Epoch 186/200 completed.\n",
            "Epoch 187, Average Training Loss (based on rating): 0.0591\n",
            "Epoch 187/200 completed.\n",
            "Epoch 188, Average Training Loss (based on rating): 0.0605\n",
            "Epoch 188/200 completed.\n",
            "Epoch 189, Average Training Loss (based on rating): 0.0588\n",
            "Epoch 189/200 completed.\n",
            "Epoch 190, Average Training Loss (based on rating): 0.0638\n",
            "Epoch 190/200 completed.\n",
            "Epoch 191, Average Training Loss (based on rating): 0.0573\n",
            "Epoch 191/200 completed.\n",
            "Epoch 192, Average Training Loss (based on rating): 0.0615\n",
            "Epoch 192/200 completed.\n",
            "Epoch 193, Average Training Loss (based on rating): 0.0556\n",
            "Epoch 193/200 completed.\n",
            "Epoch 194, Average Training Loss (based on rating): 0.0577\n",
            "Epoch 194/200 completed.\n",
            "Epoch 195, Average Training Loss (based on rating): 0.0617\n",
            "Epoch 195/200 completed.\n",
            "Epoch 196, Average Training Loss (based on rating): 0.0658\n",
            "Epoch 196/200 completed.\n",
            "Epoch 197, Average Training Loss (based on rating): 0.0603\n",
            "Epoch 197/200 completed.\n",
            "Epoch 198, Average Training Loss (based on rating): 0.0597\n",
            "Epoch 198/200 completed.\n",
            "Epoch 199, Average Training Loss (based on rating): 0.0559\n",
            "Epoch 199/200 completed.\n",
            "Epoch 200, Average Training Loss (based on rating): 0.0613\n",
            "Epoch 200/200 completed.\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning on my own data\n",
        "\n",
        "- The item embeddings were learned during training. I will freeze the item tower.\n",
        "- I will use my own dataset of books I like to train the user embeddings."
      ],
      "metadata": {
        "id": "eQx6S_WNwJt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FineTuningDataset(Dataset):\n",
        "    def __init__(self, data, encoders, scalers):\n",
        "        self.data = data.copy()\n",
        "        self.encoders = encoders\n",
        "        self.scalers = scalers\n",
        "        self.preprocess()\n",
        "\n",
        "    def preprocess(self):\n",
        "        categorical_cols = ['User-ID', 'ISBN', 'Book-Title', 'Book-Author', 'Publisher']\n",
        "        for col in categorical_cols:\n",
        "            self.data[col] = self.data[col].astype(str).map(self.encoders[col]).fillna(0).astype(int)\n",
        "\n",
        "        numeric_cols = ['Age', 'Year-Of-Publication']\n",
        "        for col in numeric_cols:\n",
        "            scaled_vals = self.scalers[col].transform(self.data[[col]])\n",
        "            self.data[col] = scaled_vals.flatten()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        return {\n",
        "            'User-ID': torch.tensor(item['User-ID'], dtype=torch.long),\n",
        "            'User-Age': torch.tensor(item['Age'], dtype=torch.float32),\n",
        "            'Book-ISBN': torch.tensor(item['ISBN'], dtype=torch.long),\n",
        "            'Book-Title': torch.tensor(item['Book-Title'], dtype=torch.long),\n",
        "            'Book-Author': torch.tensor(item['Book-Author'], dtype=torch.long),\n",
        "            'Book-Publisher': torch.tensor(item['Publisher'], dtype=torch.long),\n",
        "            'Book-Year-Of-Publication': torch.tensor(item['Year-Of-Publication'], dtype=torch.float32),\n",
        "            'Rating': torch.tensor(item['Book-Rating'], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "fine_tuning_df = pd.read_csv(\"./drive/MyDrive/fine-tuning-book-set.txt\")\n",
        "fine_tuning_dataset = FineTuningDataset(fine_tuning_df, dataset.encoders, dataset.scalers)\n",
        "fine_tuning_loader = DataLoader(fine_tuning_dataset, batch_size=len(fine_tuning_dataset), shuffle=False)\n"
      ],
      "metadata": {
        "id": "QaAlkRdS5OK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(fine_tuning_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVIWocr_67TD",
        "outputId": "85da9e1e-926c-45bf-b2ac-077daf1b189c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'User-ID': tensor([6656, 6656, 6656, 6656, 6656, 6656, 6656, 6656, 6656, 6656, 6656, 6656,\n",
              "         6656, 6656, 6656, 6656]),\n",
              " 'User-Age': tensor([-1.1064, -1.1064, -1.1064, -1.1064, -1.1064, -1.1064, -1.1064, -1.1064,\n",
              "         -1.1064, -1.1064, -1.1064, -1.1064, -1.1064, -1.1064, -1.1064, -1.1064]),\n",
              " 'Book-ISBN': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'Book-Title': tensor([3120,    0,    0, 3187,    0, 6151, 5646,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0]),\n",
              " 'Book-Author': tensor([ 793,    0, 4880, 1884, 1884, 1884, 2624,  787,  787,    0,  118,    0,\n",
              "         5533,    4,    4, 1884]),\n",
              " 'Book-Publisher': tensor([  12,    0,   62,   95,   95,    1,   25,  206,  299, 1816, 1054,   62,\n",
              "            0,  149,  149,   95]),\n",
              " 'Book-Year-Of-Publication': tensor([-20.8847,   1.2564,  -7.4075,  -5.6197,  -5.7572,  -8.0951,  -5.0696,\n",
              "         -54.5776, -54.0275,   0.8439,  -0.2563,   0.4313,  -8.2326,  -3.6944,\n",
              "          -7.8200,  -5.7572]),\n",
              " 'Rating': tensor([5., 5., 3., 5., 5., 5., 4., 3., 5., 5., 3., 5., 5., 4., 5., 5.])}"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in two_towers.item_tower.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "FINE_TUNING_EPOCHS = 10\n",
        "for epoch in range(1, FINE_TUNING_EPOCHS + 1):\n",
        "\n",
        "    # -- Main Loop --\n",
        "    running_loss = 0.0\n",
        "    two_towers.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(fine_tuning_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = two_towers(batch)\n",
        "        targets = batch['Rating']\n",
        "        loss = loss_fn(preds, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch}, Average Training Loss (based on rating): {avg_loss:.4f}\")\n",
        "\n",
        "avg_loss = running_loss / len(train_loader)\n",
        "print(f\"Fine-Tuned Set {epoch}, Average Training Loss (based on rating): {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP5kcHRSwJZY",
        "outputId": "fc05dba1-6e61-48cc-f110-4bff1789761a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Training Loss (based on rating): 0.0564\n",
            "Epoch 2, Average Training Loss (based on rating): 0.0534\n",
            "Epoch 3, Average Training Loss (based on rating): 0.0496\n",
            "Epoch 4, Average Training Loss (based on rating): 0.0457\n",
            "Epoch 5, Average Training Loss (based on rating): 0.0426\n",
            "Epoch 6, Average Training Loss (based on rating): 0.0400\n",
            "Epoch 7, Average Training Loss (based on rating): 0.0375\n",
            "Epoch 8, Average Training Loss (based on rating): 0.0350\n",
            "Epoch 9, Average Training Loss (based on rating): 0.0327\n",
            "Epoch 10, Average Training Loss (based on rating): 0.0304\n",
            "Fine-Tuned Set 10, Average Training Loss (based on rating): 0.0304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_row = next(iter(fine_tuning_loader))\n",
        "user_vector = two_towers.user_tower.get_embedding(user_row)\n",
        "\n",
        "scores = torch.matmul(item_vector, user_vector.T)\n",
        "scores = scores.T\n",
        "\n",
        "topk_scores, topk_indices = torch.topk(scores, k=50, dim=1)\n",
        "print(topk_scores[0])\n",
        "print(topk_indices[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8puYkWCe8qYy",
        "outputId": "4ee5ad60-6b5f-440e-d36b-f6957d16d66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.0188, 1.9747, 1.9732, 1.9061, 1.8053, 1.7696, 1.7078, 1.6479, 1.6479,\n",
            "        1.6479, 1.6479, 1.6479, 1.6479, 1.6287, 1.5782, 1.5779, 1.5756, 1.5756,\n",
            "        1.5756, 1.5473, 1.5473, 1.5418, 1.5412, 1.5389, 1.5158, 1.5127, 1.4987,\n",
            "        1.4983, 1.4848, 1.4848, 1.4820, 1.4672, 1.4652, 1.4563, 1.4563, 1.4563,\n",
            "        1.4563, 1.4558, 1.4495, 1.4467, 1.4446, 1.4422, 1.4418, 1.4393, 1.4391,\n",
            "        1.4295, 1.4260, 1.4235, 1.4161, 1.4109], grad_fn=<SelectBackward0>)\n",
            "tensor([10515,  4893,  2018, 11214,  8978, 11918,  6423,  8944,  2326,  1300,\n",
            "         3220,  7048,  2893,  2961,  1890,  9754,  8463,  6034,   580,   620,\n",
            "         8080,  8191,  7015,  4680,   859,  9614,   564,  4231,  8339,  4528,\n",
            "         1347,  6032,  1943,   997,  1071,  7111, 13145,  6824,  8677,  5447,\n",
            "         1529, 13033,  5250,  6303,  3933,  6708,  8716,  4290,  8739,  1653])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recommended_books = df.iloc[topk_indices[0].tolist()]\n",
        "print(recommended_books[['Book-Title']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UaF8hbi-Eqm",
        "outputId": "8a5390ff-82f7-4357-a7b0-ed0c8e63e0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Book-Title\n",
            "37229                                         The Palace\n",
            "18732  Cowboy With A Secret (Harlequin American Roman...\n",
            "10439  The New Rabbi : A Congregation Searches for It...\n",
            "52068                                 Cyrano De Bergerac\n",
            "32322          The Far Shore of Time (Eschaton Sequence)\n",
            "54838                          Orlando. Eine Biographie.\n",
            "23081  All Change: Project Manager's Secret Handbook ...\n",
            "32180  The Exploits of the Incomparable Mulla Nasrudi...\n",
            "11289                             The Kitchen God's Wife\n",
            "8010   Michael KÃƒÂ¶hlmeiers Sagen des klassischen Alte...\n",
            "13448                      By the Sword (Kerowyn's Tale)\n",
            "25581               Flatland (Shambhala Pocket Classics)\n",
            "12490             Child of Thunder (Daw Book Collectors)\n",
            "12628  Bold Science: Seven Scientists Who Are Changin...\n",
            "10099             Tollivers Reisen. Stadtgeschichten IV.\n",
            "34729                                       White Plague\n",
            "30295    Divine Secrets of the Ya-Ya Sisterhood: A Novel\n",
            "21562                     Long Dark Tea Time of the Soul\n",
            "1706                         James Herriot's Cat Stories\n",
            "1905                                 The Conspiracy Club\n",
            "29161  Night Of The Living Dad (Baby Blues Scrapbook ...\n",
            "29649                                           Pen Pals\n",
            "25494                                 A Song for Arbonne\n",
            "18236                                        Wild Animus\n",
            "2663                It Was on Fire When I Lay Down on It\n",
            "34327                                        Rose Madder\n",
            "1654                                            The Main\n",
            "17098                        Reino del Dragon de Oro, El\n",
            "30007        No Language but a Cry (Laurel-Leaf Library)\n",
            "17897  Midnight Champagne : A Novel (Mysteries &amp; ...\n",
            "8320   Chicken Soup for the Soul (Chicken Soup for th...\n",
            "21559                               The Heart of a Woman\n",
            "10250                     The Piano Lesson (Plume Drama)\n",
            "3040                    A Wizard in Chaos (Rogue Wizard)\n",
            "3328         Dreammaker (Harlequin Historicals, No. 486)\n",
            "25873                             Snow Falling on Cedars\n",
            "58850                                        Paradox Rey\n",
            "24158                                          River God\n",
            "31076       Heart of darkness (Broadview literary texts)\n",
            "20000  The Last Madam: A Life in the New Orleans Unde...\n",
            "9070   El gust amarg de la cervesa (CollecciÃƒÂ³ ClÃƒÂ ss...\n",
            "58361  Alabama Gardener's Guide The What, Where, When...\n",
            "19571      All the Pretty Horses (Border Trilogy, Vol 1)\n",
            "22726  The Curious Incident of the Dog in the Night-T...\n",
            "15789                                       Dog Handling\n",
            "23795                                  Shepherds Abiding\n",
            "31397                                             Anthem\n",
            "17219                                      Das Superwieb\n",
            "31466                                        13th Valley\n",
            "9593                                  The Mystic Masseur\n"
          ]
        }
      ]
    }
  ]
}