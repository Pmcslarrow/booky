{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIc9Gw2PdC/gRYVuyCUs60"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dataset"
      ],
      "metadata": {
        "id": "L3c5igSoT9JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install kagglehub[pandas-datasets]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kVvdwqPSMaoM",
        "outputId": "7fcdf6be-3708-443c-b23c-ce845a86e5ac"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "slTaZzDAMLUM",
        "outputId": "ddfb073d-b3fd-46f8-8332-b6d2a33f9e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-885755572.py:20: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  books_df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'book-recommendation-dataset' dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/kagglehub/pandas_datasets.py:91: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  result = read_function(\n",
            "/tmp/ipython-input-885755572.py:26: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  ratings_df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'book-recommendation-dataset' dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-885755572.py:32: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  users_df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'book-recommendation-dataset' dataset.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Load the latest version\n",
        "books_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Books.csv\",\n",
        ")\n",
        "\n",
        "ratings_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Ratings.csv\",\n",
        ")\n",
        "\n",
        "users_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Users.csv\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ratings_books = pd.merge(ratings_df, books_df, on=\"ISBN\", how='inner')\n",
        "df = pd.merge(df_ratings_books, users_df, on='User-ID')\n",
        "df['User-ID'] = df['User-ID'].astype(str)\n",
        "df['Year-Of-Publication'] = pd.to_numeric(df['Year-Of-Publication'], errors='coerce')\n",
        "df = df.dropna(subset=['Year-Of-Publication'])\n",
        "df = df.dropna(subset=['Age'])\n",
        "df = df[df['Book-Rating'] > 0]\n",
        "df = df[df['Age'] <= 100]\n",
        "df = df[df['Year-Of-Publication'] > 0]\n",
        "df['Book-Rating'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "93r6ilG_NQPw",
        "outputId": "c302b499-fd00-489c-ea98-565c600b4f0d"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    264742.000000\n",
              "mean          7.738848\n",
              "std           1.813809\n",
              "min           1.000000\n",
              "25%           7.000000\n",
              "50%           8.000000\n",
              "75%           9.000000\n",
              "max          10.000000\n",
              "Name: Book-Rating, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Book-Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>264742.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>7.738848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.813809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "personal_df = pd.read_csv(\"./drive/MyDrive/fine-tuning-book-set.txt\")\n",
        "end_index = len(df)\n",
        "df = pd.concat([df, personal_df], ignore_index=True, sort=False)"
      ],
      "metadata": {
        "id": "8tSGkmikGSCI"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df['Book-Rating'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "_kxkPFigfCob"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df['Age'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "f5BbXb9fi-iW"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXl51Y5OQM0e",
        "outputId": "af72057e-7929-41e8-f7b4-295a8f9edd20"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer\n",
        "import ast\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# User Tower -- User-ID, Age\n",
        "# Item Tower -- ISBN, Book-Title, Book-Author, Publisher, Year-Of-Publication\n",
        "\n",
        "class BookRecommenderDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class for book recommendation tasks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataframe : pd.DataFrame\n",
        "        The input data containing user, item, and possibly interaction features.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        The processed version of the input dataframe.\n",
        "    encoders : dict\n",
        "        A dictionary mapping column names to fitted label encoders.\n",
        "    reverse_encoders : dict\n",
        "        A dictionary mapping column names to reverse label encoders (index to label).\n",
        "    scalers : dict\n",
        "        A dictionary mapping column names to fitted scalers for numerical features.\n",
        "    user_item_interaction : dict\n",
        "        A dictionary mapping encoded User-IDs to a list of positive example encoded ISBNs\n",
        "    negative_examples : int\n",
        "        An integer hyperparameter for the number of negative examples to use for contrastive learning\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, negative_examples=1):\n",
        "        self.encoders = {} # {'Column name': {'value': idx, ...}, ...}\n",
        "        self.reverse_encoders = {} # {'Column name': {idx: 'value', ...}, ...}\n",
        "        self.scalers = {}\n",
        "        self.user_item_interactions = {} # {encoded userid: [encoded ISBN]}\n",
        "        self.negative_examples = negative_examples\n",
        "        self.data = data.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
        "        self.preprocess(self.data)\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        self.encode_information()\n",
        "        self.generate_positives()\n",
        "\n",
        "    def generate_positives(self):\n",
        "        self.user_item_interaction = (\n",
        "            self.data\n",
        "            .groupby('User-ID')['ISBN']\n",
        "            .apply(list)\n",
        "            .to_dict()\n",
        "        )\n",
        "\n",
        "    def encode_information(self):\n",
        "        \"\"\"\n",
        "        Maps {key: index} pairs and StandardScaler for real valued numbers\n",
        "        \"\"\"\n",
        "        label_encoders = ['User-ID', 'ISBN', 'Book-Title', 'Book-Author', 'Publisher']\n",
        "        standard_scalers = ['Age', 'Year-Of-Publication']\n",
        "\n",
        "        for col in label_encoders:\n",
        "            unique_vals = self.data[col].astype(str).unique()\n",
        "            self.encoders[col] = {val: idx + 1 for idx, val in enumerate(unique_vals)}\n",
        "            self.reverse_encoders[col] = {idx + 1: val for idx, val in enumerate(unique_vals)}\n",
        "            self.data[col] = self.data[col].astype(str).map(self.encoders[col]).fillna(0).astype(int)\n",
        "\n",
        "        for col in standard_scalers:\n",
        "            self.scalers[col] = StandardScaler()\n",
        "            self.data[[col]] = self.scalers[col].fit_transform(self.data[[col]])\n",
        "\n",
        "        # Manually adding my own User-ID so I don't need to adjust nn.Embedding later\n",
        "        # max_user_idx = max(self.encoders['User-ID'].values())\n",
        "        # self.encoders['User-ID'][\"1234567890\"] = max_user_idx + 1\n",
        "        # self.reverse_encoders['User-ID'][max_user_idx + 1] = \"1234567890\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "            - user-tower data (User-ID and User-Age)\n",
        "            - positive item data (pos_item)\n",
        "            - negative item data (neg_items)\n",
        "            - Target rating\n",
        "        \"\"\"\n",
        "        positive_item = self.data.iloc[idx]\n",
        "        positive_user_id = positive_item['User-ID']\n",
        "        positive_isbns = self.user_item_interaction[positive_user_id]\n",
        "\n",
        "        negative_examples = []\n",
        "        while len(negative_examples) < self.negative_examples:\n",
        "            candidate = self.data.sample(n=1).iloc[0]\n",
        "            candidate_isbn = candidate['ISBN']\n",
        "            if candidate_isbn not in positive_isbns:\n",
        "                negative_examples.append(candidate)\n",
        "\n",
        "        output = {\n",
        "            'User-ID': torch.tensor(positive_item['User-ID'], dtype=torch.long),\n",
        "            'User-Age': torch.tensor(positive_item['Age'], dtype=torch.float32),\n",
        "            'Rating': torch.tensor(positive_item['Book-Rating'], dtype=torch.float32),\n",
        "\n",
        "            'pos_item': {\n",
        "                'Book-ISBN': torch.tensor(positive_item['ISBN'], dtype=torch.long),\n",
        "                'Book-Title': torch.tensor(positive_item['Book-Title'], dtype=torch.long),\n",
        "                'Book-Author': torch.tensor(positive_item['Book-Author'], dtype=torch.long),\n",
        "                'Book-Publisher': torch.tensor(positive_item['Publisher'], dtype=torch.long),\n",
        "                'Book-Year-Of-Publication': torch.tensor(positive_item['Year-Of-Publication'], dtype=torch.float32),\n",
        "            },\n",
        "\n",
        "            'neg_items': [\n",
        "                {\n",
        "                    'Book-ISBN': torch.tensor(neg['ISBN'], dtype=torch.long),\n",
        "                    'Book-Title': torch.tensor(neg['Book-Title'], dtype=torch.long),\n",
        "                    'Book-Author': torch.tensor(neg['Book-Author'], dtype=torch.long),\n",
        "                    'Book-Publisher': torch.tensor(neg['Publisher'], dtype=torch.long),\n",
        "                    'Book-Year-Of-Publication': torch.tensor(neg['Year-Of-Publication'], dtype=torch.float32),\n",
        "                }\n",
        "                for neg in negative_examples\n",
        "            ]\n",
        "        }\n",
        "        return output\n",
        "\n",
        "\n",
        "dataset = BookRecommenderDataset(df, negative_examples=3)\n"
      ],
      "metadata": {
        "id": "hzV4YkTNPSuc"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNCVwwNWwUc7",
        "outputId": "9a63e206-bda3-43e2-8f4d-8f80a9a7969e"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'User-ID': tensor(1),\n",
              " 'User-Age': tensor(-0.8395),\n",
              " 'Rating': tensor(8.),\n",
              " 'pos_item': {'Book-ISBN': tensor(1),\n",
              "  'Book-Title': tensor(1),\n",
              "  'Book-Author': tensor(1),\n",
              "  'Book-Publisher': tensor(1),\n",
              "  'Book-Year-Of-Publication': tensor(0.2982)},\n",
              " 'neg_items': [{'Book-ISBN': tensor(334),\n",
              "   'Book-Title': tensor(334),\n",
              "   'Book-Author': tensor(307),\n",
              "   'Book-Publisher': tensor(42),\n",
              "   'Book-Year-Of-Publication': tensor(-1.1344)},\n",
              "  {'Book-ISBN': tensor(1347),\n",
              "   'Book-Title': tensor(1335),\n",
              "   'Book-Author': tensor(1105),\n",
              "   'Book-Publisher': tensor(490),\n",
              "   'Book-Year-Of-Publication': tensor(0.4415)},\n",
              "  {'Book-ISBN': tensor(646),\n",
              "   'Book-Title': tensor(644),\n",
              "   'Book-Author': tensor(562),\n",
              "   'Book-Publisher': tensor(244),\n",
              "   'Book-Year-Of-Publication': tensor(1.0145)}]}"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.7 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "5UuengF0Tojk"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vznGD0IGTxA7",
        "outputId": "91a26a03-59ed-4b57-d4c9-cf3d92793198"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'User-ID': tensor([ 266,   10, 1932,  841,  120,  317, 1247, 1516,  694,  471, 1414, 1183,\n",
              "          609,  493,  128,  844, 1459,   50,  734,  148, 1871,  939,  392,  342,\n",
              "          236,  323,  198,  676, 1786,  432,  223,  618,  825,  391, 1708,  972,\n",
              "           56,  509,  737, 1386,  752, 1619,  122,  607, 1236, 1469, 1032, 1744,\n",
              "         1536, 1202,  536,  128,  832,  651, 1191,  896, 1361,  778,  584,  231,\n",
              "          203,  253, 1233, 1667]),\n",
              " 'User-Age': tensor([-0.5174, -0.3564,  0.7707, -1.0005,  0.5292, -0.6784, -0.1149,  0.9317,\n",
              "          0.1266, -0.0344, -0.5174,  1.6563,  0.2876, -1.1615,  0.6097, -0.8395,\n",
              "          1.4147,  1.2537, -0.4369,  1.0122,  0.7707,  0.0461, -0.1149, -1.0005,\n",
              "          1.5758,  3.1859, -0.5979,  0.2876, -0.1149,  1.6563, -0.9200, -0.5174,\n",
              "         -0.8395, -0.6784,  1.2537, -0.4369,  0.5292, -1.0810, -1.0005,  0.6902,\n",
              "          0.8512, -0.9200, -0.5174,  0.9317, -0.1149,  2.3003, -1.0005,  1.1732,\n",
              "         -0.6784,  0.0461, -1.1615,  0.6097, -1.1615, -0.0344,  2.0588, -0.6784,\n",
              "          0.4487, -0.1954,  2.2198, -0.1149, -1.0810,  0.1266,  2.4613, -0.2759]),\n",
              " 'Rating': tensor([ 5., 10.,  9.,  5.,  1.,  8.,  7.,  2.,  9., 10.,  8.,  8.,  5., 10.,\n",
              "         10.,  8.,  5.,  8., 10.,  9., 10.,  7.,  7.,  9.,  7., 10.,  6., 10.,\n",
              "          5.,  8.,  9., 10.,  8.,  9.,  7.,  6.,  9., 10., 10.,  8.,  8.,  8.,\n",
              "          8.,  7.,  5.,  8., 10., 10.,  9., 10.,  6.,  9.,  9.,  8.,  7., 10.,\n",
              "          5.,  9.,  9.,  7.,  5., 10., 10., 10.]),\n",
              " 'pos_item': {'Book-ISBN': tensor([ 285,  574,   37,  956,  904, 1264, 1448,  729,  787,  520, 1673, 1368,\n",
              "           278,  547,  997,  960, 1738, 1888,   88,  153, 2283, 1070, 2105,  371,\n",
              "           249,  185, 1399,  764, 2176,  476,  233,  694,  937, 2288, 2062, 1111,\n",
              "           564, 1502,  829, 1636,  850, 1953,  123, 1766,  533, 1754, 2249, 2114,\n",
              "          1850, 1394,  601, 1715,  944,  738, 1381, 1059, 1602,  878, 2056,  243,\n",
              "           211,  268, 1427, 2400]),\n",
              "  'Book-Title': tensor([ 285,  573,   37,  949,  897, 1253, 1434,  649,  781,  520, 1657, 1354,\n",
              "           278,  547,  989,  953, 1719, 1865,   88,  153, 2247, 1062, 2075,  371,\n",
              "           249,  185, 1385,  520, 2144,  476,  233,  690,  930, 2252, 2034, 1103,\n",
              "           563, 1487,  823, 1620,  843, 1929,  123, 1747,  533, 1735, 2215, 2084,\n",
              "          1829, 1380,  599, 1698,  937,  733, 1367, 1051, 1586,  871, 2028,  243,\n",
              "           211,  268, 1413, 2362]),\n",
              "  'Book-Author': tensor([ 266,  505,   37,  586,  773, 1047, 1178,  631,  680,  464, 1336, 1119,\n",
              "            14,  487,  849,  817, 1070, 1487,   87,  149, 1732,  907, 1631,  341,\n",
              "           236,  179, 1141,  661, 1677,  430,  220,  604,  799, 1736, 1603,  936,\n",
              "           498, 1217,  714, 1312,  730, 1529,  120, 1395,  476, 1137, 1713, 1638,\n",
              "           268,  549,  149, 1361,  805,  640,  307,  901,   98,  521, 1599,  230,\n",
              "           201,  251, 1163, 1814]),\n",
              "  'Book-Publisher': tensor([171,  52,  27, 390, 375, 327, 523, 102,  92, 252, 481, 159,  64, 261,\n",
              "           55, 391, 135,  39,  21, 107, 424,  70, 116,  38,  46, 119, 505, 332,\n",
              "          335, 211, 144, 309, 313,  66, 132, 429,  38, 536, 353, 300, 273, 640,\n",
              "           90,   1,  49,  68,  44, 212, 617,  36, 109, 152, 109, 109,  22, 423,\n",
              "           73,  53, 121, 151,  55, 132, 511, 385]),\n",
              "  'Book-Year-Of-Publication': tensor([ 0.4415, -0.7046,  0.8712, -0.4181,  0.4415,  1.1578, -0.5614,  1.0145,\n",
              "          -0.2748, -1.5642,  0.5847,  0.5847,  0.5847, -1.7074, -0.8479,  0.8712,\n",
              "           0.5847,  0.7280,  0.8712,  0.1549, -1.5642,  0.4415,  0.5847, -0.1316,\n",
              "          -1.1344,  1.0145, -0.5614,  0.5847,  0.4415, -1.4209,  0.8712, -1.4209,\n",
              "          -0.2748, -1.4209,  0.2982,  0.7280, -0.4181, -0.9911,  0.1549,  0.0117,\n",
              "           0.1549, -2.4237,  0.1549,  0.0117,  0.5847,  0.1549,  1.0145, -1.2777,\n",
              "           0.8712,  0.2982, -0.4181, -0.7046, -2.8535,  1.0145,  0.5847, -5.5755,\n",
              "          -1.7074, -2.4237,  0.5847, -0.5614, -0.4181, -1.4209,  0.7280, -0.8479])},\n",
              " 'neg_items': [{'Book-ISBN': tensor([1374, 1522, 2016,  463, 1082, 1923, 1025, 1167, 1613,  936, 2388, 1222,\n",
              "            670,  775, 2439, 2132,  419,  207,  272, 1707, 1856, 1413,  849, 1532,\n",
              "            943, 2214, 1477,  990,  290, 2423, 1374,  108,  661, 1327, 2442, 1235,\n",
              "            456,  867,  317,  588, 1003, 2153,  717,  198, 2035,  698, 2033,  615,\n",
              "           1259, 2070,  180,  992, 2011, 1846,  274, 1440,  969,   41, 2289, 1275,\n",
              "           1075, 1264, 1732,  987]),\n",
              "   'Book-Title': tensor([1360, 1507, 1988,  463, 1074, 1900, 1017, 1159, 1597,  929, 2350, 1211,\n",
              "            667,  769, 2400, 2101,  419,  207,  272, 1690, 1399, 1399,  842, 1517,\n",
              "            936, 2181, 1463,  983,  290, 2384, 1360,  108,  658, 1315, 2403, 1224,\n",
              "            456,  860,  317,  587,  995, 2121,  713,  198, 2007,  694, 2005,  613,\n",
              "           1248, 2042,  180,  984, 1983, 1825,  274, 1426,  962,   41, 2253, 1264,\n",
              "           1067, 1253, 1714,  980]),\n",
              "   'Book-Author': tensor([1124, 1230, 1570,  419,  140, 1510,  872,  467,  274,  798, 1806, 1011,\n",
              "            583,  211, 1839, 1650,  383,  198,  255, 1356,  181,  181,  729,  278,\n",
              "             73, 1696,  107,  843,  271, 1795, 1124,  105,  574, 1095, 1842, 1023,\n",
              "            414,  736,  292,  517,  854, 1360,  619,  191, 1582,  607, 1580,  537,\n",
              "             37, 1609,  175,  844, 1566, 1456,  257, 1171,  685,   41, 1737, 1056,\n",
              "            414, 1047, 1069,  841]),\n",
              "   'Book-Publisher': tensor([109,  51, 656,  55,  11,  43, 416,  73, 243, 211, 184, 458,  25,  46,\n",
              "            84,  49,   8, 132, 165, 172, 288,  44,  58,  53, 389, 212,  72, 404,\n",
              "            44,  46, 109,  72,   6, 484, 745,  46,   8,  73,  38,  73, 407,  33,\n",
              "           316,  78, 165, 311, 257, 280, 348, 263, 117, 405,  66, 184, 166, 517,\n",
              "           247,  33,  38, 345,   8, 327,  72,  38]),\n",
              "   'Book-Year-Of-Publication': tensor([-0.5614,  0.5847, -1.4209, -0.1316,  0.5847, -0.7046,  0.1549, -1.9940,\n",
              "            0.5847,  0.1549,  1.0145,  1.0145,  0.8712, -0.2748,  0.2982,  1.0145,\n",
              "            0.5847, -1.8507,  1.1578,  0.8712,  0.8712,  1.0145, -2.8535,  0.4415,\n",
              "            0.4415, -0.4181,  0.0117, -1.5642,  1.0145,  0.4415, -0.5614,  0.2982,\n",
              "            0.8712, -2.1372, -1.5642, -1.8507, -0.8479,  0.2982, -1.8507, -0.1316,\n",
              "            0.7280,  0.1549, -3.9996, -1.8507,  0.8712, -0.4181, -7.1513, -0.5614,\n",
              "            0.8712,  0.4415,  1.0145, -2.2805,  0.1549, -1.5642,  0.0117,  0.8712,\n",
              "           -1.5642,  0.1549, -2.1372, -0.4181,  0.1549,  1.1578, -0.7046,  0.8712])},\n",
              "  {'Book-ISBN': tensor([1963,   37,  915, 1777, 1917, 1015, 2229,   18,  894, 1447, 1469, 2274,\n",
              "           1356,  553,  509,  530,  409, 1882,  267, 1262,  673, 2207, 1878, 1387,\n",
              "           2162,  261, 1162, 1599, 1169, 1026, 2297,  208, 2008, 1001, 1578,  437,\n",
              "           1600,  699, 2345,   70, 1760,  303,  617, 1951, 1326, 1545, 1644, 1262,\n",
              "           1196, 2145,  990,   84, 2000,   59, 1009, 1553, 1483,  604, 1949,  798,\n",
              "           1085, 2367, 2235, 1377]),\n",
              "   'Book-Title': tensor([1937,   37,  908, 1758, 1894, 1007, 2195,   18,  887, 1433, 1455, 2239,\n",
              "           1343,  553,  509,  530,  409, 1859,  267, 1251,  669, 2174, 1855, 1373,\n",
              "           2130,  261, 1154, 1583, 1161, 1018, 2261,  208, 1980,  993, 1563,  437,\n",
              "           1584,  695, 1597,   70, 1741,  303,  615, 1927, 1314, 1530, 1628, 1251,\n",
              "           1187, 2113,  983,   84, 1972,   59, 1001, 1538, 1469,  602, 1925,  792,\n",
              "           1077, 2329, 2201, 1363]),\n",
              "   'Book-Author': tensor([1537,   37,  783, 1402, 1507,  864, 1701,   18,  764, 1177, 1193,  530,\n",
              "             60,  492,  455,  473,  373, 1483,  250, 1045,  585, 1692, 1480, 1133,\n",
              "            549,  246,  972, 1284,  977,  701,   73,   32, 1228,  852, 1267,  400,\n",
              "           1285,  608,  274,   69, 1390,  280,  539,  841, 1094, 1245, 1316, 1045,\n",
              "            994, 1657,  843,   83, 1559,   58,  115,   44,  321,  528, 1528,  689,\n",
              "            916,   60, 1703, 1126]),\n",
              "   'Book-Publisher': tensor([121,  27, 332, 193, 632,   1,  78,  15,  86, 522, 528, 309,  49, 263,\n",
              "            56, 256, 215,  12,  12, 469, 300, 697, 132, 174,  44,  57, 307, 499,\n",
              "           440,  43, 402,  55,  20, 145, 554, 224, 225,  46, 725,  55,  73,   8,\n",
              "           122,  38, 483, 546, 571, 469,  47, 184, 404,  63, 130,  46, 409,  26,\n",
              "           278, 279, 205, 145, 288,  17, 234,  55]),\n",
              "   'Book-Year-Of-Publication': tensor([-0.1316,  0.8712, -0.7046,  0.2982,  0.0117,  0.5847,  0.2982,  0.2982,\n",
              "            0.4415,  0.5847,  0.2982, -1.2777,  0.7280,  0.0117,  0.1549,  0.8712,\n",
              "            0.4415,  0.8712,  0.7280, -1.1344,  0.4415, -1.7074,  0.2982,  0.7280,\n",
              "            0.2982, -1.2777,  1.0145, -1.2777,  1.0145,  0.8712,  1.0145,  0.4415,\n",
              "            0.1549, -2.1372,  0.1549, -2.1372, -0.1316,  0.4415,  0.4415,  0.0117,\n",
              "            0.4415, -0.1316,  0.1549,  0.1549,  0.4415,  0.0117, -0.5614, -1.1344,\n",
              "           -0.4181, -0.1316, -1.5642, -1.5642,  0.7280,  0.1549, -3.2833, -0.1316,\n",
              "            0.5847,  0.7280, -2.4237,  0.8712, -1.4209,  1.0145,  0.1549,  0.4415])},\n",
              "  {'Book-ISBN': tensor([2257,    2, 2428, 2428, 1014,  174, 2481, 2019, 1189, 1131, 1739,  581,\n",
              "            106, 1041, 1258, 2127, 1622,  468,  789,  756, 1452, 1497,  368, 1979,\n",
              "           1394,  147,  182,  975, 1134,  903, 1828, 2143,  456, 1564, 1961,  338,\n",
              "           1825,  390, 1930,  356, 1706,  341, 2094, 1890,  904, 1616, 2094,   59,\n",
              "           2164, 1839, 2300,  854, 2144,  699, 1650, 2056,  613,  582, 1780, 2419,\n",
              "           2025, 1404, 1066, 1138]),\n",
              "   'Book-Title': tensor([2223,    2, 2389, 2389, 1006,  174, 2442, 1991, 1180, 1123, 1720,  580,\n",
              "            106, 1033, 1247, 2096, 1606,  468,  783,  751, 1438, 1482,  368, 1953,\n",
              "           1380,  147,  182,  968, 1126,  896, 1807, 2111,  456, 1549, 1935,  338,\n",
              "           1805,  390, 1907,  356, 1689,  341, 2064, 1867,  897, 1600, 2064,   59,\n",
              "           2132, 1818, 2264,  847, 2112,  695, 1634, 2028,  611,  581, 1761, 2380,\n",
              "           1997, 1390, 1058, 1130]),\n",
              "   'Book-Author': tensor([1719,    2, 1830, 1830,  181,  170, 1865, 1518,  693,  504, 1374,  511,\n",
              "            103,  885, 1043, 1648, 1301,  140,   38,  495, 1181, 1212,  338, 1547,\n",
              "            549,  143,  177,  830,  952,  138, 1442, 1655,  414, 1257, 1535,  310,\n",
              "           1441,  357, 1515,  327,  545,  313, 1566, 1489,  773, 1296, 1566,   58,\n",
              "            646, 1450,  390,  733, 1656,  608, 1320, 1599,  535,  512,  586, 1825,\n",
              "           1575, 1145,  905,  246]),\n",
              "   'Book-Publisher': tensor([  8,   2,  78,  78, 132,  73, 148, 220, 152,  46,  78, 272,  29, 419,\n",
              "           148, 682, 565,  29,   1,  38, 524, 532,  38, 644,  36, 104,  39, 398,\n",
              "           434, 278, 154, 281,   8, 510, 158, 191, 228,  29,  99, 199, 138, 192,\n",
              "           130, 323, 375,  35, 130,  46, 690, 613,  53,  29, 204,  46,  46, 121,\n",
              "            99, 185, 132, 282,  38, 107, 424, 437]),\n",
              "   'Book-Year-Of-Publication': tensor([ 0.8712,  1.0145,  0.1549,  0.1549, -0.7046,  0.2982,  0.4415, -0.2748,\n",
              "            0.8712, -1.2777,  0.0117, -0.7046, -0.5614, -0.2748,  0.4415,  0.1549,\n",
              "           -0.8479, -1.7074, -0.2748, -1.9940,  1.0145,  0.4415,  0.8712,  0.5847,\n",
              "            0.2982,  0.1549,  0.0117, -0.1316,  0.8712, -0.2748,  0.5847,  0.2982,\n",
              "           -0.8479, -0.9911, -1.2777, -0.1316,  0.7280, -0.1316,  0.5847, -1.2777,\n",
              "            0.0117,  0.7280,  0.2982, -0.5614,  0.4415,  0.0117,  0.2982,  0.1549,\n",
              "           -0.9911,  1.0145,  0.0117,  0.4415,  0.4415,  0.4415,  0.8712,  0.5847,\n",
              "            1.0145, -0.5614, -0.8479,  0.1549,  0.4415, -4.2861, -1.4209, -2.8535])}]}"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two Tower Model for Recommendations"
      ],
      "metadata": {
        "id": "qjn33eipT_-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserTower(nn.Module):\n",
        "\n",
        "    # User Tower -- User-ID, Age\n",
        "\n",
        "    def __init__(self, num_users, embedding_dim=32):\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.user_mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim + 1, 128), # 1 embedding + 1 numerical\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, user_id, age):\n",
        "        \"\"\"\n",
        "        user_id: (batch,) int64\n",
        "        review_mean: (batch,) float32\n",
        "        \"\"\"\n",
        "        user_emb = self.user_embedding(user_id)\n",
        "        age = age.unsqueeze(1)\n",
        "        x = torch.cat([user_emb, age], dim=1)\n",
        "        return self.user_mlp(x)\n",
        "\n",
        "    def get_embedding(self, data):\n",
        "        return self.forward(data['User-ID'], data['User-Age'])\n"
      ],
      "metadata": {
        "id": "J4Rh2nDqUCxf"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ItemTower(nn.Module):\n",
        "    def __init__(self, num_isbn, num_titles, num_authors, num_publishers, embedding_dim=32):\n",
        "        super().__init__()\n",
        "\n",
        "        # Item Tower -- ISBN, Book-Title, Book-Author, Publisher, Year-Of-Publication\n",
        "\n",
        "        self.book_isbn_embedding = nn.Embedding(num_isbn, embedding_dim, padding_idx=0)\n",
        "        self.book_title_embedding = nn.Embedding(num_titles, embedding_dim, padding_idx=0)\n",
        "        self.book_author_embedding = nn.Embedding(num_authors, embedding_dim, padding_idx=0)\n",
        "        self.book_publisher_embedding = nn.Embedding(num_publishers, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.item_mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim * 4 + 1, 128),  # 4 embeddings + 1 numerical\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, isbn, book_title, book_author, book_publisher, book_year_of_publication):\n",
        "        book_isbn_emb = self.book_isbn_embedding(isbn)\n",
        "        book_title_emb = self.book_title_embedding(book_title)\n",
        "        book_author_emb = self.book_author_embedding(book_author)\n",
        "        book_publisher_emb = self.book_publisher_embedding(book_publisher)\n",
        "        book_year = book_year_of_publication.unsqueeze(1)\n",
        "\n",
        "        x = torch.cat([\n",
        "            book_isbn_emb,\n",
        "            book_title_emb,\n",
        "            book_author_emb,\n",
        "            book_publisher_emb,\n",
        "            book_year\n",
        "        ], dim=1)\n",
        "\n",
        "        return self.item_mlp(x)\n",
        "\n",
        "    def get_embedding(self, data):\n",
        "        return self.forward(\n",
        "            data['Book-ISBN'],\n",
        "            data['Book-Title'],\n",
        "            data['Book-Author'],\n",
        "            data['Book-Publisher'],\n",
        "            data['Book-Year-Of-Publication'],\n",
        "        )\n"
      ],
      "metadata": {
        "id": "hLSZeUSEUNBY"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoTowers(nn.Module):\n",
        "    def __init__(self, user_tower: UserTower, item_tower: ItemTower):\n",
        "        super().__init__()\n",
        "        self.user_tower = user_tower\n",
        "        self.item_tower = item_tower\n",
        "\n",
        "    def forward(self, data):\n",
        "        user_vector = self.user_tower.get_embedding(data)\n",
        "        item_vector = self.item_tower.get_embedding(data['pos_item'])\n",
        "        return (user_vector * item_vector).sum(dim=1)"
      ],
      "metadata": {
        "id": "l-fw4HLLUOXk"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_data = next(iter(train_loader))\n",
        "\n",
        "NUM_USERS = len(dataset.encoders['User-ID']) + 1\n",
        "NUM_ISBN = len(dataset.encoders['ISBN']) + 1\n",
        "NUM_TITLES = len(dataset.encoders['Book-Title']) + 1\n",
        "NUM_AUTHORS = len(dataset.encoders['Book-Author']) + 1\n",
        "NUM_PUBLISHERS = len(dataset.encoders['Publisher']) + 1\n",
        "\n",
        "user_tower = UserTower(num_users=NUM_USERS)\n",
        "\n",
        "item_tower = ItemTower(\n",
        "    num_isbn=NUM_ISBN,\n",
        "    num_titles=NUM_TITLES,\n",
        "    num_authors=NUM_AUTHORS,\n",
        "    num_publishers=NUM_PUBLISHERS,\n",
        ")\n",
        "\n",
        "two_towers = TwoTowers(\n",
        "    user_tower,\n",
        "    item_tower\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "jlqHUau8WMjo"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "A3QGAbFDa_Lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Functions"
      ],
      "metadata": {
        "id": "bcsBmPJIb196"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Helpers ---\n",
        "def train_one_epoch(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, data in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(data)\n",
        "        targets = data['Rating']\n",
        "        loss = loss_fn(preds, targets)\n",
        "\n",
        "        # Sanity checks\n",
        "        assert not torch.isnan(preds).any(), \"NaN in predictions\"\n",
        "        assert not torch.isnan(targets).any(), \"NaN in targets\"\n",
        "        assert not torch.isinf(preds).any(), \"Inf in predictions\"\n",
        "        assert not torch.isinf(targets).any(), \"Inf in targets\"\n",
        "\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CLIP_GRAD_NORM)\n",
        "        optimizer.step()\n",
        "\n",
        "        yield batch_idx, loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_test_loss(model, test_loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for data in test_loader:\n",
        "        preds = model(data)\n",
        "        targets = data['Rating']\n",
        "        loss = loss_fn(preds, targets)\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "\n",
        "def evaluate_and_checkpoint(model, epoch, global_step, best_loss, counter, test_loader, loss_fn):\n",
        "    test_loss = calculate_test_loss(model, test_loader, loss_fn)\n",
        "\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        counter = 0\n",
        "        timestamp = datetime.datetime.now().strftime('%Y%m%d')\n",
        "        save_path = f\"{MODEL_SAVE_PATH}/two_towers_best_model_{timestamp}.pt\"\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"[Epoch {epoch}] ✅ Improved! Test Loss: {test_loss:.4f}. Model saved.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"[Epoch {epoch}] No improvement. Test Loss: {test_loss:.4f} ({counter}/{EARLY_STOPPING_PATIENCE})\")\n",
        "\n",
        "    return best_loss, counter\n",
        "\n",
        "def move_batch_to_device(batch, device):\n",
        "    batch['User-ID'] = batch['User-ID'].to(device)\n",
        "    batch['User-Age'] = batch['User-Age'].to(device)\n",
        "    batch['Rating'] = batch['Rating'].to(device)\n",
        "\n",
        "    for key in batch['pos_item']:\n",
        "        batch['pos_item'][key] = batch['pos_item'][key].to(device)\n",
        "\n",
        "    for neg_item in batch['neg_items']:\n",
        "        for key in neg_item:\n",
        "            neg_item[key] = neg_item[key].to(device)\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "w5-fijrXb3Nm"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Training Loop"
      ],
      "metadata": {
        "id": "Hn06WdV-b4Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf ./logs/"
      ],
      "metadata": {
        "id": "bGt_U5rrg7eA"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "\n",
        "EPOCHS = 30\n",
        "LOG_INTERVAL = 100\n",
        "CLIP_GRAD_NORM = 1.0\n",
        "LEARNING_RATE = 0.001\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models\"\n",
        "\n",
        "# loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(two_towers.parameters(), lr=LEARNING_RATE)\n",
        "writer = SummaryWriter('./logs/')\n",
        "\n",
        "best_test_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "global_step = 0"
      ],
      "metadata": {
        "id": "GlzzNo4_mM0E"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    # -- Main Loop --\n",
        "    running_train_loss = 0.0\n",
        "    two_towers.train()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = move_batch_to_device(batch, device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        user_embedding = two_towers.user_tower.get_embedding(batch)  # [B, D]\n",
        "\n",
        "        # Calculate similarity between user and positive example\n",
        "        pos_sim = two_towers(batch)\n",
        "\n",
        "        # Calculate similarity between user and negative examples\n",
        "        #\n",
        "        # Logits (first position will be positive that we want to maximize, and everything else is negative)\n",
        "        # For example, if we have 1 positive example and 1 negative example our logits are:\n",
        "        #   [[pos, neg], ..batch_size..]\n",
        "        #\n",
        "        # Now the label we want cross entropy to maximize is in the 0th position (positive)\n",
        "        #   labels = [0, ..batch_size..]\n",
        "        #\n",
        "        #\n",
        "        neg_sims = []\n",
        "        for neg_item_dict in batch['neg_items']:\n",
        "            neg_emb = two_towers.item_tower.get_embedding(neg_item_dict)  # [B, D]\n",
        "            neg_sim = torch.sum(user_embedding * neg_emb, dim=1)  # [B]\n",
        "            neg_sims.append(neg_sim)\n",
        "\n",
        "        # Contrastive Loss\n",
        "        logits = torch.stack([pos_sim] + neg_sims, dim=1)\n",
        "        labels = torch.zeros(logits.size(0), dtype=torch.long, device=device)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    two_towers.eval()\n",
        "    running_test_loss = 0.0\n",
        "    for batch in test_loader:\n",
        "        batch = move_batch_to_device(batch, device)\n",
        "\n",
        "        pos_sim = two_towers(batch)\n",
        "        user_embedding = two_towers.user_tower.get_embedding(batch)\n",
        "        neg_embedding = two_towers.item_tower.get_embedding(batch['neg_items'][0])\n",
        "        neg_sim = torch.sum(user_embedding * neg_embedding, dim=1)\n",
        "\n",
        "        logits = torch.stack([pos_sim, neg_sim], dim=1)\n",
        "        labels = torch.zeros(logits.size(0), dtype=torch.long)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        running_test_loss += loss.item()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        avg_train_loss = running_train_loss / len(train_loader)\n",
        "        avg_test_loss = running_test_loss / len(test_loader)\n",
        "        print(f\"Epoch {epoch}/{EPOCHS}, Average Training Loss: {avg_train_loss:.4f}, \")\n",
        "        print(f\"Epoch {epoch}/{EPOCHS}, Average Test Loss: {avg_test_loss:.4f}, \")\n",
        "\n",
        "writer.close()\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "UhQ3lphfa_nU",
        "outputId": "98dc8c08-0739-43c4-9ef3-e8a0c44ec98d"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Average Training Loss: 0.7528, \n",
            "Epoch 1/30, Average Test Loss: 0.9950, \n",
            "Epoch 2/30, Average Training Loss: 0.5444, \n",
            "Epoch 2/30, Average Test Loss: 1.1698, \n",
            "Epoch 3/30, Average Training Loss: 0.4222, \n",
            "Epoch 3/30, Average Test Loss: 1.3075, \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1176461159.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtwo_towers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mrunning_test_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmove_batch_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-345725808.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mnegative_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_examples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mcandidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mcandidate_isbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ISBN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_isbn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_isbns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   6117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6118\u001b[0m         \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6119\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4131\u001b[0m             )\n\u001b[1;32m   4132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4133\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   4134\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4135\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         return self.reindex_indexer(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             new_blocks = [\n\u001b[0;32m--> 688\u001b[0;31m                 blk.take_nd(\n\u001b[0m\u001b[1;32m    689\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m     def _unstack(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block_same_class\u001b[0;34m(self, values, placement, refs)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     def make_block_same_class(\n\u001b[1;32m    294\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # -----------------------\n",
        "    # -- Recall@K Metric --\n",
        "    #\n",
        "    # Work in progress... Implement the recall@k metric to see out of the user's total ratings, how many of these appeared in the top-k.\n",
        "    #\n",
        "\n",
        "    # user_row = next(iter(train_loader))\n",
        "    # user_vector = two_towers.user_tower.get_embedding(user_row)\n",
        "\n",
        "    # scores = torch.matmul(item_vector, user_vector.T)\n",
        "    # scores = scores.T\n",
        "    #\n",
        "    # top_k = 20\n",
        "    # for i, user_id_encoded in enumerate(user_row['User-ID']):\n",
        "    #     top_scores, top_indices = torch.topk(scores[i], top_k)\n",
        "    #     user_id_decoded = dataset.reverse_encoders['User-ID'][user_id_encoded.item()]\n",
        "    #     ground_truth_books = df[df['User-ID'] == user_id_decoded]['ISBN']\n",
        "\n",
        "    #     print(f\"Top k books predicted for user {user_id_decoded}\")\n",
        "    #     print(top_indices)\n",
        "    #     print(f\"Book ratings for user {user_id_decoded}\")\n",
        "    #     print(ground_truth_books)\n",
        "    #\n",
        "    # -----------------------"
      ],
      "metadata": {
        "id": "ZlE4VAFWDDGs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}