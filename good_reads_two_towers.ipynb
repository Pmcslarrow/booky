{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmRfoyfvneme32aUZuaxL/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dataset"
      ],
      "metadata": {
        "id": "L3c5igSoT9JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install kagglehub[pandas-datasets]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kVvdwqPSMaoM",
        "outputId": "985eb868-b7f4-44f3-ff3b-7aa9d2a0aef1"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "slTaZzDAMLUM",
        "outputId": "64399adf-1663-4606-b289-bd476d279f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-885755572.py:20: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  books_df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'book-recommendation-dataset' dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/kagglehub/pandas_datasets.py:91: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  result = read_function(\n",
            "/tmp/ipython-input-885755572.py:26: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  ratings_df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'book-recommendation-dataset' dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-885755572.py:32: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  users_df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'book-recommendation-dataset' dataset.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Load the latest version\n",
        "books_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Books.csv\",\n",
        ")\n",
        "\n",
        "ratings_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Ratings.csv\",\n",
        ")\n",
        "\n",
        "users_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Users.csv\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ratings_books = pd.merge(ratings_df, books_df, on=\"ISBN\", how='inner')\n",
        "df = pd.merge(df_ratings_books, users_df, on='User-ID')\n",
        "df['User-ID'] = df['User-ID'].astype(str)\n",
        "df['Year-Of-Publication'] = pd.to_numeric(df['Year-Of-Publication'], errors='coerce')\n",
        "df = df.dropna(subset=['Year-Of-Publication'])\n",
        "df = df.dropna(subset=['Age'])\n",
        "df = df[df['Book-Rating'] > 0]\n",
        "df = df[df['Age'] <= 100]\n",
        "df = df[df['Year-Of-Publication'] > 0]\n",
        "df['Book-Rating'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "93r6ilG_NQPw",
        "outputId": "cca20a35-a128-4157-89ca-07419dafa75d"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    264742.000000\n",
              "mean          7.738848\n",
              "std           1.813809\n",
              "min           1.000000\n",
              "25%           7.000000\n",
              "50%           8.000000\n",
              "75%           9.000000\n",
              "max          10.000000\n",
              "Name: Book-Rating, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Book-Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>264742.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>7.738848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.813809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "personal_df = pd.read_csv(\"./drive/MyDrive/fine-tuning-book-set.txt\")\n",
        "end_index = len(df)\n",
        "df = pd.concat([df, personal_df], ignore_index=True, sort=False)"
      ],
      "metadata": {
        "id": "8tSGkmikGSCI"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Book-Rating'].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kxkPFigfCob",
        "outputId": "91725f06-e455-40a0-9fe5-7499db0051e0"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Book-Rating\n",
            "1       862\n",
            "2      1525\n",
            "3      3263\n",
            "4      4993\n",
            "5     27214\n",
            "6     21047\n",
            "7     44663\n",
            "8     63605\n",
            "9     44496\n",
            "10    53090\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Age'].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5BbXb9fi-iW",
        "outputId": "d4a2c6e7-a3e4-48ba-a3f4-00c130f4aed9"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age\n",
            "0.0      203\n",
            "1.0      198\n",
            "2.0      191\n",
            "3.0       82\n",
            "4.0      143\n",
            "        ... \n",
            "96.0       2\n",
            "97.0      46\n",
            "98.0       1\n",
            "99.0       5\n",
            "100.0     51\n",
            "Name: count, Length: 96, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer\n",
        "import ast\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# User Tower -- User-ID, Age\n",
        "# Item Tower -- ISBN, Book-Title, Book-Author, Publisher, Year-Of-Publication\n",
        "\n",
        "class BookRecommenderDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class for book recommendation tasks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataframe : pd.DataFrame\n",
        "        The input data containing user, item, and possibly interaction features.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        The processed version of the input dataframe.\n",
        "    encoders : dict\n",
        "        A dictionary mapping column names to fitted label encoders.\n",
        "    reverse_encoders : dict\n",
        "        A dictionary mapping column names to reverse label encoders (index to label).\n",
        "    scalers : dict\n",
        "        A dictionary mapping column names to fitted scalers for numerical features.\n",
        "    user_item_interaction : dict\n",
        "        A dictionary mapping encoded User-IDs to a list of positive example encoded ISBNs\n",
        "    negative_examples : int\n",
        "        An integer hyperparameter for the number of negative examples to use for contrastive learning\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, negative_examples=1):\n",
        "        self.encoders = {} # {'Column name': {'value': idx, ...}, ...}\n",
        "        self.reverse_encoders = {} # {'Column name': {idx: 'value', ...}, ...}\n",
        "        self.scalers = {}\n",
        "        self.user_item_interactions = {} # {encoded userid: [encoded ISBN]}\n",
        "        self.negative_examples = negative_examples\n",
        "        self.data = data.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
        "        self.preprocess(self.data)\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        self.encode_information()\n",
        "        self.generate_positives()\n",
        "\n",
        "    def generate_positives(self):\n",
        "        self.user_item_interaction = (\n",
        "            self.data\n",
        "            .groupby('User-ID')['ISBN']\n",
        "            .apply(list)\n",
        "            .to_dict()\n",
        "        )\n",
        "\n",
        "    def encode_information(self):\n",
        "        \"\"\"\n",
        "        Maps {key: index} pairs and StandardScaler for real valued numbers\n",
        "        \"\"\"\n",
        "        label_encoders = ['User-ID', 'ISBN', 'Book-Title', 'Book-Author', 'Publisher']\n",
        "        standard_scalers = ['Age', 'Year-Of-Publication']\n",
        "\n",
        "        for col in label_encoders:\n",
        "            unique_vals = self.data[col].astype(str).unique()\n",
        "            self.encoders[col] = {val: idx + 1 for idx, val in enumerate(unique_vals)}\n",
        "            self.reverse_encoders[col] = {idx + 1: val for idx, val in enumerate(unique_vals)}\n",
        "            self.data[col] = self.data[col].astype(str).map(self.encoders[col]).fillna(0).astype(int)\n",
        "\n",
        "        for col in standard_scalers:\n",
        "            self.scalers[col] = StandardScaler()\n",
        "            self.data[[col]] = self.scalers[col].fit_transform(self.data[[col]])\n",
        "\n",
        "        # Manually adding my own User-ID so I don't need to adjust nn.Embedding later\n",
        "        # max_user_idx = max(self.encoders['User-ID'].values())\n",
        "        # self.encoders['User-ID'][\"1234567890\"] = max_user_idx + 1\n",
        "        # self.reverse_encoders['User-ID'][max_user_idx + 1] = \"1234567890\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "            - user-tower data (User-ID and User-Age)\n",
        "            - positive item data (pos_item)\n",
        "            - negative item data (neg_items)\n",
        "            - Target rating\n",
        "        \"\"\"\n",
        "        positive_item = self.data.iloc[idx]\n",
        "        positive_user_id = positive_item['User-ID']\n",
        "        positive_isbns = self.user_item_interaction[positive_user_id]\n",
        "\n",
        "        negative_examples = []\n",
        "        while len(negative_examples) < self.negative_examples:\n",
        "            candidate = self.data.sample(n=1).iloc[0]\n",
        "            candidate_isbn = candidate['ISBN']\n",
        "            if candidate_isbn not in positive_isbns:\n",
        "                negative_examples.append(candidate)\n",
        "\n",
        "        output = {\n",
        "            'User-ID': torch.tensor(positive_item['User-ID'], dtype=torch.long),\n",
        "            'User-Age': torch.tensor(positive_item['Age'], dtype=torch.float32),\n",
        "            'Rating': torch.tensor(positive_item['Book-Rating'], dtype=torch.float32),\n",
        "\n",
        "            'pos_item': {\n",
        "                'Book-ISBN': torch.tensor(positive_item['ISBN'], dtype=torch.long),\n",
        "                'Book-Title': torch.tensor(positive_item['Book-Title'], dtype=torch.long),\n",
        "                'Book-Author': torch.tensor(positive_item['Book-Author'], dtype=torch.long),\n",
        "                'Book-Publisher': torch.tensor(positive_item['Publisher'], dtype=torch.long),\n",
        "                'Book-Year-Of-Publication': torch.tensor(positive_item['Year-Of-Publication'], dtype=torch.float32),\n",
        "            },\n",
        "\n",
        "            'neg_items': [\n",
        "                {\n",
        "                    'Book-ISBN': torch.tensor(neg['ISBN'], dtype=torch.long),\n",
        "                    'Book-Title': torch.tensor(neg['Book-Title'], dtype=torch.long),\n",
        "                    'Book-Author': torch.tensor(neg['Book-Author'], dtype=torch.long),\n",
        "                    'Book-Publisher': torch.tensor(neg['Publisher'], dtype=torch.long),\n",
        "                    'Book-Year-Of-Publication': torch.tensor(neg['Year-Of-Publication'], dtype=torch.float32),\n",
        "                }\n",
        "                for neg in negative_examples\n",
        "            ]\n",
        "        }\n",
        "        return output\n",
        "\n",
        "\n",
        "dataset = BookRecommenderDataset(df, negative_examples=3)\n"
      ],
      "metadata": {
        "id": "hzV4YkTNPSuc"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNCVwwNWwUc7",
        "outputId": "ca551c0d-ea50-4505-915e-85fd946b9681"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'User-ID': tensor(1),\n",
              " 'User-Age': tensor(-0.8395),\n",
              " 'Rating': tensor(8.),\n",
              " 'pos_item': {'Book-ISBN': tensor(1),\n",
              "  'Book-Title': tensor(1),\n",
              "  'Book-Author': tensor(1),\n",
              "  'Book-Publisher': tensor(1),\n",
              "  'Book-Year-Of-Publication': tensor(0.2982)},\n",
              " 'neg_items': [{'Book-ISBN': tensor(1929),\n",
              "   'Book-Title': tensor(1906),\n",
              "   'Book-Author': tensor(1514),\n",
              "   'Book-Publisher': tensor(635),\n",
              "   'Book-Year-Of-Publication': tensor(-0.9911)},\n",
              "  {'Book-ISBN': tensor(2233),\n",
              "   'Book-Title': tensor(2199),\n",
              "   'Book-Author': tensor(61),\n",
              "   'Book-Publisher': tensor(701),\n",
              "   'Book-Year-Of-Publication': tensor(0.1549)},\n",
              "  {'Book-ISBN': tensor(1099),\n",
              "   'Book-Title': tensor(1091),\n",
              "   'Book-Author': tensor(926),\n",
              "   'Book-Publisher': tensor(38),\n",
              "   'Book-Year-Of-Publication': tensor(-1.2777)}]}"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.7 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "5UuengF0Tojk"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vznGD0IGTxA7",
        "outputId": "79f72db3-3542-41cc-82d4-86aa67dc44e8"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'User-ID': tensor([ 619, 1622,  769,  661,  789,   89,  297, 1644, 1008,  554,   50,  502,\n",
              "         1013, 1998,  128, 1181,   10,  395,   98, 1091,  801, 1193, 1652,  355,\n",
              "          140,  287,  570,  207,  467, 1230,  865, 1413,   23,  614,  805,  364,\n",
              "          726,  569,  834, 1734, 1404,  640,  704, 1891,  421, 1281, 1814,  498,\n",
              "         1727,  860,  147,  247, 1211, 1630,  736,   50, 1348,   18,  129, 1249,\n",
              "           62, 1682, 1641,  505]),\n",
              " 'User-Age': tensor([ 0.2876, -0.5979,  1.2537,  1.4147,  0.0461, -0.2759,  0.0461, -0.6784,\n",
              "          3.0249, -0.6784,  1.2537,  0.4487, -0.0344, -0.4369,  0.6097, -0.4369,\n",
              "         -0.3564,  0.8512,  1.1732,  1.0927, -1.0810, -1.0810, -0.6784, -1.0005,\n",
              "         -1.1615, -0.6784, -0.1954,  2.0588,  1.6563, -0.2759, -0.9200, -1.0005,\n",
              "          0.0461, -0.4369,  0.2876,  1.4953,  1.8173,  1.2537, -0.5174,  0.5292,\n",
              "          1.3342,  2.1393,  0.6902, -1.2420, -0.3564, -1.0005, -2.2081, -1.4835,\n",
              "         -0.5174, -0.5174, -0.7590,  0.3682, -0.2759, -0.7590, -0.5979,  1.2537,\n",
              "         -0.1954,  0.0461,  1.0122, -0.5174,  0.8512, -0.8395,  2.8639,  1.2537]),\n",
              " 'Rating': tensor([ 5.,  6.,  7.,  9.,  7., 10.,  8.,  8., 10.,  5.,  8., 10.,  8.,  5.,\n",
              "          5., 10., 10.,  5.,  8.,  5.,  6., 10.,  8., 10.,  9.,  8.,  4.,  5.,\n",
              "         10.,  8.,  9.,  7.,  9.,  6.,  9.,  3.,  8.,  7.,  1., 10.,  8., 10.,\n",
              "         10., 10.,  7.,  8.,  5.,  5.,  1., 10.,  3.,  5.,  5.,  3.,  5.,  9.,\n",
              "          7.,  8., 10.,  7.,  8.,  6.,  7.,  7.]),\n",
              " 'pos_item': {'Book-ISBN': tensor([ 695, 1959,  868,  748,  889,   89,  318, 1987, 1149,  737, 1127,  558,\n",
              "          1686, 2458, 2089, 1366, 2245, 1818,   98, 1250,  906, 1084, 1997,  388,\n",
              "           144,  306, 2117,  214,  515, 1424, 2485, 1672,   23,  689, 1494,  398,\n",
              "           820, 1902,  946, 2099, 1662, 1807, 1896, 2314,  464, 1501, 2212,  502,\n",
              "          2090,  979,  152, 1790, 2322, 1969, 2172, 2298, 1590,   18, 1108, 1451,\n",
              "          2030, 2029, 1984,  699]),\n",
              "  'Book-Title': tensor([ 691, 1171,  861,  743,  882,   89,  318, 1961, 1141,  732, 1119,  558,\n",
              "          1670, 2419, 2060, 1352, 2211, 1798,   98, 1239,  899, 1076, 1970,  388,\n",
              "           144,  306, 2086,  214,  515, 1410, 2446, 1656,   23,  685, 1479,  398,\n",
              "           814, 1879,  939, 2069, 1646, 1787, 1873, 2277,  464, 1486, 2179,  502,\n",
              "          2061,  972,  152, 1770, 2285, 1943, 2140, 2262, 1575,   18, 1100, 1437,\n",
              "          2002, 2001, 1958,  695]),\n",
              "  'Book-Author': tensor([ 492,  140,  170,  649,  234,   88,  293, 1200,  963,  639,  947,  333,\n",
              "          1344,   88, 1622,  234,  356, 1435,   96, 1036,  775,  915, 1557,  356,\n",
              "           140,  283, 1640,  203,  460, 1160,  490, 1335,   23,  599, 1209,  364,\n",
              "           628, 1498,  807,  455,  549,   60,  181, 1360,  420, 1216, 1695,  451,\n",
              "           307,  834,  148, 1413, 1759,  195,  740, 1744, 1278,   18,  933, 1180,\n",
              "          1578,  579, 1550,  608]),\n",
              "  'Book-Publisher': tensor([263,  44,  42, 113,  73,  46, 148, 237,  84,  73,  55, 212,  40,  73,\n",
              "          189,  73, 705, 550,  72,  28, 199,  49,  56,  47,   1, 164, 136,  26,\n",
              "           78, 203,  86, 580,  19,  58, 169, 112, 109, 629,   8,  65,  44, 549,\n",
              "          205, 323, 235, 535, 423, 248,  42,  61,  72, 211,  66, 130, 107, 411,\n",
              "          557,  15,  50,  70, 447,  44, 244,  46]),\n",
              "  'Book-Year-Of-Publication': tensor([ 0.4415, -1.5642,  0.8712,  0.2982, -0.8479, -0.2748,  0.5847, -0.1316,\n",
              "          -1.1344, -0.1316,  0.8712, -0.2748,  0.8712, -0.5614, -3.1400,  1.0145,\n",
              "          -1.4209,  1.1578,  0.0117, -0.2748, -0.4181,  0.8712,  0.8712,  0.7280,\n",
              "           0.7280,  1.0145,  0.0117,  1.1578, -0.9911, -0.1316,  0.0117, -3.4266,\n",
              "           0.1549,  0.5847,  0.7280,  0.7280,  0.8712,  0.4415,  0.5847,  0.1549,\n",
              "           0.2982,  0.8712, -0.2748,  0.5847, -0.9911, -1.1344, -1.2777,  0.8712,\n",
              "           0.4415,  0.1549, -1.4209,  0.5847, -0.4181, -0.1316,  0.8712,  0.8712,\n",
              "           0.5847,  0.2982, -0.9911,  0.2982,  0.4415,  0.5847,  0.8712,  0.4415])},\n",
              " 'neg_items': [{'Book-ISBN': tensor([1521, 1441,   84,   39, 1754,  966, 1538,  135, 1228,   33, 1467, 2414,\n",
              "           1913,  988,  139, 1967,   91, 2438, 1190, 2227,  168, 1094,    6,  280,\n",
              "           1746, 1122,  302,  762, 2287, 2248,   37, 1314,  291, 1767, 1583,  127,\n",
              "           1471,   90, 1435, 2195,  214,  298, 1108, 1187,  485, 1191,  383, 1624,\n",
              "            551,  582,  216, 1713,  815, 1839,  996, 1845, 1313, 1018,  599,  637,\n",
              "            388,  141, 1322, 1056]),\n",
              "   'Book-Title': tensor([1506, 1427,   84,   39, 1735,  959, 1523,  135, 1217,   33, 1453, 2375,\n",
              "           1890,  981,  139, 1941,   91, 2399, 1181, 2193,  168, 1086,    6,  280,\n",
              "           1727, 1114,  302,  757, 2251, 2214,   37, 1302,  291, 1748, 1568,  127,\n",
              "           1457,   90, 1421, 2163,  214,  298, 1100, 1178,  485, 1182,  383, 1608,\n",
              "            551,  581,  216, 1696,  809, 1818,  988, 1824, 1301, 1010,  597,  635,\n",
              "            388,  141, 1310, 1048]),\n",
              "   'Book-Author': tensor([ 717, 1172,   83,   39, 1137,   14, 1240,  132, 1017,   33,   38, 1824,\n",
              "           1504,  842,  136, 1539,   61, 1838,  140, 1699,  164,  922,    6,  262,\n",
              "           1379,  944,  141,  140, 1735, 1712,   37,  832,  272,  703,  654,  124,\n",
              "             32,   89, 1168, 1687,  203,  278,  933,  988,   73,  990,  351, 1303,\n",
              "            490,  512,    3,  107,  703, 1450,  848,   53, 1084,   38,  525,  553,\n",
              "            356,  138, 1091,  898]),\n",
              "   'Book-Publisher': tensor([ 49, 518,  63,  31,  68, 393, 544,  73, 461,  27,   1, 372, 631,  55,\n",
              "            28, 643,  68,   4, 447, 253,  38,  56,   1,  55, 590, 289, 177,  29,\n",
              "           558,  38,  27, 212, 174,  44,  39,  94,  26,  67, 130, 109,  26,  53,\n",
              "            50,  53,  51,  68,   3, 566, 262, 185,   1, 177,  44, 613, 240,  44,\n",
              "            60,   1,  38, 258,  47,  23, 232, 422]),\n",
              "   'Book-Year-Of-Publication': tensor([ 0.5847,  0.4415, -1.5642,  0.2982,  0.1549, -0.5614,  1.1578,  0.5847,\n",
              "           -2.4237, -0.7046, -0.1316,  0.7280, -0.7046,  1.0145,  0.7280,  0.4415,\n",
              "           -3.4266,  0.5847,  0.7280, -1.5642, -0.1316, -0.5614,  0.1549,  0.8712,\n",
              "            0.7280, -0.2748,  1.0145, -2.4237,  0.7280, -2.1372,  0.8712, -0.4181,\n",
              "           -1.1344,  0.8712,  0.1549,  0.5847, -0.2748,  0.4415,  0.8712, -0.5614,\n",
              "            1.1578,  0.7280, -0.9911,  0.5847,  0.5847,  0.7280,  1.0145,  0.8712,\n",
              "           -0.2748, -0.5614,  0.5847,  0.7280, -0.4181,  1.0145, -0.1316,  0.7280,\n",
              "            0.8712, -0.5614, -1.5642, -0.7046,  0.7280,  0.0117, -1.2777, -2.1372])},\n",
              "  {'Book-ISBN': tensor([  46,   15,  864,  324, 1480,  388,  596, 2058,  168,  709, 2115,  398,\n",
              "            455, 1544, 1735, 1501,  497, 2336, 2204, 2297, 1467, 1989, 1870, 2048,\n",
              "           1331, 1369,  245, 1938, 1223, 1272,  645,  970, 1070, 1352,  248, 1125,\n",
              "            779, 1630, 1001, 1741, 2205,  699,  391,  278, 2017, 1892, 1043, 1305,\n",
              "           1287,   37,  139,  404,  873, 1219,  704,  456,  938, 1745,   95,  669,\n",
              "           1924, 1603, 1956, 2275]),\n",
              "   'Book-Title': tensor([  46,   15,  857,  324, 1466,  388,  594, 2030,  168,  705,  163,  398,\n",
              "            455, 1529, 1240, 1486,  497, 2299, 2171, 2261, 1453, 1963, 1848, 2020,\n",
              "           1319, 1355,  245, 1914, 1212, 1261,  643,  963, 1062, 1339,  248, 1117,\n",
              "            773, 1614,  993, 1722, 2172,  695,  391,  278, 1989, 1869, 1035, 1293,\n",
              "           1276,   37,  139,  404,  866, 1209,  700,  456,  931, 1726,   95,  666,\n",
              "           1901, 1587, 1931, 2240]),\n",
              "   'Book-Author': tensor([  46,   15,  287,  298,  895,  356,  523,  542,  164,  613,  159,  364,\n",
              "            140,  383, 1037, 1216,  447, 1770, 1690,   73,   38, 1553, 1473, 1592,\n",
              "            140, 1120,  232, 1520, 1012, 1053,  561,  825,  907, 1108,  235,  877,\n",
              "            672,  914,  852, 1375, 1691,  608,  358,   14,  218, 1491,  887, 1077,\n",
              "           1065,   37,  136,  369,  675, 1009,  610,  414,  800, 1378,   93,  582,\n",
              "           1511,   88, 1531,  333]),\n",
              "   'Book-Publisher': tensor([ 38,  12,  49, 184,  65,  47,  99, 668,  38, 312,  51, 112,  29,   8,\n",
              "           244, 535, 245,  51, 223, 402,   1, 648, 537, 429, 414,  78, 153,  89,\n",
              "           459,  73,  70, 109,  70, 492,  70,  22,  29,  91, 145, 102, 696,  46,\n",
              "           208,  64, 481, 530,  58, 478, 109,  27,  28, 109, 112, 457,  53,   8,\n",
              "            60, 150,  71, 299,  55,  73, 227,  72]),\n",
              "   'Book-Year-Of-Publication': tensor([ 0.2982,  1.1578, -0.1316, -0.1316,  1.1578,  0.7280,  0.1549, -0.2748,\n",
              "           -0.1316, -1.7074, -1.1344,  0.7280, -1.4209, -0.5614,  0.4415, -1.1344,\n",
              "           -1.4209,  0.4415,  1.1578,  1.0145, -0.1316, -0.1316,  0.7280, -0.2748,\n",
              "           -0.2748,  0.4415,  0.2982,  0.5847,  0.7280,  1.0145,  1.1578, -0.4181,\n",
              "            0.4415,  0.2982, -0.2748, -0.5614,  0.1549,  0.2982, -2.1372,  0.8712,\n",
              "           -0.8479,  0.4415,  0.7280,  0.5847, -2.2805, -0.4181, -0.8479,  0.8712,\n",
              "            0.1549,  0.8712,  0.7280,  1.0145,  0.4415,  0.5847,  0.7280, -0.8479,\n",
              "            1.1578,  0.0117,  0.4415,  0.1549, -0.9911,  1.0145, -0.2748, -0.4181])},\n",
              "  {'Book-ISBN': tensor([  82,  407, 2049, 1601, 1002, 1709, 1569,   88, 1094, 2461, 1848, 1986,\n",
              "           1571,  413, 1026, 2129, 1499,  445,   18, 1156, 1087, 1135, 1335,  219,\n",
              "           1302,  943,  168, 1433,  253, 1805, 1379, 2228,  264,  673, 1096,  697,\n",
              "           1858,  921, 2021, 1652,  913, 2416, 2000, 1700, 2284,   10, 1045,   57,\n",
              "            469,  489, 1301,  125,  695,  130, 2372,    7, 1572, 2399, 1249, 2075,\n",
              "           1839, 1407,  947, 2057]),\n",
              "   'Book-Title': tensor([  82,  407, 2021, 1585,  994, 1692, 1554,   88, 1086, 2422, 1827, 1960,\n",
              "           1556,  413, 1018, 2098, 1484,  445,   18, 1148, 1079, 1127, 1323,  219,\n",
              "           1290,  936,  168, 1419,  253, 1785, 1365, 2194,  264,  669, 1088,  693,\n",
              "           1836,  914, 1993, 1636,  906, 2377, 1972, 1683, 2248,   10, 1037,   57,\n",
              "            469,  489, 1289,  125,  691,  130, 2334,    7, 1557, 2361, 1238, 2047,\n",
              "           1818, 1393,  940, 2029]),\n",
              "   'Book-Author': tensor([  81,  371, 1593,  491,  853, 1358, 1262,   87,  922, 1854, 1458,   70,\n",
              "            140,  377,  701, 1208, 1214,  405,   18,  383,  274,  953, 1100,  207,\n",
              "            138,   73,  164, 1167,   99,   32, 1128, 1700,  248,  585,  924,  606,\n",
              "           1464,  390, 1572, 1321,   96,  567, 1559, 1353,   32,   10,  888,   56,\n",
              "            424,  353, 1075,  122,  492,  127, 1794,    7, 1051,  298, 1035,  287,\n",
              "           1450, 1148,  808, 1600]),\n",
              "   'Book-Publisher': tensor([ 61, 214, 429,  73,   8, 310, 550,  21,  56,  73,  27, 647,  83, 150,\n",
              "            43,  53, 533,  23,  15,   8,  73, 435, 310,  39, 109, 389,  38, 513,\n",
              "            51,  26, 138, 577,  49, 300,  13, 310, 378, 112,  12, 574,  72,  44,\n",
              "           130, 585,  26,   8, 164,  13, 238,  89, 193,  92, 263,  97, 109,   5,\n",
              "            84, 184, 327,  13, 613, 349,  17, 522]),\n",
              "   'Book-Year-Of-Publication': tensor([ 1.1578,  1.0145,  0.1549, -0.4181, -1.5642,  0.5847,  0.2982,  0.8712,\n",
              "           -0.5614, -0.5614,  0.4415,  0.1549,  1.0145,  0.1549,  0.8712, -0.4181,\n",
              "           -0.9911,  0.0117,  0.2982, -0.9911,  1.0145, -0.7046, -0.1316,  0.8712,\n",
              "            1.0145,  0.4415, -0.1316,  0.1549, -0.1316,  0.8712, -0.7046, -0.1316,\n",
              "            1.1578,  0.4415,  0.7280,  0.7280,  1.0145,  0.0117,  0.8712, -0.2748,\n",
              "           -0.7046,  0.0117,  0.7280, -2.1372,  1.0145, -1.4209,  0.2982,  1.0145,\n",
              "           -1.7074, -0.1316, -0.1316,  0.4415,  0.4415,  0.0117,  0.1549,  0.8712,\n",
              "            0.5847,  0.4415,  0.8712,  0.7280,  1.0145,  0.2982, -1.2777,  0.7280])}]}"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two Tower Model for Recommendations"
      ],
      "metadata": {
        "id": "qjn33eipT_-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserTower(nn.Module):\n",
        "\n",
        "    # User Tower -- User-ID, Age\n",
        "\n",
        "    def __init__(self, num_users, embedding_dim=32):\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.user_mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim + 1, 128), # 1 embedding + 1 numerical\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, user_id, age):\n",
        "        \"\"\"\n",
        "        user_id: (batch,) int64\n",
        "        review_mean: (batch,) float32\n",
        "        \"\"\"\n",
        "        user_emb = self.user_embedding(user_id)\n",
        "        age = age.unsqueeze(1)\n",
        "        x = torch.cat([user_emb, age], dim=1)\n",
        "        return self.user_mlp(x)\n",
        "\n",
        "    def get_embedding(self, data):\n",
        "        return self.forward(data['User-ID'], data['User-Age'])\n"
      ],
      "metadata": {
        "id": "J4Rh2nDqUCxf"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ItemTower(nn.Module):\n",
        "    def __init__(self, num_isbn, num_titles, num_authors, num_publishers, embedding_dim=32):\n",
        "        super().__init__()\n",
        "\n",
        "        # Item Tower -- ISBN, Book-Title, Book-Author, Publisher, Year-Of-Publication\n",
        "\n",
        "        self.book_isbn_embedding = nn.Embedding(num_isbn, embedding_dim, padding_idx=0)\n",
        "        self.book_title_embedding = nn.Embedding(num_titles, embedding_dim, padding_idx=0)\n",
        "        self.book_author_embedding = nn.Embedding(num_authors, embedding_dim, padding_idx=0)\n",
        "        self.book_publisher_embedding = nn.Embedding(num_publishers, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.item_mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim * 4 + 1, 128),  # 4 embeddings + 1 numerical\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, isbn, book_title, book_author, book_publisher, book_year_of_publication):\n",
        "        book_isbn_emb = self.book_isbn_embedding(isbn)\n",
        "        book_title_emb = self.book_title_embedding(book_title)\n",
        "        book_author_emb = self.book_author_embedding(book_author)\n",
        "        book_publisher_emb = self.book_publisher_embedding(book_publisher)\n",
        "        book_year = book_year_of_publication.unsqueeze(1)\n",
        "\n",
        "        x = torch.cat([\n",
        "            book_isbn_emb,\n",
        "            book_title_emb,\n",
        "            book_author_emb,\n",
        "            book_publisher_emb,\n",
        "            book_year\n",
        "        ], dim=1)\n",
        "\n",
        "        return self.item_mlp(x)\n",
        "\n",
        "    def get_embedding(self, data):\n",
        "        return self.forward(\n",
        "            data['Book-ISBN'],\n",
        "            data['Book-Title'],\n",
        "            data['Book-Author'],\n",
        "            data['Book-Publisher'],\n",
        "            data['Book-Year-Of-Publication'],\n",
        "        )\n"
      ],
      "metadata": {
        "id": "hLSZeUSEUNBY"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoTowers(nn.Module):\n",
        "    def __init__(self, user_tower: UserTower, item_tower: ItemTower):\n",
        "        super().__init__()\n",
        "        self.user_tower = user_tower\n",
        "        self.item_tower = item_tower\n",
        "\n",
        "    def forward(self, data):\n",
        "        user_vector = self.user_tower.get_embedding(data)\n",
        "        item_vector = self.item_tower.get_embedding(data['pos_item'])\n",
        "        return (user_vector * item_vector).sum(dim=1)"
      ],
      "metadata": {
        "id": "l-fw4HLLUOXk"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_data = next(iter(train_loader))\n",
        "\n",
        "NUM_USERS = len(dataset.encoders['User-ID']) + 1\n",
        "NUM_ISBN = len(dataset.encoders['ISBN']) + 1\n",
        "NUM_TITLES = len(dataset.encoders['Book-Title']) + 1\n",
        "NUM_AUTHORS = len(dataset.encoders['Book-Author']) + 1\n",
        "NUM_PUBLISHERS = len(dataset.encoders['Publisher']) + 1\n",
        "\n",
        "user_tower = UserTower(num_users=NUM_USERS)\n",
        "\n",
        "item_tower = ItemTower(\n",
        "    num_isbn=NUM_ISBN,\n",
        "    num_titles=NUM_TITLES,\n",
        "    num_authors=NUM_AUTHORS,\n",
        "    num_publishers=NUM_PUBLISHERS,\n",
        ")\n",
        "\n",
        "two_towers = TwoTowers(\n",
        "    user_tower,\n",
        "    item_tower\n",
        ")"
      ],
      "metadata": {
        "id": "jlqHUau8WMjo"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "A3QGAbFDa_Lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Functions"
      ],
      "metadata": {
        "id": "bcsBmPJIb196"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Helpers ---\n",
        "def train_one_epoch(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, data in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(data)\n",
        "        targets = data['Rating']\n",
        "        loss = loss_fn(preds, targets)\n",
        "\n",
        "        # Sanity checks\n",
        "        assert not torch.isnan(preds).any(), \"NaN in predictions\"\n",
        "        assert not torch.isnan(targets).any(), \"NaN in targets\"\n",
        "        assert not torch.isinf(preds).any(), \"Inf in predictions\"\n",
        "        assert not torch.isinf(targets).any(), \"Inf in targets\"\n",
        "\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CLIP_GRAD_NORM)\n",
        "        optimizer.step()\n",
        "\n",
        "        yield batch_idx, loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_test_loss(model, test_loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for data in test_loader:\n",
        "        preds = model(data)\n",
        "        targets = data['Rating']\n",
        "        loss = loss_fn(preds, targets)\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "\n",
        "def evaluate_and_checkpoint(model, epoch, global_step, best_loss, counter, test_loader, loss_fn):\n",
        "    test_loss = calculate_test_loss(model, test_loader, loss_fn)\n",
        "\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        counter = 0\n",
        "        timestamp = datetime.datetime.now().strftime('%Y%m%d')\n",
        "        save_path = f\"{MODEL_SAVE_PATH}/two_towers_best_model_{timestamp}.pt\"\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"[Epoch {epoch}] âœ… Improved! Test Loss: {test_loss:.4f}. Model saved.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"[Epoch {epoch}] No improvement. Test Loss: {test_loss:.4f} ({counter}/{EARLY_STOPPING_PATIENCE})\")\n",
        "\n",
        "    return best_loss, counter"
      ],
      "metadata": {
        "id": "w5-fijrXb3Nm"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Training Loop"
      ],
      "metadata": {
        "id": "Hn06WdV-b4Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf ./logs/"
      ],
      "metadata": {
        "id": "bGt_U5rrg7eA"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "\n",
        "EPOCHS = 30\n",
        "LOG_INTERVAL = 100\n",
        "CLIP_GRAD_NORM = 1.0\n",
        "LEARNING_RATE = 0.001\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models\"\n",
        "\n",
        "# loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(two_towers.parameters(), lr=LEARNING_RATE)\n",
        "writer = SummaryWriter('./logs/')\n",
        "\n",
        "best_test_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "global_step = 0"
      ],
      "metadata": {
        "id": "GlzzNo4_mM0E"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    # -- Main Loop --\n",
        "    running_train_loss = 0.0\n",
        "    two_towers.train()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        user_embedding = two_towers.user_tower.get_embedding(batch)  # [B, D]\n",
        "\n",
        "        # Calculate similarity between user and positive example\n",
        "        pos_sim = two_towers(batch)\n",
        "\n",
        "        # Calculate similarity between user and negative examples\n",
        "        #\n",
        "        # Logits (first position will be positive that we want to maximize, and everything else is negative)\n",
        "        # For example, if we have 1 positive example and 1 negative example our logits are:\n",
        "        #   [[pos, neg], ..batch_size..]\n",
        "        #\n",
        "        # Now the label we want cross entropy to maximize is in the 0th position (positive)\n",
        "        #   labels = [0, ..batch_size..]\n",
        "        #\n",
        "        #\n",
        "        neg_sims = []\n",
        "        for neg_item_dict in batch['neg_items']:\n",
        "            neg_emb = two_towers.item_tower.get_embedding(neg_item_dict)  # [B, D]\n",
        "            neg_sim = torch.sum(user_embedding * neg_emb, dim=1)  # [B]\n",
        "            neg_sims.append(neg_sim)\n",
        "\n",
        "        # Contrastive Loss\n",
        "        logits = torch.stack([pos_sim] + neg_sims, dim=1)\n",
        "        labels = torch.zeros(logits.size(0), dtype=torch.long)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    two_towers.eval()\n",
        "    running_test_loss = 0.0\n",
        "    for batch_idx, batch in enumerate(test_loader):\n",
        "        pos_sim = two_towers(batch)\n",
        "        user_embedding = two_towers.user_tower.get_embedding(batch)\n",
        "        neg_embedding = two_towers.item_tower.get_embedding(batch['neg_items'][0])\n",
        "        neg_sim = torch.sum(user_embedding * neg_embedding, dim=1)\n",
        "\n",
        "        logits = torch.stack([pos_sim, neg_sim], dim=1)\n",
        "        labels = torch.zeros(logits.size(0), dtype=torch.long)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        running_test_loss += loss.item()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        avg_train_loss = running_train_loss / len(train_loader)\n",
        "        avg_test_loss = running_test_loss / len(test_loader)\n",
        "        print(f\"Epoch {epoch}/{EPOCHS}, Average Training Loss: {avg_train_loss:.4f}, \")\n",
        "        print(f\"Epoch {epoch}/{EPOCHS}, Average Test Loss: {avg_test_loss:.4f}, \")\n",
        "\n",
        "writer.close()\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "UhQ3lphfa_nU",
        "outputId": "fa934c75-81db-4f8c-d030-1f3f54d932c7"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/30, Average Training Loss: 0.1117, \n",
            "Epoch 10/30, Average Test Loss: 2.1022, \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2951995662.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtwo_towers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 clone.update(\n\u001b[1;32m    171\u001b[0m                     {\n\u001b[0;32m--> 172\u001b[0;31m                         key: collate(\n\u001b[0m\u001b[1;32m    173\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # -----------------------\n",
        "    # -- Recall@K Metric --\n",
        "    #\n",
        "    # Work in progress... Implement the recall@k metric to see out of the user's total ratings, how many of these appeared in the top-k.\n",
        "    #\n",
        "\n",
        "    # user_row = next(iter(train_loader))\n",
        "    # user_vector = two_towers.user_tower.get_embedding(user_row)\n",
        "\n",
        "    # scores = torch.matmul(item_vector, user_vector.T)\n",
        "    # scores = scores.T\n",
        "    #\n",
        "    # top_k = 20\n",
        "    # for i, user_id_encoded in enumerate(user_row['User-ID']):\n",
        "    #     top_scores, top_indices = torch.topk(scores[i], top_k)\n",
        "    #     user_id_decoded = dataset.reverse_encoders['User-ID'][user_id_encoded.item()]\n",
        "    #     ground_truth_books = df[df['User-ID'] == user_id_decoded]['ISBN']\n",
        "\n",
        "    #     print(f\"Top k books predicted for user {user_id_decoded}\")\n",
        "    #     print(top_indices)\n",
        "    #     print(f\"Book ratings for user {user_id_decoded}\")\n",
        "    #     print(ground_truth_books)\n",
        "    #\n",
        "    # -----------------------"
      ],
      "metadata": {
        "id": "ZlE4VAFWDDGs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}