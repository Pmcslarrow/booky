{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0vYuxT0pJFwGo8bua3SgB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dataset"
      ],
      "metadata": {
        "id": "L3c5igSoT9JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install kagglehub[pandas-datasets]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kVvdwqPSMaoM",
        "outputId": "97167bff-6f52-4e73-9c72-918097fce351"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "slTaZzDAMLUM",
        "outputId": "1d2fdfcc-1606-4930-d301-a18feb41f005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-885755572.py:20: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  books_df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'book-recommendation-dataset' dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/kagglehub/pandas_datasets.py:91: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  result = read_function(\n",
            "/tmp/ipython-input-885755572.py:26: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  ratings_df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'book-recommendation-dataset' dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-885755572.py:32: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  users_df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'book-recommendation-dataset' dataset.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Load the latest version\n",
        "books_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Books.csv\",\n",
        ")\n",
        "\n",
        "ratings_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Ratings.csv\",\n",
        ")\n",
        "\n",
        "users_df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"arashnic/book-recommendation-dataset\",\n",
        "  \"Users.csv\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ratings_books = pd.merge(ratings_df, books_df, on=\"ISBN\", how='inner')\n",
        "df = pd.merge(df_ratings_books, users_df, on='User-ID')\n",
        "df['User-ID'] = df['User-ID'].astype(str)\n",
        "df['Year-Of-Publication'] = pd.to_numeric(df['Year-Of-Publication'], errors='coerce')\n",
        "df = df.dropna(subset=['Year-Of-Publication'])\n",
        "df = df.dropna(subset=['Age'])\n",
        "df = df[df['Book-Rating'] > 0]\n",
        "df = df[df['Age'] <= 100]\n",
        "df = df[df['Year-Of-Publication'] > 0]\n",
        "df['Book-Rating'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "93r6ilG_NQPw",
        "outputId": "3149717e-28f2-47c5-e1d8-fc49482bdfa6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    264742.000000\n",
              "mean          7.738848\n",
              "std           1.813809\n",
              "min           1.000000\n",
              "25%           7.000000\n",
              "50%           8.000000\n",
              "75%           9.000000\n",
              "max          10.000000\n",
              "Name: Book-Rating, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Book-Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>264742.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>7.738848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.813809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Book-Rating'].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kxkPFigfCob",
        "outputId": "c6914473-85d6-43cb-d2cc-483119509c66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Book-Rating\n",
            "1       862\n",
            "2      1525\n",
            "3      3260\n",
            "4      4991\n",
            "5     27203\n",
            "6     21047\n",
            "7     44663\n",
            "8     63605\n",
            "9     44496\n",
            "10    53090\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Age'].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5BbXb9fi-iW",
        "outputId": "cd4f682d-247e-4426-c50e-be1259d68d62"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age\n",
            "0.0      203\n",
            "1.0      198\n",
            "2.0      191\n",
            "3.0       82\n",
            "4.0      143\n",
            "        ... \n",
            "96.0       2\n",
            "97.0      46\n",
            "98.0       1\n",
            "99.0       5\n",
            "100.0     51\n",
            "Name: count, Length: 96, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer\n",
        "import ast\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# User Tower -- User-ID, Age\n",
        "# Item Tower -- ISBN, Book-Title, Book-Author, Publisher, Year-Of-Publication\n",
        "\n",
        "class BookRecommenderDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class for book recommendation tasks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataframe : pd.DataFrame\n",
        "        The input data containing user, item, and possibly interaction features.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        The processed version of the input dataframe.\n",
        "    encoders : dict\n",
        "        A dictionary mapping column names to fitted label encoders.\n",
        "    reverse_encoders : dict\n",
        "        A dictionary mapping column names to reverse label encoders (index to label).\n",
        "    scalers : dict\n",
        "        A dictionary mapping column names to fitted scalers for numerical features.\n",
        "    user_item_interaction : dict\n",
        "        A dictionary mapping encoded User-IDs to a list of positive example encoded ISBNs\n",
        "    negative_examples : int\n",
        "        An integer hyperparameter for the number of negative examples to use for contrastive learning\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, negative_examples=1):\n",
        "        self.encoders = {} # {'Column name': {'value': idx, ...}, ...}\n",
        "        self.reverse_encoders = {} # {'Column name': {idx: 'value', ...}, ...}\n",
        "        self.scalers = {}\n",
        "        self.user_item_interactions = {} # {encoded userid: [encoded ISBN]}\n",
        "        self.negative_examples = negative_examples\n",
        "        self.data = data.sample(frac=0.05, random_state=42).reset_index(drop=True)\n",
        "        self.preprocess(self.data)\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        self.encode_information()\n",
        "        self.generate_positives()\n",
        "\n",
        "    def generate_positives(self):\n",
        "        self.user_item_interaction = (\n",
        "            self.data\n",
        "            .groupby('User-ID')['ISBN']\n",
        "            .apply(list)\n",
        "            .to_dict()\n",
        "        )\n",
        "\n",
        "    def encode_information(self):\n",
        "        \"\"\"\n",
        "        Maps {key: index} pairs and StandardScaler for real valued numbers\n",
        "        \"\"\"\n",
        "        label_encoders = ['User-ID', 'ISBN', 'Book-Title', 'Book-Author', 'Publisher']\n",
        "        standard_scalers = ['Age', 'Year-Of-Publication']\n",
        "\n",
        "        for col in label_encoders:\n",
        "            unique_vals = self.data[col].astype(str).unique()\n",
        "            self.encoders[col] = {val: idx + 1 for idx, val in enumerate(unique_vals)}\n",
        "            self.reverse_encoders[col] = {idx + 1: val for idx, val in enumerate(unique_vals)}\n",
        "            self.data[col] = self.data[col].astype(str).map(self.encoders[col]).fillna(0).astype(int)\n",
        "\n",
        "        for col in standard_scalers:\n",
        "            self.scalers[col] = StandardScaler()\n",
        "            self.data[[col]] = self.scalers[col].fit_transform(self.data[[col]])\n",
        "\n",
        "        # Manually adding my own User-ID so I don't need to adjust nn.Embedding later\n",
        "        max_user_idx = max(self.encoders['User-ID'].values())\n",
        "        self.encoders['User-ID'][\"1234567890\"] = max_user_idx + 1\n",
        "        self.reverse_encoders['User-ID'][max_user_idx + 1] = \"1234567890\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "            - user-tower data (User-ID and User-Age)\n",
        "            - positive item data (pos_item)\n",
        "            - negative item data (neg_items)\n",
        "            - Target rating\n",
        "        \"\"\"\n",
        "        positive_item = self.data.iloc[idx]\n",
        "        positive_user_id = positive_item['User-ID']\n",
        "        positive_isbns = self.user_item_interaction[positive_user_id]\n",
        "\n",
        "        negative_examples = []\n",
        "        while len(negative_examples) < self.negative_examples:\n",
        "            candidate = self.data.sample(n=1).iloc[0]\n",
        "            candidate_isbn = candidate['ISBN']\n",
        "            if candidate_isbn not in positive_isbns:\n",
        "                negative_examples.append(candidate)\n",
        "\n",
        "        output = {\n",
        "            'User-ID': torch.tensor(positive_item['User-ID'], dtype=torch.long),\n",
        "            'User-Age': torch.tensor(positive_item['Age'], dtype=torch.float32),\n",
        "            'Rating': torch.tensor(positive_item['Book-Rating'], dtype=torch.float32),\n",
        "\n",
        "            'pos_item': {\n",
        "                'Book-ISBN': torch.tensor(positive_item['ISBN'], dtype=torch.long),\n",
        "                'Book-Title': torch.tensor(positive_item['Book-Title'], dtype=torch.long),\n",
        "                'Book-Author': torch.tensor(positive_item['Book-Author'], dtype=torch.long),\n",
        "                'Book-Publisher': torch.tensor(positive_item['Publisher'], dtype=torch.long),\n",
        "                'Book-Year-Of-Publication': torch.tensor(positive_item['Year-Of-Publication'], dtype=torch.float32),\n",
        "            },\n",
        "\n",
        "            'neg_items': [\n",
        "                {\n",
        "                    'Book-ISBN': torch.tensor(neg['ISBN'], dtype=torch.long),\n",
        "                    'Book-Title': torch.tensor(neg['Book-Title'], dtype=torch.long),\n",
        "                    'Book-Author': torch.tensor(neg['Book-Author'], dtype=torch.long),\n",
        "                    'Book-Publisher': torch.tensor(neg['Publisher'], dtype=torch.long),\n",
        "                    'Book-Year-Of-Publication': torch.tensor(neg['Year-Of-Publication'], dtype=torch.float32),\n",
        "                }\n",
        "                for neg in negative_examples\n",
        "            ]\n",
        "        }\n",
        "        return output\n",
        "\n",
        "\n",
        "dataset = BookRecommenderDataset(df)\n"
      ],
      "metadata": {
        "id": "hzV4YkTNPSuc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNCVwwNWwUc7",
        "outputId": "0ab3b9ca-c9ec-429b-a082-4a004873fdf9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'User-ID': tensor(1),\n",
              " 'User-Age': tensor(-0.2955),\n",
              " 'Rating': tensor(7.),\n",
              " 'pos_item': {'Book-ISBN': tensor(1),\n",
              "  'Book-Title': tensor(1),\n",
              "  'Book-Author': tensor(1),\n",
              "  'Book-Publisher': tensor(1),\n",
              "  'Book-Year-Of-Publication': tensor(0.4313)},\n",
              " 'neg_items': [{'Book-ISBN': tensor(432),\n",
              "   'Book-Title': tensor(429),\n",
              "   'Book-Author': tensor(332),\n",
              "   'Book-Publisher': tensor(6),\n",
              "   'Book-Year-Of-Publication': tensor(0.8439)}]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.5 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_data, test_data = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "5UuengF0Tojk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vznGD0IGTxA7",
        "outputId": "eda53411-30ca-4982-fced-d0215c2c7105"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'User-ID': tensor([2087, 2239, 4866, 3085, 1548, 2328, 3379, 1420, 2097,  631, 1762,   44,\n",
              "          607,  323, 2940, 1000, 2238, 1387, 3731,   38,  607,   38, 1564, 3064,\n",
              "         5677, 3786,  426, 2307,  156,  124,  121, 1500, 4743,   38, 4404, 2018,\n",
              "         6449,  802,  536, 4578, 1013, 1463,  201,  207, 3603, 2705,  545, 1046,\n",
              "          542,  545, 5467, 1953, 3396, 5200,  545, 6586,  900,  575, 2861,   38,\n",
              "         1342,  297, 5510,   38]),\n",
              " 'User-Age': tensor([ 1.3264,  0.7587, -0.8631,  0.8398,  0.1100, -1.1064, -0.9442, -0.3766,\n",
              "          0.9209, -1.5930, -0.7010,  0.5965, -0.2144, -0.6199, -0.3766,  1.1642,\n",
              "          0.8398, -0.4577, -0.4577,  1.2453, -0.2144,  1.2453, -0.6199, -0.5388,\n",
              "          0.7587, -0.1333,  0.4343, -1.5930,  1.2453, -1.1875, -0.2144, -0.2955,\n",
              "         -0.0522,  1.2453, -0.6199,  1.6507, -1.0253, -1.0253,  0.8398, -1.1064,\n",
              "          0.4343,  1.6507, -1.1875, -1.3497, -0.0522, -0.4577,  1.2453, -0.0522,\n",
              "         -0.7010,  1.2453,  0.4343, -1.5119,  1.1642, -1.3497,  1.2453, -0.0522,\n",
              "          1.2453, -0.9442, -0.9442,  1.2453, -0.1333,  1.8940, -1.0253,  1.2453]),\n",
              " 'Rating': tensor([ 8.,  7.,  9.,  9.,  5., 10.,  9.,  8.,  9., 10.,  8.,  7.,  8.,  8.,\n",
              "          7.,  8., 10., 10., 10., 10., 10., 10.,  7.,  8.,  5., 10., 10.,  8.,\n",
              "          8.,  8., 10.,  8.,  8.,  9.,  8.,  8.,  8., 10., 10.,  8.,  7., 10.,\n",
              "          6.,  8.,  9.,  8., 10.,  3.,  4.,  9.,  5.,  8.,  4.,  4.,  9.,  9.,\n",
              "         10., 10.,  3.,  5.,  5.,  5.,  8.,  8.]),\n",
              " 'pos_item': {'Book-ISBN': tensor([ 2568,  2802,  7299, 10673,  1849,  2918,  5496,  5330,  2582,  7716,\n",
              "           6997,  3327,  8057, 10180,  6539,  5952,  5410,  5804,  9103,  8530,\n",
              "           2031,  2166,  1873,  9654,  8867,  5278,   466,   712,  7176,  3664,\n",
              "            127,  9071,  5490,  8883,  6382,  2470, 10543,   901, 10523,  6700,\n",
              "           3220,  1727,   212,   218,  4968,  3527,   749,  1205,   593,  1269,\n",
              "           8488,  2390,  4600,  7940,  3010, 10832,  1744,  3182,  3760,  2504,\n",
              "           4652, 10513,  8562, 10049]),\n",
              "  'Book-Title': tensor([ 2528,  2751,  7036, 10221,  1825,  2862,  5324,  5164,  2541,  7434,\n",
              "           6750,  3256,  7758,  9766,  6306,  5750,  5242,  5614,  8736,  8194,\n",
              "           2001,  2132,  1848,  9263,  8515,  5114,   463,   706,  6922,  3449,\n",
              "            126,  8706,  5318,  3721,  6157,  2434, 10100,   894, 10080,  6463,\n",
              "           3154,  1517,   210,   216,  4819,  3448,   743,  1192,   588,  1256,\n",
              "           8154,  2355,  4464,  7646,  2952, 10369,  1721,  3118,  3672,  2466,\n",
              "           4514, 10070,  8225,  9638]),\n",
              "  'Book-Author': tensor([1935, 2076, 2261,   87, 1443, 1167,  625, 1882,  311, 1941, 4471, 2376,\n",
              "           208, 6171, 4217, 3892,  174, 3813, 5608, 5297, 1562, 1654,  784, 5891,\n",
              "          5478, 3525,  419,  627, 4563,  680,  119, 2852,  294, 2666, 2111, 1867,\n",
              "          6351,  782, 6343, 4303, 1065,  711,  197,  203, 3346,  144,  658, 1006,\n",
              "           527,  132,  612, 1812, 3135, 4972, 2199,  645, 1373, 1758, 2634, 1890,\n",
              "          3161, 1013, 5313, 6105]),\n",
              "  'Book-Publisher': tensor([  40,  815,  348,   40,  250,  811,   71,   43,    5,  181,  120,   18,\n",
              "            51,  429,   99,   74, 1235,  253,  114,   30,  284,  149,   54, 1779,\n",
              "          1688,  129,   12,   99, 1479,   58,   98,   25,   43,   76,  303,  747,\n",
              "             2,   46, 1888,  125,   12,   21,  147,  150,   43,   17,    5,  463,\n",
              "            94,  223,    2,   93,   21,  128,   93,   21,  598,   30,   46,   93,\n",
              "          1119,  277,   30,  628]),\n",
              "  'Book-Year-Of-Publication': tensor([ 1.1189,  0.8439,  0.5688, -1.3565, -1.0815, -0.3938, -0.3938, -0.6689,\n",
              "           0.7063,  0.7063, -0.1188,  0.5688, -0.1188, -0.6689,  0.7063, -1.3565,\n",
              "           0.9814,  0.8439, -0.3938,  1.1189, -0.2563,  0.9814,  0.9814, -0.1188,\n",
              "           0.8439,  0.7063,  0.5688, -0.2563, -0.6689,  0.1562, -5.2071, -5.4822,\n",
              "          -0.8064,  0.8439,  0.2938,  0.5688, -1.0815,  0.4313,  0.1562, -1.7691,\n",
              "           0.4313,  0.4313, -0.2563,  0.9814,  0.8439, -0.5314,  0.5688, -0.1188,\n",
              "          -0.8064,  0.4313,  0.5688, -0.5314, -0.8064, -2.0441, -0.5314, -0.2563,\n",
              "          -5.0696, -0.1188,  0.4313,  1.1189, -0.9439,  0.7063,  0.9814,  0.8439])},\n",
              " 'neg_items': [{'Book-ISBN': tensor([ 2141,  6179,  5663,  7521,  4722,   732,  2779,   187,  1842,  7081,\n",
              "            4392,  5227,   973,  2409,  9916,  2289,  6170,  9110,   416,  6326,\n",
              "            7137,   483,  4062,  8474,  1409,  5514,  1845,  1108, 10912,  6571,\n",
              "            8454,  9691, 10088,  3832,  7912, 10347, 10259,   819,  8716,  5038,\n",
              "            2777,  9817,   463, 10937,   394,    11,  1940,  8154,  8442,  6323,\n",
              "            9994,  8732,  6873,  7974,   498,  5001,  3402,  7186,  7182,  4310,\n",
              "             446,   756,  2698, 10219]),\n",
              "   'Book-Title': tensor([ 2107,  5965,  5478,  7248,  4580,   726,  2728,   185,  1818,  6829,\n",
              "            4265,  5067,   965,  2374,  9513,  2254,  5956,  8743,   413,  6104,\n",
              "            6884,   480,  3954,  8141,  1393,  5342,  1821,  1098, 10441,  6337,\n",
              "            8123,  9300,  9676,  3736,  7618,  9915,  9838,   813,  8368,  4885,\n",
              "            2726,  9419,   460, 10465,   391,    11,  1913,  4745,  8112,  6101,\n",
              "            9586,  8384,  6630,  7678,   495,  4850,  3328,  6931,  6927,  4189,\n",
              "             443,   750,  2652,  9804]),\n",
              "   'Book-Author': tensor([ 228, 1768, 3727, 2419,  625,  619,  842,  172, 1439,  886, 3019, 3496,\n",
              "            832,  718, 2122, 1735,  531, 5611,  380, 1013,   33,  434,  567, 5268,\n",
              "           1146, 1295, 1440,  267, 6560, 1088, 5260, 5911, 6125,  225, 4957, 6264,\n",
              "           4302,  666,   59, 3389,  625, 4673,  416, 1822,  360,   11, 1502,  871,\n",
              "           5251, 4101, 1253, 5404,  476, 4993,  446, 3365, 2419, 4568,  418,  329,\n",
              "            400,  664,  557, 6192]),\n",
              "   'Book-Publisher': tensor([ 173,  120, 1279,   33,   71,   70,  120,    3,  618,  363,   69,  866,\n",
              "             30,   37,   79,  147,   12,  733,    5,   12,   12,  179,  420,  154,\n",
              "             62,   74,  166,    1,  234,  250,   29,   37,  258,    1,  149, 1868,\n",
              "            829,  115,  149,  258,   71,  149,  256,   18,  225,   11,  226,   25,\n",
              "           1638,  654, 1283,  386,  363,  265,  266, 1169,  919,  821,  114,  177,\n",
              "              6,   16,   99,  157]),\n",
              "   'Book-Year-Of-Publication': tensor([ 0.7063,  0.9814, -0.5314,  0.7063, -0.8064, -0.1188,  0.5688,  0.5688,\n",
              "            0.5688, -1.9066,  0.7063,  0.2938,  0.7063,  0.7063, -0.2563,  0.7063,\n",
              "            0.8439,  0.9814,  0.8439,  0.0187,  1.1189,  0.2938, -0.2563, -0.2563,\n",
              "           -0.2563, -2.1816,  1.1189, -1.0815,  0.8439, -0.6689,  0.8439,  1.1189,\n",
              "            0.7063, -0.3938,  0.8439,  0.8439, -0.3938,  0.9814,  0.9814,  0.7063,\n",
              "           -0.8064,  0.9814,  0.4313, -0.5314,  0.5688,  0.9814,  0.7063,  0.0187,\n",
              "            0.5688,  1.1189,  1.1189,  1.1189, -2.0441,  0.1562,  0.2938, -0.1188,\n",
              "            0.4313, -0.5314, -0.1188, -0.1188, -0.2563,  0.2938,  0.8439,  0.2938])}]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two Tower Model for Recommendations"
      ],
      "metadata": {
        "id": "qjn33eipT_-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UserTower(nn.Module):\n",
        "\n",
        "    # User Tower -- User-ID, Age\n",
        "\n",
        "    def __init__(self, num_users, embedding_dim=32):\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.user_mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim + 1, 128), # 1 embedding + 1 numerical\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, user_id, age):\n",
        "        \"\"\"\n",
        "        user_id: (batch,) int64\n",
        "        review_mean: (batch,) float32\n",
        "        \"\"\"\n",
        "        user_emb = self.user_embedding(user_id)\n",
        "        age = age.unsqueeze(1)\n",
        "        x = torch.cat([user_emb, age], dim=1)\n",
        "        return self.user_mlp(x)\n",
        "\n",
        "    def get_embedding(self, data):\n",
        "        return self.forward(data['User-ID'], data['User-Age'])\n"
      ],
      "metadata": {
        "id": "J4Rh2nDqUCxf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ItemTower(nn.Module):\n",
        "    def __init__(self, num_isbn, num_titles, num_authors, num_publishers, embedding_dim=32):\n",
        "        super().__init__()\n",
        "\n",
        "        # Item Tower -- ISBN, Book-Title, Book-Author, Publisher, Year-Of-Publication\n",
        "\n",
        "        self.book_isbn_embedding = nn.Embedding(num_isbn, embedding_dim, padding_idx=0)\n",
        "        self.book_title_embedding = nn.Embedding(num_titles, embedding_dim, padding_idx=0)\n",
        "        self.book_author_embedding = nn.Embedding(num_authors, embedding_dim, padding_idx=0)\n",
        "        self.book_publisher_embedding = nn.Embedding(num_publishers, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.item_mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim * 4 + 1, 128),  # 4 embeddings + 1 numerical\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, embedding_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, isbn, book_title, book_author, book_publisher, book_year_of_publication):\n",
        "        book_isbn_emb = self.book_isbn_embedding(isbn)\n",
        "        book_title_emb = self.book_title_embedding(book_title)\n",
        "        book_author_emb = self.book_author_embedding(book_author)\n",
        "        book_publisher_emb = self.book_publisher_embedding(book_publisher)\n",
        "        book_year = book_year_of_publication.unsqueeze(1)\n",
        "\n",
        "        x = torch.cat([\n",
        "            book_isbn_emb,\n",
        "            book_title_emb,\n",
        "            book_author_emb,\n",
        "            book_publisher_emb,\n",
        "            book_year\n",
        "        ], dim=1)\n",
        "\n",
        "        return self.item_mlp(x)\n",
        "\n",
        "    def get_embedding(self, data):\n",
        "        return self.forward(\n",
        "            data['Book-ISBN'],\n",
        "            data['Book-Title'],\n",
        "            data['Book-Author'],\n",
        "            data['Book-Publisher'],\n",
        "            data['Book-Year-Of-Publication'],\n",
        "        )\n"
      ],
      "metadata": {
        "id": "hLSZeUSEUNBY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoTowers(nn.Module):\n",
        "    def __init__(self, user_tower: UserTower, item_tower: ItemTower):\n",
        "        super().__init__()\n",
        "        self.user_tower = user_tower\n",
        "        self.item_tower = item_tower\n",
        "\n",
        "    def forward(self, data):\n",
        "        user_vector = self.user_tower.get_embedding(data)\n",
        "        item_vector = self.item_tower.get_embedding(data['pos_item'])\n",
        "        return (user_vector * item_vector).sum(dim=1)"
      ],
      "metadata": {
        "id": "l-fw4HLLUOXk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_data = next(iter(train_loader))\n",
        "\n",
        "NUM_USERS = len(dataset.encoders['User-ID']) + 1\n",
        "NUM_ISBN = len(dataset.encoders['ISBN']) + 1\n",
        "NUM_TITLES = len(dataset.encoders['Book-Title']) + 1\n",
        "NUM_AUTHORS = len(dataset.encoders['Book-Author']) + 1\n",
        "NUM_PUBLISHERS = len(dataset.encoders['Publisher']) + 1\n",
        "\n",
        "user_tower = UserTower(num_users=NUM_USERS)\n",
        "\n",
        "item_tower = ItemTower(\n",
        "    num_isbn=NUM_ISBN,\n",
        "    num_titles=NUM_TITLES,\n",
        "    num_authors=NUM_AUTHORS,\n",
        "    num_publishers=NUM_PUBLISHERS,\n",
        ")\n",
        "\n",
        "two_towers = TwoTowers(\n",
        "    user_tower,\n",
        "    item_tower\n",
        ")"
      ],
      "metadata": {
        "id": "jlqHUau8WMjo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "A3QGAbFDa_Lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Functions"
      ],
      "metadata": {
        "id": "bcsBmPJIb196"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Helpers ---\n",
        "def train_one_epoch(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, data in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(data)\n",
        "        targets = data['Rating']\n",
        "        loss = loss_fn(preds, targets)\n",
        "\n",
        "        # Sanity checks\n",
        "        assert not torch.isnan(preds).any(), \"NaN in predictions\"\n",
        "        assert not torch.isnan(targets).any(), \"NaN in targets\"\n",
        "        assert not torch.isinf(preds).any(), \"Inf in predictions\"\n",
        "        assert not torch.isinf(targets).any(), \"Inf in targets\"\n",
        "\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CLIP_GRAD_NORM)\n",
        "        optimizer.step()\n",
        "\n",
        "        yield batch_idx, loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_test_loss(model, test_loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for data in test_loader:\n",
        "        preds = model(data)\n",
        "        targets = data['Rating']\n",
        "        loss = loss_fn(preds, targets)\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches if num_batches > 0 else float('inf')\n",
        "\n",
        "def evaluate_and_checkpoint(model, epoch, global_step, best_loss, counter, test_loader, loss_fn):\n",
        "    test_loss = calculate_test_loss(model, test_loader, loss_fn)\n",
        "\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        counter = 0\n",
        "        timestamp = datetime.datetime.now().strftime('%Y%m%d')\n",
        "        save_path = f\"{MODEL_SAVE_PATH}/two_towers_best_model_{timestamp}.pt\"\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"[Epoch {epoch}] âœ… Improved! Test Loss: {test_loss:.4f}. Model saved.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"[Epoch {epoch}] No improvement. Test Loss: {test_loss:.4f} ({counter}/{EARLY_STOPPING_PATIENCE})\")\n",
        "\n",
        "    return best_loss, counter"
      ],
      "metadata": {
        "id": "w5-fijrXb3Nm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Training Loop"
      ],
      "metadata": {
        "id": "Hn06WdV-b4Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf ./logs/"
      ],
      "metadata": {
        "id": "bGt_U5rrg7eA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "\n",
        "EPOCHS = 200\n",
        "LOG_INTERVAL = 100\n",
        "CLIP_GRAD_NORM = 1.0\n",
        "LEARNING_RATE = 0.001\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models\"\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(two_towers.parameters(), lr=LEARNING_RATE)\n",
        "writer = SummaryWriter('./logs/')\n",
        "\n",
        "best_test_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "global_step = 0"
      ],
      "metadata": {
        "id": "GlzzNo4_mM0E"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    # -- Main Loop --\n",
        "    running_loss = 0.0\n",
        "    two_towers.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate similarity between user and positive examples\n",
        "        pos_sim = two_towers(batch)  # shape: [batch_size]\n",
        "\n",
        "        # Calculate similarity between user and negative examples\n",
        "        user_embedding = two_towers.user_tower.get_embedding(batch)          # [batch_size, d]\n",
        "        neg_embedding = two_towers.item_tower.get_embedding(batch['neg_items'][0])  # [batch_size, d]\n",
        "        neg_sim = torch.sum(user_embedding * neg_embedding, dim=1)\n",
        "\n",
        "        #\n",
        "        # Logits (first position will be positive that we want to maximize, and everything else is negative)\n",
        "        # For example, if we have 1 positive example and 1 negative example our logits are:\n",
        "        #   [[pos, neg], ..batch_size..]\n",
        "        #\n",
        "        # Now the label we want cross entropy to maximize is in the 0th position (positive)\n",
        "        #   labels = [0, ..batch_size..]\n",
        "        #\n",
        "        logits = torch.stack([pos_sim, neg_sim], dim=1)\n",
        "        labels = torch.zeros(logits.size(0), dtype=torch.long)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}, Average Training Loss (based on rating): {avg_loss:.4f}, \")\n",
        "\n",
        "writer.close()\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "UhQ3lphfa_nU",
        "outputId": "d6853fb0-6aac-49a1-f738-fe1ed326bed7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Average Training Loss (based on rating): 0.7072, \n",
            "Epoch 2/200, Average Training Loss (based on rating): 0.6546, \n",
            "Epoch 3/200, Average Training Loss (based on rating): 0.5976, \n",
            "Epoch 4/200, Average Training Loss (based on rating): 0.5282, \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1990182441.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtwo_towers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1473677978.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mnegative_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_examples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mcandidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mcandidate_isbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ISBN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_isbn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_isbns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   6117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6118\u001b[0m         \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6119\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4131\u001b[0m             )\n\u001b[1;32m   4132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4133\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   4134\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4135\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         return self.reindex_indexer(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             new_blocks = [\n\u001b[0;32m--> 688\u001b[0;31m                 blk.take_nd(\n\u001b[0m\u001b[1;32m    689\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;31m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m         new_values = algos.take_nd(\n\u001b[0m\u001b[1;32m   1308\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_take_nd_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_platform_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     dtype, fill_value, mask_info = _take_preprocess_indexer_and_fill_value(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36m_take_preprocess_indexer_and_fill_value\u001b[0;34m(arr, indexer, fill_value, allow_fill, mask)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0mneeds_masking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0mmask_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_masking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mneeds_masking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py\u001b[0m in \u001b[0;36m_any\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mumr_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning on my own data\n",
        "\n",
        "- The item embeddings were learned during training. I will freeze the item tower.\n",
        "- I will use my own dataset of books I like to train the user embeddings."
      ],
      "metadata": {
        "id": "eQx6S_WNwJt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # -----------------------\n",
        "    # -- Recall@K Metric --\n",
        "    #\n",
        "    # Work in progress... Implement the recall@k metric to see out of the user's total ratings, how many of these appeared in the top-k.\n",
        "    #\n",
        "\n",
        "    # user_row = next(iter(train_loader))\n",
        "    # user_vector = two_towers.user_tower.get_embedding(user_row)\n",
        "\n",
        "    # scores = torch.matmul(item_vector, user_vector.T)\n",
        "    # scores = scores.T\n",
        "    #\n",
        "    # top_k = 20\n",
        "    # for i, user_id_encoded in enumerate(user_row['User-ID']):\n",
        "    #     top_scores, top_indices = torch.topk(scores[i], top_k)\n",
        "    #     user_id_decoded = dataset.reverse_encoders['User-ID'][user_id_encoded.item()]\n",
        "    #     ground_truth_books = df[df['User-ID'] == user_id_decoded]['ISBN']\n",
        "\n",
        "    #     print(f\"Top k books predicted for user {user_id_decoded}\")\n",
        "    #     print(top_indices)\n",
        "    #     print(f\"Book ratings for user {user_id_decoded}\")\n",
        "    #     print(ground_truth_books)\n",
        "    #\n",
        "    # -----------------------"
      ],
      "metadata": {
        "id": "ZlE4VAFWDDGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_items_loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
        "items = next(iter(all_items_loader))\n",
        "item_vector = two_towers.item_tower.get_embedding(items)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "lujShgzX0qEl",
        "outputId": "83dfd580-657d-46c2-b217-696d9563c94a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Book-ISBN'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-70624001.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_items_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_items_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mitem_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_towers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_tower\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1578111566.py\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         return self.forward(\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Book-ISBN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Book-Title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Book-Author'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Book-ISBN'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FineTuningDataset(Dataset):\n",
        "    def __init__(self, data, encoders, scalers):\n",
        "        self.data = data.copy()\n",
        "        self.encoders = encoders\n",
        "        self.scalers = scalers\n",
        "        self.preprocess()\n",
        "\n",
        "    def preprocess(self):\n",
        "        categorical_cols = ['User-ID', 'ISBN', 'Book-Title', 'Book-Author', 'Publisher']\n",
        "        for col in categorical_cols:\n",
        "            self.data[col] = self.data[col].astype(str).map(self.encoders[col]).fillna(0).astype(int)\n",
        "\n",
        "        numeric_cols = ['Age', 'Year-Of-Publication']\n",
        "        for col in numeric_cols:\n",
        "            scaled_vals = self.scalers[col].transform(self.data[[col]])\n",
        "            self.data[col] = scaled_vals.flatten()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        return {\n",
        "            'User-ID': torch.tensor(item['User-ID'], dtype=torch.long),\n",
        "            'User-Age': torch.tensor(item['Age'], dtype=torch.float32),\n",
        "            'Book-ISBN': torch.tensor(item['ISBN'], dtype=torch.long),\n",
        "            'Book-Title': torch.tensor(item['Book-Title'], dtype=torch.long),\n",
        "            'Book-Author': torch.tensor(item['Book-Author'], dtype=torch.long),\n",
        "            'Book-Publisher': torch.tensor(item['Publisher'], dtype=torch.long),\n",
        "            'Book-Year-Of-Publication': torch.tensor(item['Year-Of-Publication'], dtype=torch.float32),\n",
        "            'Rating': torch.tensor(item['Book-Rating'], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "fine_tuning_df = pd.read_csv(\"./drive/MyDrive/fine-tuning-book-set.txt\")\n",
        "fine_tuning_dataset = FineTuningDataset(fine_tuning_df, dataset.encoders, dataset.scalers)\n",
        "fine_tuning_loader = DataLoader(fine_tuning_dataset, batch_size=len(fine_tuning_dataset), shuffle=False)\n"
      ],
      "metadata": {
        "id": "QaAlkRdS5OK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(fine_tuning_loader))"
      ],
      "metadata": {
        "id": "kVIWocr_67TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in two_towers.item_tower.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "FINE_TUNING_EPOCHS = 10\n",
        "for epoch in range(1, FINE_TUNING_EPOCHS + 1):\n",
        "\n",
        "    # -- Main Loop --\n",
        "    running_loss = 0.0\n",
        "    two_towers.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(fine_tuning_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = two_towers(batch)\n",
        "        targets = batch['Rating']\n",
        "        loss = loss_fn(preds, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch}, Average Training Loss (based on rating): {avg_loss:.4f}\")\n",
        "\n",
        "avg_loss = running_loss / len(train_loader)\n",
        "print(f\"Fine-Tuned Set {epoch}, Average Training Loss (based on rating): {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "gP5kcHRSwJZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_row = next(iter(fine_tuning_loader))\n",
        "user_vector = two_towers.user_tower.get_embedding(user_row)\n",
        "\n",
        "scores = torch.matmul(item_vector, user_vector.T)\n",
        "scores = scores.T\n",
        "\n",
        "topk_scores, topk_indices = torch.topk(scores, k=50, dim=1)\n",
        "print(topk_scores[0])\n",
        "print(topk_indices[0])"
      ],
      "metadata": {
        "id": "8puYkWCe8qYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommended_books = df.iloc[topk_indices[0].tolist()]\n",
        "print(recommended_books[['Book-Title']])"
      ],
      "metadata": {
        "id": "6UaF8hbi-Eqm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}